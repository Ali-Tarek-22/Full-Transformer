{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbmFfj4ux3wk"
      },
      "source": [
        "Let's Create The Transfomrer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGxIxUNULEBs"
      },
      "source": [
        "# **Includes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKN-FL446vi7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV3ku7rvkjfM"
      },
      "source": [
        "# **Transformer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9F5lyOEK4CF"
      },
      "source": [
        "## **Preprocess**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTC0XcOsK9Es"
      },
      "source": [
        "### **Positinoal Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb32MW6bK_nD"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        even_indices = torch.arange(0, self.d_model, 2).float() # d_model/2 # Even indices\n",
        "        denominator = torch.pow(10000, even_indices/self.d_model) # d_model/2 # Denominator (same for even and odd indices)\n",
        "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1) # max_sequence_length x 1 # 0 -> max_sequence_length\n",
        "        even_PE = torch.sin(position / denominator) # max_sequence_length x d_model/2\n",
        "        odd_PE = torch.cos(position / denominator) # max_sequence_length x d_model/2\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2) # max_sequence_length x d_model/2 x 2\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2) # max_sequence_length x d_model\n",
        "        return PE.unsqueeze(0) # 1 x max_sequence_length x d_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jqmYHpSqa7k"
      },
      "source": [
        "### **Tokenize and Embedding**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAVG9lKORt_T"
      },
      "source": [
        " In PyTorch, tensors live on a specific device (CPU or GPU).\\\n",
        " Operations cannot mix devices: you can’t add a CPU tensor to a GPU tensor.\\\n",
        " You have to manually move tensors and models to the same device using .to(device) or .cuda(). (like x = tensor([1, 2, 3]).cuda())\\\n",
        "_______________________________\n",
        " TensorFlow has a default device context.\\\n",
        " When you create a tensor, TF automatically puts it on a “default” device (usually GPU if available).\\\n",
        " You don’t usually have to manually call .to(device); TensorFlow moves everything under the hood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6cM5ZnOLO5_"
      },
      "outputs": [],
      "source": [
        "def get_device(): # Return cuda (GPU) if possible, else return cpu\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o4aitJyqe7-"
      },
      "outputs": [],
      "source": [
        "class SentenceEmbedding(nn.Module):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model) # maps token IDs → dense vectors of size d_model\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "        # compute positional encoding\n",
        "        self.positional_encoding = self.position_encoder().to(get_device())\n",
        "\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "\n",
        "        def tokenize(sentence, start_token, end_token): # Tokenize the sentence\n",
        "            # Convert tokens into indices\n",
        "            sentence_indices = [self.language_to_index[token] for token in list(sentence)] # Iterate over letters, if u want to iterate over words, use sentence.split(), if you switch to words, make sure the vocabulary is words not letters, also that max_seq_len is in number of words not letters, finally You may want a <UNK> token for words not in your vocabulary, which wasn’t needed for characters.\n",
        "            # Insert start token at the begining\n",
        "            if start_token:\n",
        "                sentence_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            # Append the end token at the end\n",
        "            if end_token:\n",
        "                sentence_indices.append(self.language_to_index[self.END_TOKEN])\n",
        "            # Pad to max_sequence_length\n",
        "            for _ in range(len(sentence_indices), self.max_sequence_length):\n",
        "                sentence_indices.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "            # Truncate if too long\n",
        "            sentence_indices = sentence_indices[:self.max_sequence_length]\n",
        "            return torch.tensor(sentence_indices)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence in range(len(batch)): # Iterate through sentences in the batch\n",
        "           tokenized.append(tokenize(batch[sentence], start_token, end_token)) # Tokenize the sentence\n",
        "        # tokenized now is a python list of tensor each with length max_seq_len, the list has batch_size tensors\n",
        "        # We want to convert this list of tensors into a 2d tensor with shape batch_size x max_seq_len\n",
        "        tokenized = torch.stack(tokenized)\n",
        "        return tokenized.to(get_device()) # get_device() returns the device (GPU, CPU ..) then .to(device) moves the tensor to that device\n",
        "        # This ensures that your tokenized tensor is on the same device as the model before you feed it into the embedding layer. Without it, you could get a device mismatch error .\n",
        "    def forward(self, x, start_token, end_token): # sentence\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x) # Convert tokens into embeddings\n",
        "        x = self.dropout(x + self.positional_encoding) # When you do x + positional_encoding, PyTorch requires both tensors to be on the same device. If x is on GPU but pos is still on CPU, you would get an error, that's why we also moved the positional encoding to gpu.\n",
        "        return x\n",
        "        # Now x is on GPU (if possible), any further computation on it, the output will be on the same device. If a tensor is computed from other tensors already on the correct device, it automatically stays on that device.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD143phx6tg6"
      },
      "source": [
        "### **Masks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIecYEzm6vy1"
      },
      "outputs": [],
      "source": [
        "class Masks(nn.Module):\n",
        "  def __init__(self, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.max_seq_len = max_seq_len\n",
        "    # Compute the look ahead mask in __init__ only once as it's the same for all batches avoiding recreating it every forward pass — more efficient.\n",
        "    self.look_ahead_mask = torch.triu(torch.ones((self.max_seq_len, self.max_seq_len), dtype = torch.bool), diagonal = 1) # All elements below diagonal + diagonal are False\n",
        "\n",
        "  def forward(self, enc_lang_batch, dec_lang_batch,\n",
        "                  enc_start_token, enc_end_token,\n",
        "                  dec_start_token, dec_end_token, NEG_INF = -1e9):\n",
        "          num_sentences = len(enc_lang_batch)\n",
        "          # Create the masks with boolean values where Trues will be replaced by -inf and Falses with 0\n",
        "          encoder_padding_mask_selfAttention = torch.zeros((num_sentences, self.max_seq_len, self.max_seq_len), dtype = torch.bool) # the matrix is filled with zeros, then making the type boolean converts them to False\n",
        "          decoder_padding_mask_selfAttention = torch.zeros((num_sentences, self.max_seq_len, self.max_seq_len), dtype = torch.bool)\n",
        "          decoder_padding_mask_crossAttention = torch.zeros((num_sentences, self.max_seq_len, self.max_seq_len), dtype = torch.bool)\n",
        "          # You can shape the masks as [num_sentences, 1, max_seq_len, max_seq_len] so you can remove the permutation in the\n",
        "          # scaled product function and let the masks broadcast through the heads\n",
        "          # To do this, in the end of this function, add: encoder_padding_mask_selfAttention = encoder_padding_mask_selfAttention.unsqueeze(1) (same for the other two)\n",
        "\n",
        "          for idx in range(num_sentences):\n",
        "            enc_lang_sent_len, dec_lang_sent_len = len(enc_lang_batch[idx]), len(dec_lang_batch[idx])\n",
        "\n",
        "            enc_padding_start = enc_lang_sent_len + (1 if enc_start_token else 0) + (1 if enc_end_token else 0)\n",
        "            enc_lang_padding = np.arange(enc_padding_start , self.max_seq_len)\n",
        "\n",
        "            dec_padding_start = dec_lang_sent_len + (1 if dec_start_token else 0) + (1 if dec_end_token else 0)\n",
        "            dec_lang_padding = np.arange(dec_padding_start , self.max_seq_len)\n",
        "\n",
        "            encoder_padding_mask_selfAttention[idx, :, enc_lang_padding] = True\n",
        "            encoder_padding_mask_selfAttention[idx, enc_lang_padding, :] = True\n",
        "\n",
        "            decoder_padding_mask_selfAttention[idx, :, dec_lang_padding] = True\n",
        "            decoder_padding_mask_selfAttention[idx, dec_lang_padding, :] = True\n",
        "\n",
        "            decoder_padding_mask_crossAttention[idx, :, enc_lang_padding] = True\n",
        "            # decoder_padding_mask_crossAttention[idx, dec_lang_padding, :] = True # Remove the line masking decoder padding in cross-attention.\n",
        "            # It is redundant and incorrect; only encoder padding should be masked there.\n",
        "\n",
        "          encoder_mask_selfAttention = torch.where(encoder_padding_mask_selfAttention, NEG_INF, 0)\n",
        "          decoder_mask_selfAttention = torch.where(decoder_padding_mask_selfAttention | self.look_ahead_mask, NEG_INF, 0)\n",
        "          decoder_mask_crossAttention = torch.where(decoder_padding_mask_crossAttention, NEG_INF, 0)\n",
        "\n",
        "          encoder_mask_selfAttention = encoder_mask_selfAttention.unsqueeze(1)\n",
        "          decoder_mask_selfAttention = decoder_mask_selfAttention.unsqueeze(1)\n",
        "          decoder_mask_crossAttention = decoder_mask_crossAttention.unsqueeze(1)\n",
        "\n",
        "          return encoder_mask_selfAttention.to(get_device()), decoder_mask_selfAttention.to(get_device()), decoder_mask_crossAttention.to(get_device())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qR99YlSFgbq"
      },
      "source": [
        "## **Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H5LN7ymAnC_"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2R3KbnXAt6f"
      },
      "outputs": [],
      "source": [
        "# Multi-head attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # d_model\n",
        "        self.num_heads = num_heads # num_heads\n",
        "        self.head_dim = d_model // num_heads # head_dim\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model) # d_model x 3*d_model\n",
        "        self.linear_layer = nn.Linear(d_model, d_model) # d_model\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, max_sequence_length, d_model = x.size()  # Batch x max_seq_len, d_model\n",
        "        qkv = self.qkv_layer(x) # Batch x max_seq_len x 3*d_model\n",
        "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim) # Batch x max_seq_len x num_heads x 3*head_dim\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # Batch x num_heads x max_seq_len x 3*head_dim\n",
        "        q, k, v = qkv.chunk(3, dim=-1) # Batch x num_heads x max_seq_len x head_dim each\n",
        "        values, attention = self.scaled_dot_product(q, k, v, mask) # Batch x num_heads x max_seq_len x head_dim\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim) # value = Batch x max_seq_len x d_model, attention = Batch x num_heads x max_seq_len x max_seq_len\n",
        "        out = self.linear_layer(values) # Batch x max_seq_len x d_model\n",
        "        return out\n",
        "\n",
        "    def scaled_dot_product(self, q, k, v, mask=None):\n",
        "        # q, k, v are Batch x num_heads x max_seq_len x head_dim each\n",
        "        d_k = q.size()[-1] # head_dim\n",
        "        scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k) # Batch x num_heads x max_seq_len x max_seq_len\n",
        "        if mask is not None:\n",
        "            scaled += mask\n",
        "        attention = F.softmax(scaled, dim=-1) # Batch x num_heads x max_seq_len x max_seq_len\n",
        "        values = torch.matmul(attention, v) # Batch x num_heads x max_seq_len x head_dim\n",
        "        return values, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjN8xd7GNoCr"
      },
      "source": [
        "### Multi-Head Cross Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt97hPRHLMfc"
      },
      "outputs": [],
      "source": [
        "# MultiHead Cross Attention\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "      super().__init__()\n",
        "      self.d_model = d_model\n",
        "      self.num_heads = num_heads\n",
        "      self.head_dim = d_model // num_heads\n",
        "      self.kv_layer = nn.Linear(d_model , 2 * d_model) # 1024\n",
        "      self.q_layer = nn.Linear(d_model , d_model)\n",
        "      self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, x, y, mask=None):\n",
        "      batch_size, sequence_length, d_model = x.size() # Batch x max_seq_len x d_model\n",
        "      kv = self.kv_layer(x) # Batch x max_seq_len x 1024 # Output of Encoder to generate the keys and values\n",
        "      q = self.q_layer(y) # Batch x max_seq_len x d_model\n",
        "      kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)  # Batch x max_seq_len x num_heads x 2*head_dim\n",
        "      q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)  # Batch x max_seq_len x num_heads x head_dim\n",
        "      kv = kv.permute(0, 2, 1, 3) # Batch x num_heads x max_seq_len x 2*head_dim\n",
        "      q = q.permute(0, 2, 1, 3) # Batch x num_heads x max_seq_len x head_dim\n",
        "      k, v = kv.chunk(2, dim=-1) # K: Batch x num_heads x max_seq_len x head_dim, v: Batch x num_heads x max_seq_len x head_dim\n",
        "      values, attention = scaled_dot_product(q, k, v, mask) #  Batch x num_heads x max_seq_len x head_dim\n",
        "      values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model) #  Batch x max_seq_len x d_model\n",
        "      out = self.linear_layer(values)  #  Batch x max_seq_len x d_model\n",
        "      return out  #  Batch x max_seq_len x d_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3XhGlnCAy21"
      },
      "source": [
        "### Layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyoLnIk1A0uF"
      },
      "outputs": [],
      "source": [
        "# Layer Normalization\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape # d_model\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape)) # d_model\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape)) # d_model\n",
        "\n",
        "    def forward(self, inputs): # Batch x max_seq_len x d_model\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))] # [-1]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True) # Batch x max_seq_len x 1 (1 bcs of keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True) # Batch x max_seq_len x 1\n",
        "        std = (var + self.eps).sqrt() # Batch x max_seq_len x d_model\n",
        "        y = (inputs - mean) / std # Batch x max_seq_len x d_model\n",
        "        out = self.gamma * y  + self.beta # Batch x max_seq_len x d_model\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4QmiEOTA47w"
      },
      "source": [
        "### Position-wise FeedForward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn_P1vsbA-rX"
      },
      "outputs": [],
      "source": [
        "# Postion-wise Feedforward\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__() # After python 3, you can remove the class name and self arguments from super, just write super().__init__() like we did in the classes above\n",
        "        self.linear1 = nn.Linear(d_model, hidden) # d_model x 2045\n",
        "        self.linear2 = nn.Linear(hidden, d_model) # 2048 x d_model\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x): # Batch x max_seq_len x d_model\n",
        "        x = self.linear1(x) # Batch x max_seq_len x 2048\n",
        "        x = self.relu(x) # Batch x max_seq_len x 2048\n",
        "        x = self.dropout(x) # Batch x max_seq_len x 2048\n",
        "        x = self.linear2(x) # Batch x max_seq_len x d_model\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yGGGtmx7FjA"
      },
      "source": [
        "## **Encoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7OGrBzp8xer"
      },
      "source": [
        "### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6X7LxrT-8wwj"
      },
      "outputs": [],
      "source": [
        "# Single Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "    super(EncoderLayer, self).__init__() # After python 3, you can remove the class name and self arguments from super, just write super().__init__() like we did in the classes above\n",
        "    self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "    self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "    self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "      residual_x = x # Batch x max_seq_len x d_model\n",
        "      x = self.attention(x, mask=mask) # Batch x max_seq_len x d_model\n",
        "      x = self.dropout1(x) # Batch x max_seq_len x d_model\n",
        "      x = self.norm1(x + residual_x) # Batch x max_seq_len x d_model\n",
        "      residual_x = x # Batch x max_seq_len x d_model\n",
        "      x = self.ffn(x) # Batch x max_seq_len x d_model\n",
        "      x = self.dropout2(x) # Batch x max_seq_len x d_model\n",
        "      x = self.norm2(x + residual_x) # Batch x max_seq_len x d_model\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJBdfQG8h--R"
      },
      "source": [
        "### **Sequential Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLLoQiIJiB1w"
      },
      "outputs": [],
      "source": [
        "class SequentialEncoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, self_attention_mask  = inputs\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, self_attention_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5laQadOM4BEV"
      },
      "source": [
        "### Full Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt-KibJ37FFL"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers,max_sequence_length,language_to_index,START_TOKEN,END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, self_attention_mask, start_token, end_token):\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "        x = self.layers(x, self_attention_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81jS1ltwLJug"
      },
      "source": [
        "## **Decoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAf7OcaBN894"
      },
      "source": [
        "  ### Decoder Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ney5tsINxtW"
      },
      "outputs": [],
      "source": [
        "# Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob,):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.multi_head_cross_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        residual_y = y # Batch x max_seq_len x d_model\n",
        "        y = self.self_attention(y, mask=self_attention_mask) # Batch x max_seq_len x d_model\n",
        "        y = self.dropout1(y) # Batch x max_seq_len x d_model\n",
        "        y = self.norm1(y + residual_y) # Batch x max_seq_len x d_model\n",
        "\n",
        "        residual_y = y # Batch x max_seq_len x d_model\n",
        "        y = self.multi_head_cross_attention(x, y, mask=cross_attention_mask) #Batch x max_seq_len x d_model\n",
        "        y = self.dropout2(y)\n",
        "        y = self.norm2(y + residual_y)  #Batch x max_seq_len x d_model\n",
        "\n",
        "        residual_y = y  #Batch x max_seq_len x d_model\n",
        "        y = self.ffn(y) #Batch x max_seq_len x d_model\n",
        "        y = self.dropout3(y) #Batch x max_seq_len x d_model\n",
        "        y = self.norm3(y + residual_y) #Batch x max_seq_len x d_model\n",
        "        return y #Batch x max_seq_len x d_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJUllyZiN31V"
      },
      "source": [
        "### Sequential Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz_UKt0HN6Sy"
      },
      "outputs": [],
      "source": [
        "# Sequential Decoder\n",
        "# We implement our own sequential because we want to pass more than one parameter x, y, and mask\n",
        "# and the normal sequential accepts only one, so this extends sequential\n",
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, self_attention_mask, cross_attention_mask) #Batch x max_seq_len x d_model # the output y is passed to the next decoder layer and so on\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w9Q_VhLN0HI"
      },
      "source": [
        "### Full Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXJ4SgLwOCm7"
      },
      "outputs": [],
      "source": [
        "# Full Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers,max_sequence_length, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "        #x : Input (output of decoder):  Batch x max_seq_len x d_model\n",
        "        #y : Previous Decoder output(ground-truth for first decoder) :  Batch x max_seq_len x d_model\n",
        "        #mask : max_seq_len x max_seq_len\n",
        "        y = self.sentence_embedding(y, start_token, end_token)\n",
        "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y # Batch x max_seq_len x d_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBzRQj1glUK0"
      },
      "source": [
        "## **Full Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mP0pHM-Bps2U"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                d_model,\n",
        "                ffn_hidden,\n",
        "                num_heads,\n",
        "                drop_prob,\n",
        "                num_layers,\n",
        "                max_sequence_length,\n",
        "                out_lang_vocab_size,\n",
        "                input_language_to_index,\n",
        "                output_language_to_index,\n",
        "                START_TOKEN,\n",
        "                END_TOKEN,\n",
        "                PADDING_TOKEN\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.create_masks = Masks(max_sequence_length)\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, input_language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, output_language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.linear = nn.Linear(d_model, out_lang_vocab_size)\n",
        "\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                y,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=True, # We should make this true\n",
        "                dec_end_token=False): # x, y are batch of sentences\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = self.create_masks(x, y, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7_A8_HrGhO7"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlcFJ77HGjdc",
        "outputId": "db2d68a5-258c-4104-f6d6-24e5b3f1d637"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"hf://datasets/salehalmansour/english-to-arabic-translate/en_ar_final.tsv\", sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vk43uDnIZzk",
        "outputId": "8b0b519a-2e7c-460d-95e4-2670aa387085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of null rows = 12\n",
            "\n",
            "Null count per column:\n",
            "en    12\n",
            "ar     0\n",
            "dtype: int64\n",
            "____________________________________________________________________________________________________\n",
            "Number of English nulls = 12\n",
            "Number of Arabic nulls = 0\n",
            "____________________________________________________________________________________________________\n",
            "Null rows:\n",
            "          en                           ar\n",
            "801677   NaN  >التدابير الرئيسية الحالية<\n",
            "801678   NaN  >التدابير الرئيسية الحالية<\n",
            "801679   NaN  >التدابير الرئيسية الحالية<\n",
            "1007977  NaN         >الأسلحة البيولوجية<\n",
            "1007978  NaN         >الأسلحة الكيميائية<\n",
            "1007979  NaN            >الأسلحة النووية<\n",
            "1007982  NaN         >الأسلحة البيولوجية<\n",
            "1007986  NaN         >الأسلحة الكيميائية<\n",
            "1007987  NaN            >الأسلحة النووية<\n",
            "1008001  NaN            >الأسلحة النووية<\n",
            "1008002  NaN              >وسائل إيصالها<\n",
            "1008003  NaN         >الأسلحة الكيميائية<\n"
          ]
        }
      ],
      "source": [
        "# Get all rows that contain at least one null value\n",
        "rows_with_null = df[df.isnull().any(axis=1)] # axis=1 to search through columns\n",
        "\n",
        "print(f'Number of null rows = {len(rows_with_null)}\\n')\n",
        "print(\"Null rows:\")\n",
        "print(rows_with_null)\n",
        "print(\"_\" * 100)\n",
        "\n",
        "print(\"Null count per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Nulls in specific columns\n",
        "en_null = df[df['en'].isnull()]\n",
        "ar_null = df[df['ar'].isnull()]\n",
        "\n",
        "print(\"_\" * 100)\n",
        "print(f'Number of English nulls = {len(en_null)}')\n",
        "print(f'Number of Arabic nulls = {len(ar_null)}')\n",
        "print(\"_\" * 100)\n",
        "\n",
        "\n",
        "# Drop rows with any null values\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf1lQ194GxK2",
        "outputId": "8454c63c-3722-4a08-d33e-20570ee48570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1325887\n",
            "1325887\n"
          ]
        }
      ],
      "source": [
        "# values without Nulls\n",
        "Arabic = df['ar']\n",
        "English = df['en']\n",
        "print(len(Arabic))\n",
        "print(len(English))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVQFE2OAsD92"
      },
      "outputs": [],
      "source": [
        "# Vocabulary\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "PADDING_TOKEN = '<PADDING>'\n",
        "\n",
        "arabic_voc = [\n",
        "     START_TOKEN, END_TOKEN, PADDING_TOKEN, ' ', '؟', '!', '.', ',',  #  punctuation\n",
        "    '0','1','2','3','4','5','6','7','8','9',  # digits\n",
        "    'ء', 'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', # letter\n",
        "    'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل',\n",
        "    'م', 'ن', 'ه', 'و', 'ى', 'ي']\n",
        "\n",
        "english_voc = [START_TOKEN, END_TOKEN, PADDING_TOKEN, ' ', '?', '!', '.', ',',  #  punctuation\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', # digits\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', # letter\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVwD3wm2x-5X",
        "outputId": "0a835152-190f-466e-97c6-1d4013d55cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'<START>': 0, '<END>': 1, '<PADDING>': 2, ' ': 3, '؟': 4, '!': 5, '.': 6, ',': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, 'ء': 18, 'آ': 19, 'أ': 20, 'ؤ': 21, 'إ': 22, 'ئ': 23, 'ا': 24, 'ب': 25, 'ة': 26, 'ت': 27, 'ث': 28, 'ج': 29, 'ح': 30, 'خ': 31, 'د': 32, 'ذ': 33, 'ر': 34, 'ز': 35, 'س': 36, 'ش': 37, 'ص': 38, 'ض': 39, 'ط': 40, 'ظ': 41, 'ع': 42, 'غ': 43, 'ف': 44, 'ق': 45, 'ك': 46, 'ل': 47, 'م': 48, 'ن': 49, 'ه': 50, 'و': 51, 'ى': 52, 'ي': 53}\n",
            "{'<START>': 0, '<END>': 1, '<PADDING>': 2, ' ': 3, '?': 4, '!': 5, '.': 6, ',': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, 'a': 18, 'b': 19, 'c': 20, 'd': 21, 'e': 22, 'f': 23, 'g': 24, 'h': 25, 'i': 26, 'j': 27, 'k': 28, 'l': 29, 'm': 30, 'n': 31, 'o': 32, 'p': 33, 'q': 34, 'r': 35, 's': 36, 't': 37, 'u': 38, 'v': 39, 'w': 40, 'x': 41, 'y': 42, 'z': 43}\n"
          ]
        }
      ],
      "source": [
        "# character to index maps\n",
        "arabic_to_index = {c:i for i, c in enumerate(arabic_voc)}\n",
        "english_to_index = {c:i for i, c in enumerate(english_voc)}\n",
        "index_to_arabic = {i:c for i, c in enumerate(arabic_voc)}\n",
        "index_to_english = {i:c for i, c in enumerate(english_voc)}\n",
        "print(arabic_to_index)\n",
        "print(english_to_index)\n",
        "print(index_to_arabic)\n",
        "print(index_to_english)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWohkh53gtJf",
        "outputId": "2ea74af1-47a3-44c6-a2b8-d9c74512d165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<START><END><PADDING>\\ ؟!\\.,0123456789ءآأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىي\n",
            "<START><END><PADDING>\\ \\?!\\.,0123456789abcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ],
      "source": [
        "# Convert vocab to string for regex (escape special characters)\n",
        "# re.escape(c) → ensures special characters like ?, ., ! are treated literally in regex\n",
        "arabic_chars = ''.join([re.escape(c) for c in arabic_voc])\n",
        "english_chars = ''.join([re.escape(c) for c in english_voc])\n",
        "print(arabic_chars)\n",
        "print(english_chars)\n",
        "\n",
        "# Optional Arabic normalization function\n",
        "def normalize_arabic(text):\n",
        "    text = str(text)\n",
        "    # remove diacritics (tashkeel) if present\n",
        "    text = re.sub(r'[\\u0617-\\u061A\\u064B-\\u0652]', '', text)\n",
        "    # normalize Alef variants\n",
        "    text = text.replace('آ', 'ا').replace('أ', 'ا').replace('إ', 'ا')\n",
        "    # normalize final Yaa\n",
        "    text = text.replace('ى', 'ي')\n",
        "    return text\n",
        "\n",
        "# Remvoe characters from Arabic column not in Arabic vocabulary\n",
        "# f\"[^{arabic_chars}]\" → matches any character NOT in vocabulary\n",
        "# re.sub(..., \"\", x) → removes all characters not in the list\n",
        "# Preprocessing Arabic\n",
        "Arabic = Arabic.apply(lambda x: re.sub(f\"[^{arabic_chars}]\", \"\", normalize_arabic(x)))\n",
        "\n",
        "# Preprocessing English\n",
        "# Remvoe characters from English column not in English vocabulary\n",
        "English = English.apply(lambda x: re.sub(f\"[^{english_chars}]\", \"\", str(x).lower()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSHZlpIG3M2x",
        "outputId": "1d1679fa-93c6-4265-8da0-99408e15e110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English vocabulary: ['<START>', '<END>', '<PADDING>', ' ', '?', '!', '.', ',', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "English vocab size = 44\n",
            "English to index map: {'<START>': 0, '<END>': 1, '<PADDING>': 2, ' ': 3, '?': 4, '!': 5, '.': 6, ',': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, 'a': 18, 'b': 19, 'c': 20, 'd': 21, 'e': 22, 'f': 23, 'g': 24, 'h': 25, 'i': 26, 'j': 27, 'k': 28, 'l': 29, 'm': 30, 'n': 31, 'o': 32, 'p': 33, 'q': 34, 'r': 35, 's': 36, 't': 37, 'u': 38, 'v': 39, 'w': 40, 'x': 41, 'y': 42, 'z': 43}\n",
            "__________\n",
            "Arabic vocabulary ['<START>', '<END>', '<PADDING>', ' ', '؟', '!', '.', ',', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'ء', 'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي']\n",
            "Arabic vocab size = 54\n",
            "Arabic to index map: {'<START>': 0, '<END>': 1, '<PADDING>': 2, ' ': 3, '؟': 4, '!': 5, '.': 6, ',': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, 'ء': 18, 'آ': 19, 'أ': 20, 'ؤ': 21, 'إ': 22, 'ئ': 23, 'ا': 24, 'ب': 25, 'ة': 26, 'ت': 27, 'ث': 28, 'ج': 29, 'ح': 30, 'خ': 31, 'د': 32, 'ذ': 33, 'ر': 34, 'ز': 35, 'س': 36, 'ش': 37, 'ص': 38, 'ض': 39, 'ط': 40, 'ظ': 41, 'ع': 42, 'غ': 43, 'ف': 44, 'ق': 45, 'ك': 46, 'ل': 47, 'م': 48, 'ن': 49, 'ه': 50, 'و': 51, 'ى': 52, 'ي': 53}\n",
            "__________\n",
            "Random English sentence:\n",
            "rule 8 transmission of provisional agenda\n",
            "It's Arabic Translation:\n",
            "المادة 8\n"
          ]
        }
      ],
      "source": [
        "# English vodab\n",
        "print(f'English vocabulary: {english_voc}')\n",
        "print(f'English vocab size = {len(english_voc)}')\n",
        "# English to index map\n",
        "print(f'English to index map: {english_to_index}')\n",
        "# Index to English map\n",
        "print(f'Index to English map: {index_to_english}')\n",
        "print(\"_\"*10)\n",
        "# Arabic vodab\n",
        "print(f'Arabic vocabulary {arabic_voc}')\n",
        "print(f'Arabic vocab size = {len(arabic_voc)}')\n",
        "# Arabic to index map\n",
        "print(f'Arabic to index map: {arabic_to_index}')\n",
        "# Index to Arabic map\n",
        "print(f'Index to Arabic map: {index_to_arabic}')\n",
        "print(\"_\"*10)\n",
        "\n",
        "# random_idx = np.random.randint(0, len(English))\n",
        "# print(f'Random English sentence:\\n{English.iloc[random_idx]}')\n",
        "# print(f'It\\'s Arabic Translation:\\n{Arabic.iloc[random_idx]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v8LCukI7wKs"
      },
      "outputs": [],
      "source": [
        "# Convert inot lists\n",
        "english = English.tolist()\n",
        "arabic = Arabic.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z78ZsQam8BQf",
        "outputId": "7443dd70-e360-412e-b708-6c9b95ab5607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1325887\n",
            "zuma rising\n",
            "صعود نجم زوما\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of examples: {len(english)}')\n",
        "\n",
        "random_idx = np.random.randint(0, len(English))\n",
        "print(f'Random English sentence:\\n{english[random_idx]}')\n",
        "print(f'It\\'s Arabic Translation:\\n{arabic[random_idx]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rURvS7cXKO5o",
        "outputId": "c95e8190-0c0e-4bca-a21e-efe7b1114da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 1325887\n",
            "Number of valid sentences: 1290630\n"
          ]
        }
      ],
      "source": [
        "# Set the max sentece length to be more than the length of 98% of th data\n",
        "english_98th_len  = np.percentile([ len(sentence) for sentence in english], 98)\n",
        "arabic_98th_len = np.percentile([ len(sentence) for sentence in arabic], 98)\n",
        "max_sequence_length  = min(english_98th_len, arabic_98th_len)\n",
        "\n",
        "\n",
        "# check of the sentence characters all in the vocabulary\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# check if sentnece is shorter than the max sentence length\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length)\n",
        "\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(arabic)):\n",
        "    arabic_sentence, english_sentence = arabic[index], english[index]\n",
        "    if is_valid_length(arabic_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(arabic_sentence, arabic_voc) and is_valid_tokens(english_sentence, english_vocab):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(arabic)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLf1-KAqyGhN"
      },
      "outputs": [],
      "source": [
        "# Only continue with valid sentences\n",
        "arabic_sentences = [arabic[i] for i in valid_sentence_indicies]\n",
        "english_sentences = [english[i] for i in valid_sentence_indicies]\n",
        "\n",
        "# Set a mximum number of examples\n",
        "TOTAL_SENTENCES = 1000000\n",
        "arabic_sentences = arabic_sentences[:TOTAL_SENTENCES]\n",
        "english_sentences = english_sentences[:TOTAL_SENTENCES]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzUwmQkz8qoH",
        "outputId": "10e390a5-7844-4e99-db69-05841069feca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, english_sentences, arabic_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.arabic_sentences = arabic_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.arabic_sentences[idx]\n",
        "\n",
        "\n",
        "dataset = TextDataset(english_sentences, arabic_sentences)\n",
        "# dataset[5:10]\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN0ipi8t-rjK",
        "outputId": "7eda2a89-cd5f-48ac-a215-63b426907c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('and this', 'it was um', 'what is she doing here', 'i dont like it', 'did you get the part', ' its none of your business', ' uhhuh', 'i was wrong', ' others', 'im much your majesty', 'do i make myself clear', 'thank you', 'i bet you do', 'thats enough', 'yeah of course', 'im him', 'you okay', 'no no', 'shes all yours', 'you want the truth', 'how about you', ' goodbye', ' thats okay', 'im sorry', 'i must go', 'maybe we have something in common', 'right alright alright relax', ' have a seat', 'hows it goin', ' are you all right', 'whats up', ' what did you do', 'what happened', 'is this all youve got', 'ma is the greatest he bought it and delivered it', ' that right', 'as in mother id like to fuck', ' ooh ooh ooh honey', 'what is your name', 'what was that', 'smells good', 'from what', 'it wasnt easy', 'and you', 'i dont think so', 'just take your time', ' over here', 'hell no', 'excuse me', 'what are you waiting for', 'miss prescott goodbye', 'like you', 'this is why youve got me out of bed', ' thats sad', 'what are you doing here', ' all right', 'how about everyone out here', 'i agree', 'number three', 'what was that you said', 'killian thank god', 'whoa zach', 'whats going on', 'all right gang', 'no no', 'why not', 'i can still do that', ' me neither', 'oh you know', 'i cant believe it', 'hes been here', ' suck it', 'can you control it', 'are you alone', 'its pathetic', 'thats a good question', 'thats all right', 'you were right', 'how may i help you', 'screw you abed', 'what do you want', 'fiftyfourth session', 'look at that', 'thank you', 'i do not know', 'i think not', 'what is going on', ' where have you been', 'one two three', 'see god', 'no not me', ' i cant', 'are you all right', 'note by the president of the security council', 'dont forget that', 'you may call me sam out of love', 'yes i did which means theres no commission in this for you', 'thats it', 'how are you doing', 'how do you figure that', 'what happened', ' no relief  we could stop right now if you want', 'i see it', 'lou please dont do that', 'someplace nice like a beautiful beach or like a mountain or', 'its been a while', 'she just left', 'i hope so', 'right master', 'theres more than that', 'here we go', 'im not sure i can', 'which reminds me', 'im here', 'thank you', 'you saw her', 'uh i gotta go', 'maybe i should go up there', 'tagged as lassen', 'what happened out there', 'excuse me', 'youre mean', ' would you mind', 'i have no idea', 'get up', 'its important', 'ooh i just found something', 'were clear')\n",
            "('و هذه؟', '...لقد كان', 'ما الذي تفعله هناك؟', 'لا احب ذلك', 'هل حصلت علي جزء ', 'هذا ليس من شانك', 'نعم .', 'كنت علي خطا', ' الاخرون؟', 'انا ماتش فخامتك', 'هل كلامي واضح؟', 'شكرا لكم.', 'اشك في ذلك', 'هذا يكفي !', 'اجل بالطبع.', 'انا هو', 'اانت بخير ؟', ' لا .. لا ..', 'انها لك', 'تريدين الحقيقة ؟', 'ماذا عنك؟', 'وداعا .', 'لا باس .', 'انا اسفة', 'انا يجب ان اذهب', 'ربما لدينا شيء مشترك.', 'حسناحسنا اهدا', 'تفضلي بالجلوس', 'كيف الحال؟', 'هل انت بخير؟', 'هنري مالذي يحدث', 'ماذا فعلت؟', 'ماذا حدث ؟', 'اهذا كل ما لديك؟', 'ما  الاعظم هو الذي اشتراها ', 'اهذا صحيح؟', 'اختصار ل ام اود مضجعتها ؟', 'اوة اوة عسلي', 'ما هو اسمك؟', 'ماذا كان هذا؟', 'رائحة طيبة', 'من ماذا ؟', 'لم يكن الامر سهلا.', 'و انت؟', 'لا اظن ذلك', 'خذ وقتك', 'من هنا', 'طبعا لا', 'بعد اذنك', 'ماذا تنتظر؟', 'الي اللقاء.', 'مثلك .', 'لهذا اخرجتني من السرير؟', 'هذا مؤسف .', ' ماذا تفعل هنا؟', ' حسنا', 'ماذا عنكم؟', 'اوافقك الراي .', 'الكلمة الثالثة', 'ماذا قلت ؟', 'كيليان شكرا يا الله', 'عجباه زاك.', 'ما الذي يحدث؟', 'حسنا يا رفاق', 'لا .', 'لما لا ؟', 'مازلت افعل ذلك', 'ولا انا', 'كما تعلمون', 'انا مش مصدقة ..', 'لقد كان هنا.', 'تبا لك', 'هل يمكنك ان تتحكمين بها ؟', 'اانت بمفردك؟', 'انت مثير للشفقة', 'هذا سؤال جيد', 'هذا حسن', 'لقد كنت علي حق.', 'كيف استطيع مساعدتك؟', 'تبا لك', 'ماذا تريدين؟', 'الدورة الرابعة والخمسون', 'انظري لهذا.', 'شكرا لكم.', 'انا لا اعلم', 'لا اعتقد هذا', 'ماذا يحدث هنا', ' اين كنت؟', 'واحد اثنان ثلاثة.', 'هل تري؟', 'لا  ليس معي', ' لا استطيع', 'هل انت بخير ؟', 'مذكرة من رئيس مجلس الامن', 'لا تنسي هذه النقطة', 'ما اسمك؟', 'تماما مما يعني لا عمولة لك', 'هذا كل شيء', ' كيف تعمل؟', 'كيف عرفت هذا؟', 'ماذا حدث؟', 'يمكننا ان نتوقف الان ان اردت.', 'اتفهم ذلك', 'ارجوك لا تفعل هذا', 'مثل شاطئ جميل او جبل', 'لقد مضت فترة', 'غادرت فقط', 'امل ذلك', 'صحيح , سيدي', 'هنالك المزيد', 'ها نحن اولاء', 'لا اقدر', 'وهذا يذكرني ..', 'انا هنا', 'تفضلوا من فضلكم شكرا', 'هل رايتها؟', 'علي الذهاب.', 'ربما علي الذهاب الي هناك', 'موسمة كلاسن؟', 'ما الذي حدث هناك؟', 'عفوا ؟', 'انت لئيم', 'هل تمانع؟', 'ليس لدي فكرة', 'انهض !', 'انه مهم', 'لقد وجدت شيئا', 'المكان امن.')\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)\n",
        "\n",
        "print(\"First 3 batches:\")\n",
        "for batch_num, batch in enumerate(iterator):\n",
        "  if batch_num>2:\n",
        "    break\n",
        "  batch_eng, batch_ar = batch\n",
        "  print(batch_eng)\n",
        "  print(batch_ar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HjPznwx8uPx",
        "outputId": "b6a07fc8-7b1b-4a1c-bfa6-3620f7e650bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('fifth committee', 'اللجنة الخامسة')\n"
          ]
        }
      ],
      "source": [
        "# split the dataset into train and test sets\n",
        "train_size = int(0.9 * len(dataset))   # 90% train\n",
        "test_size  = len(dataset) - train_size # 10% test\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "print(f'Size of training set: {len(train_dataset)}')\n",
        "print(f'Size of test set: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHGEQqsdTbGs"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIgYRRQ65P53"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "batch_size = 30\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 80\n",
        "ar_vocab_size = len(arabic_voc)\n",
        "batch_size = 128\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_rQS2A69o3C"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_layers,\n",
        "                          max_sequence_length,\n",
        "                          ar_vocab_size,\n",
        "                          english_to_index,\n",
        "                          arabic_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yOhzB7Wk-zx8",
        "outputId": "5a59d2b8-ba3d-490d-b0f8-2e03361ade85"
      },
      "outputs": [],
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwL86xxPTdM9"
      },
      "outputs": [],
      "source": [
        "def train_transformer(transformer, dataset, batch_size=128, num_epochs, lr=1e-4, device=None):\n",
        "    # check which device is available\n",
        "    if device is None:\n",
        "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    transformer.to(device)\n",
        "\n",
        "    # Load the training dataset\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    PADDING_TOKEN = transformer.encoder.sentence_embedding.PADDING_TOKEN\n",
        "    token_to_index = transformer.encoder.sentence_embedding.language_to_index\n",
        "\n",
        "    # Loss function ignoring padding\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=token_to_index[PADDING_TOKEN], reduction = 'none') # none to get the loss of each sentence, set it to mean to get the mean loss of the batch (a single scaler)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(transformer.parameters(), lr=lr)\n",
        "\n",
        "    # Xavier initialization for weights > 1D\n",
        "    for param in transformer.parameters():\n",
        "        if param.dim() > 1:\n",
        "            nn.init.xavier_uniform_(param)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"______________________________ Epoch {epoch + 1}/{num_epochs} ______________________________\")\n",
        "        transformer.train()\n",
        "\n",
        "        for batch_num, batch in enumerate(train_loader):\n",
        "            # Unpack batch and ensure each sentence is a list\n",
        "            src_sentences, tgt_sentences = batch\n",
        "            src_sentences = list(src_sentences)\n",
        "            tgt_sentences = list(tgt_sentences)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = transformer(\n",
        "                src_sentences,\n",
        "                tgt_sentences,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=True,\n",
        "                dec_end_token=True\n",
        "            )\n",
        "\n",
        "            # Tokenize target sentences for loss (the ground truth)\n",
        "            labels = transformer.decoder.sentence_embedding.batch_tokenize(\n",
        "                tgt_sentences, start_token=False, end_token=True\n",
        "            ).to(device)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(predictions.view(-1, predictions.size(-1)), labels.view(-1))\n",
        "            # Compute mean loss without the padding tokens\n",
        "            valid_indicies = torch.where(labels.view(-1) == token_to_index[PADDING_TOKEN], False, True)\n",
        "            # Compute average loss through the batch\n",
        "            loss = loss.sum() / valid_indicies.sum()\n",
        "            # Or also compute the mean using:\n",
        "            # loss_value = loss.sum() / (labels.view(-1) != padding_idx).sum()\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            if (batch_num + 1) % 100 == 0 or (batch_num + 1) == len(train_loader):\n",
        "                print(f\"Epoch {epoch + 1}, Batch {batch_num + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2XXT8e6xDXmI",
        "outputId": "8c74ded0-9c91-406b-c811-6f00600db44a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch 6400/7032, Loss: 1.1972\n",
            "Batch 6410/7032, Loss: 1.1978\n",
            "Batch 6420/7032, Loss: 1.1237\n",
            "Batch 6430/7032, Loss: 1.2404\n",
            "Batch 6440/7032, Loss: 1.2087\n",
            "Batch 6450/7032, Loss: 1.1822\n",
            "Batch 6460/7032, Loss: 1.2713\n",
            "Batch 6470/7032, Loss: 1.1681\n",
            "Batch 6480/7032, Loss: 1.2210\n",
            "Batch 6490/7032, Loss: 1.1638\n",
            "Batch 6500/7032, Loss: 1.1429\n",
            "Batch 6510/7032, Loss: 1.1796\n",
            "Batch 6520/7032, Loss: 1.2902\n",
            "Batch 6530/7032, Loss: 1.2279\n",
            "Batch 6540/7032, Loss: 1.2637\n",
            "Batch 6550/7032, Loss: 1.1877\n",
            "Batch 6560/7032, Loss: 1.2156\n",
            "Batch 6570/7032, Loss: 1.2784\n",
            "Batch 6580/7032, Loss: 1.3165\n",
            "Batch 6590/7032, Loss: 1.2908\n",
            "Batch 6600/7032, Loss: 1.3150\n",
            "Batch 6610/7032, Loss: 1.1442\n",
            "Batch 6620/7032, Loss: 1.1963\n",
            "Batch 6630/7032, Loss: 1.0852\n",
            "Batch 6640/7032, Loss: 1.0901\n",
            "Batch 6650/7032, Loss: 1.1132\n",
            "Batch 6660/7032, Loss: 1.3150\n",
            "Batch 6670/7032, Loss: 1.2079\n",
            "Batch 6680/7032, Loss: 1.1327\n",
            "Batch 6690/7032, Loss: 1.1520\n",
            "Batch 6700/7032, Loss: 1.2450\n",
            "Batch 6710/7032, Loss: 1.1499\n",
            "Batch 6720/7032, Loss: 1.3289\n",
            "Batch 6730/7032, Loss: 1.1387\n",
            "Batch 6740/7032, Loss: 1.1129\n",
            "Batch 6750/7032, Loss: 1.0396\n",
            "Batch 6760/7032, Loss: 1.1891\n",
            "Batch 6770/7032, Loss: 1.1711\n",
            "Batch 6780/7032, Loss: 1.2687\n",
            "Batch 6790/7032, Loss: 1.1363\n",
            "Batch 6800/7032, Loss: 1.1914\n",
            "Batch 6810/7032, Loss: 1.2391\n",
            "Batch 6820/7032, Loss: 1.2930\n",
            "Batch 6830/7032, Loss: 1.0514\n",
            "Batch 6840/7032, Loss: 1.1928\n",
            "Batch 6850/7032, Loss: 1.1208\n",
            "Batch 6860/7032, Loss: 1.1816\n",
            "Batch 6870/7032, Loss: 1.3087\n",
            "Batch 6880/7032, Loss: 1.2389\n",
            "Batch 6890/7032, Loss: 1.2009\n",
            "Batch 6900/7032, Loss: 1.1775\n",
            "Batch 6910/7032, Loss: 1.2310\n",
            "Batch 6920/7032, Loss: 1.1927\n",
            "Batch 6930/7032, Loss: 1.1824\n",
            "Batch 6940/7032, Loss: 1.2495\n",
            "Batch 6950/7032, Loss: 1.1602\n",
            "Batch 6960/7032, Loss: 1.2318\n",
            "Batch 6970/7032, Loss: 1.1141\n",
            "Batch 6980/7032, Loss: 1.1461\n",
            "Batch 6990/7032, Loss: 1.1624\n",
            "Batch 7000/7032, Loss: 1.2606\n",
            "Batch 7010/7032, Loss: 1.1270\n",
            "Batch 7020/7032, Loss: 1.1708\n",
            "Batch 7030/7032, Loss: 1.1755\n",
            "Batch 7032/7032, Loss: 1.0114\n",
            "Epoch 4/10\n",
            "Batch 10/7032, Loss: 1.0816\n",
            "Batch 20/7032, Loss: 1.1553\n",
            "Batch 30/7032, Loss: 1.1060\n",
            "Batch 40/7032, Loss: 1.2415\n",
            "Batch 50/7032, Loss: 1.3183\n",
            "Batch 60/7032, Loss: 1.2462\n",
            "Batch 70/7032, Loss: 1.1320\n",
            "Batch 80/7032, Loss: 1.2477\n",
            "Batch 90/7032, Loss: 1.0505\n",
            "Batch 100/7032, Loss: 1.2064\n",
            "Batch 110/7032, Loss: 1.2506\n",
            "Batch 120/7032, Loss: 1.1365\n",
            "Batch 130/7032, Loss: 1.0647\n",
            "Batch 140/7032, Loss: 1.0591\n",
            "Batch 150/7032, Loss: 1.1221\n",
            "Batch 160/7032, Loss: 1.1597\n",
            "Batch 170/7032, Loss: 1.1047\n",
            "Batch 180/7032, Loss: 1.1982\n",
            "Batch 190/7032, Loss: 1.2096\n",
            "Batch 200/7032, Loss: 1.2005\n",
            "Batch 210/7032, Loss: 1.1123\n",
            "Batch 220/7032, Loss: 1.1431\n",
            "Batch 230/7032, Loss: 1.2222\n",
            "Batch 240/7032, Loss: 1.1869\n",
            "Batch 250/7032, Loss: 1.1926\n",
            "Batch 260/7032, Loss: 1.0988\n",
            "Batch 270/7032, Loss: 1.2002\n",
            "Batch 280/7032, Loss: 1.2083\n",
            "Batch 290/7032, Loss: 1.1191\n",
            "Batch 300/7032, Loss: 1.2204\n",
            "Batch 310/7032, Loss: 1.0838\n",
            "Batch 320/7032, Loss: 1.2508\n",
            "Batch 330/7032, Loss: 1.2155\n",
            "Batch 340/7032, Loss: 1.2645\n",
            "Batch 350/7032, Loss: 1.1699\n",
            "Batch 360/7032, Loss: 1.1212\n",
            "Batch 370/7032, Loss: 1.2160\n",
            "Batch 380/7032, Loss: 1.1567\n",
            "Batch 390/7032, Loss: 1.0544\n",
            "Batch 400/7032, Loss: 1.1971\n",
            "Batch 410/7032, Loss: 1.2104\n",
            "Batch 420/7032, Loss: 1.1667\n",
            "Batch 430/7032, Loss: 1.0971\n",
            "Batch 440/7032, Loss: 1.1240\n",
            "Batch 450/7032, Loss: 1.2252\n",
            "Batch 460/7032, Loss: 1.0387\n",
            "Batch 470/7032, Loss: 1.1920\n",
            "Batch 480/7032, Loss: 1.1398\n",
            "Batch 490/7032, Loss: 1.1736\n",
            "Batch 500/7032, Loss: 1.1785\n",
            "Batch 510/7032, Loss: 1.1779\n",
            "Batch 520/7032, Loss: 1.1966\n",
            "Batch 530/7032, Loss: 1.1720\n",
            "Batch 540/7032, Loss: 1.1752\n",
            "Batch 550/7032, Loss: 1.3181\n",
            "Batch 560/7032, Loss: 1.0936\n",
            "Batch 570/7032, Loss: 1.2606\n",
            "Batch 580/7032, Loss: 1.3029\n",
            "Batch 590/7032, Loss: 1.0282\n",
            "Batch 600/7032, Loss: 1.1847\n",
            "Batch 610/7032, Loss: 1.1397\n",
            "Batch 620/7032, Loss: 1.3113\n",
            "Batch 630/7032, Loss: 1.1862\n",
            "Batch 640/7032, Loss: 0.9704\n",
            "Batch 650/7032, Loss: 1.1127\n",
            "Batch 660/7032, Loss: 1.1564\n",
            "Batch 670/7032, Loss: 1.0791\n",
            "Batch 680/7032, Loss: 1.2719\n",
            "Batch 690/7032, Loss: 1.1532\n",
            "Batch 700/7032, Loss: 1.0768\n",
            "Batch 710/7032, Loss: 1.0919\n",
            "Batch 720/7032, Loss: 1.1475\n",
            "Batch 730/7032, Loss: 1.1314\n",
            "Batch 740/7032, Loss: 1.1549\n",
            "Batch 750/7032, Loss: 1.2696\n",
            "Batch 760/7032, Loss: 1.2406\n",
            "Batch 770/7032, Loss: 1.1832\n",
            "Batch 780/7032, Loss: 1.0895\n",
            "Batch 790/7032, Loss: 1.0967\n",
            "Batch 800/7032, Loss: 1.0309\n",
            "Batch 810/7032, Loss: 1.0717\n",
            "Batch 820/7032, Loss: 1.0987\n",
            "Batch 830/7032, Loss: 1.0761\n",
            "Batch 840/7032, Loss: 1.0918\n",
            "Batch 850/7032, Loss: 1.2159\n",
            "Batch 860/7032, Loss: 1.1742\n",
            "Batch 870/7032, Loss: 1.1773\n",
            "Batch 880/7032, Loss: 1.2832\n",
            "Batch 890/7032, Loss: 1.2666\n",
            "Batch 900/7032, Loss: 1.1984\n",
            "Batch 910/7032, Loss: 1.1650\n",
            "Batch 920/7032, Loss: 1.1729\n",
            "Batch 930/7032, Loss: 1.1841\n",
            "Batch 940/7032, Loss: 1.2215\n",
            "Batch 950/7032, Loss: 1.1951\n",
            "Batch 960/7032, Loss: 1.1561\n",
            "Batch 970/7032, Loss: 1.1372\n",
            "Batch 980/7032, Loss: 1.0993\n",
            "Batch 990/7032, Loss: 1.1053\n",
            "Batch 1000/7032, Loss: 1.2194\n",
            "Batch 1010/7032, Loss: 1.1423\n",
            "Batch 1020/7032, Loss: 1.0856\n",
            "Batch 1030/7032, Loss: 1.2437\n",
            "Batch 1040/7032, Loss: 1.0971\n",
            "Batch 1050/7032, Loss: 1.1606\n",
            "Batch 1060/7032, Loss: 1.1011\n",
            "Batch 1070/7032, Loss: 1.2257\n",
            "Batch 1080/7032, Loss: 1.2012\n",
            "Batch 1090/7032, Loss: 1.1917\n",
            "Batch 1100/7032, Loss: 1.2890\n",
            "Batch 1110/7032, Loss: 1.1525\n",
            "Batch 1120/7032, Loss: 1.3007\n",
            "Batch 1130/7032, Loss: 1.0870\n",
            "Batch 1140/7032, Loss: 1.2076\n",
            "Batch 1150/7032, Loss: 1.2052\n",
            "Batch 1160/7032, Loss: 1.3754\n",
            "Batch 1170/7032, Loss: 1.2012\n",
            "Batch 1180/7032, Loss: 1.1910\n",
            "Batch 1190/7032, Loss: 1.1619\n",
            "Batch 1200/7032, Loss: 1.2761\n",
            "Batch 1210/7032, Loss: 1.2167\n",
            "Batch 1220/7032, Loss: 1.1155\n",
            "Batch 1230/7032, Loss: 1.2353\n",
            "Batch 1240/7032, Loss: 1.1570\n",
            "Batch 1250/7032, Loss: 1.2791\n",
            "Batch 1260/7032, Loss: 1.2100\n",
            "Batch 1270/7032, Loss: 1.0940\n",
            "Batch 1280/7032, Loss: 1.1002\n",
            "Batch 1290/7032, Loss: 1.2016\n",
            "Batch 1300/7032, Loss: 1.0732\n",
            "Batch 1310/7032, Loss: 1.1582\n",
            "Batch 1320/7032, Loss: 1.1898\n",
            "Batch 1330/7032, Loss: 1.1783\n",
            "Batch 1340/7032, Loss: 1.1791\n",
            "Batch 1350/7032, Loss: 1.1085\n",
            "Batch 1360/7032, Loss: 1.2538\n",
            "Batch 1370/7032, Loss: 1.1835\n",
            "Batch 1380/7032, Loss: 1.0906\n",
            "Batch 1390/7032, Loss: 1.2343\n",
            "Batch 1400/7032, Loss: 1.1507\n",
            "Batch 1410/7032, Loss: 1.2081\n",
            "Batch 1420/7032, Loss: 1.2048\n",
            "Batch 1430/7032, Loss: 1.2052\n",
            "Batch 1440/7032, Loss: 1.2530\n",
            "Batch 1450/7032, Loss: 1.0787\n",
            "Batch 1460/7032, Loss: 1.1562\n",
            "Batch 1470/7032, Loss: 1.0871\n",
            "Batch 1480/7032, Loss: 1.0863\n",
            "Batch 1490/7032, Loss: 1.1729\n",
            "Batch 1500/7032, Loss: 1.2558\n",
            "Batch 1510/7032, Loss: 1.0151\n",
            "Batch 1520/7032, Loss: 1.2131\n",
            "Batch 1530/7032, Loss: 1.1229\n",
            "Batch 1540/7032, Loss: 1.0377\n",
            "Batch 1550/7032, Loss: 1.1332\n",
            "Batch 1560/7032, Loss: 1.1654\n",
            "Batch 1570/7032, Loss: 1.1858\n",
            "Batch 1580/7032, Loss: 1.0601\n",
            "Batch 1590/7032, Loss: 1.0307\n",
            "Batch 1600/7032, Loss: 1.2239\n",
            "Batch 1610/7032, Loss: 1.0246\n",
            "Batch 1620/7032, Loss: 1.2308\n",
            "Batch 1630/7032, Loss: 1.1527\n",
            "Batch 1640/7032, Loss: 1.2299\n",
            "Batch 1650/7032, Loss: 1.2518\n",
            "Batch 1660/7032, Loss: 1.2004\n",
            "Batch 1670/7032, Loss: 1.0913\n",
            "Batch 1680/7032, Loss: 1.1304\n",
            "Batch 1690/7032, Loss: 1.1262\n",
            "Batch 1700/7032, Loss: 1.1364\n",
            "Batch 1710/7032, Loss: 1.2180\n",
            "Batch 1720/7032, Loss: 1.1754\n",
            "Batch 1730/7032, Loss: 1.2514\n",
            "Batch 1740/7032, Loss: 1.1666\n",
            "Batch 1750/7032, Loss: 1.1286\n",
            "Batch 1760/7032, Loss: 1.1917\n",
            "Batch 1770/7032, Loss: 1.2659\n",
            "Batch 1780/7032, Loss: 1.1547\n",
            "Batch 1790/7032, Loss: 1.2082\n",
            "Batch 1800/7032, Loss: 1.1597\n",
            "Batch 1810/7032, Loss: 1.1139\n",
            "Batch 1820/7032, Loss: 1.0803\n",
            "Batch 1830/7032, Loss: 1.1271\n",
            "Batch 1840/7032, Loss: 1.2036\n",
            "Batch 1850/7032, Loss: 1.1230\n",
            "Batch 1860/7032, Loss: 1.3258\n",
            "Batch 1870/7032, Loss: 1.0282\n",
            "Batch 1880/7032, Loss: 1.1600\n",
            "Batch 1890/7032, Loss: 1.0912\n",
            "Batch 1900/7032, Loss: 1.1962\n",
            "Batch 1910/7032, Loss: 1.1884\n",
            "Batch 1920/7032, Loss: 1.0042\n",
            "Batch 1930/7032, Loss: 1.0022\n",
            "Batch 1940/7032, Loss: 1.1303\n",
            "Batch 1950/7032, Loss: 1.1269\n",
            "Batch 1960/7032, Loss: 0.9579\n",
            "Batch 1970/7032, Loss: 1.2178\n",
            "Batch 1980/7032, Loss: 1.0281\n",
            "Batch 1990/7032, Loss: 1.2627\n",
            "Batch 2000/7032, Loss: 1.1048\n",
            "Batch 2010/7032, Loss: 1.3054\n",
            "Batch 2020/7032, Loss: 1.1495\n",
            "Batch 2030/7032, Loss: 1.2007\n",
            "Batch 2040/7032, Loss: 1.2532\n",
            "Batch 2050/7032, Loss: 1.0827\n",
            "Batch 2060/7032, Loss: 1.1430\n",
            "Batch 2070/7032, Loss: 1.1252\n",
            "Batch 2080/7032, Loss: 1.0447\n",
            "Batch 2090/7032, Loss: 1.2009\n",
            "Batch 2100/7032, Loss: 1.1832\n",
            "Batch 2110/7032, Loss: 1.1147\n",
            "Batch 2120/7032, Loss: 1.1237\n",
            "Batch 2130/7032, Loss: 1.2676\n",
            "Batch 2140/7032, Loss: 1.0932\n",
            "Batch 2150/7032, Loss: 1.1172\n",
            "Batch 2160/7032, Loss: 1.2054\n",
            "Batch 2170/7032, Loss: 1.1065\n",
            "Batch 2180/7032, Loss: 1.1974\n",
            "Batch 2190/7032, Loss: 1.0680\n",
            "Batch 2200/7032, Loss: 1.0765\n",
            "Batch 2210/7032, Loss: 1.2156\n",
            "Batch 2220/7032, Loss: 1.1831\n",
            "Batch 2230/7032, Loss: 1.1326\n",
            "Batch 2240/7032, Loss: 1.0812\n",
            "Batch 2250/7032, Loss: 1.2329\n",
            "Batch 2260/7032, Loss: 1.2437\n",
            "Batch 2270/7032, Loss: 1.2228\n",
            "Batch 2280/7032, Loss: 1.1678\n",
            "Batch 2290/7032, Loss: 1.0646\n",
            "Batch 2300/7032, Loss: 1.2382\n",
            "Batch 2310/7032, Loss: 1.0705\n",
            "Batch 2320/7032, Loss: 1.1041\n",
            "Batch 2330/7032, Loss: 1.2216\n",
            "Batch 2340/7032, Loss: 1.0098\n",
            "Batch 2350/7032, Loss: 1.0657\n",
            "Batch 2360/7032, Loss: 1.0483\n",
            "Batch 2370/7032, Loss: 1.3761\n",
            "Batch 2380/7032, Loss: 1.2188\n",
            "Batch 2390/7032, Loss: 1.2549\n",
            "Batch 2400/7032, Loss: 1.2468\n",
            "Batch 2410/7032, Loss: 1.2254\n",
            "Batch 2420/7032, Loss: 1.1801\n",
            "Batch 2430/7032, Loss: 1.1672\n",
            "Batch 2440/7032, Loss: 1.0925\n",
            "Batch 2450/7032, Loss: 1.1419\n",
            "Batch 2460/7032, Loss: 1.1787\n",
            "Batch 2470/7032, Loss: 1.1891\n",
            "Batch 2480/7032, Loss: 1.1581\n",
            "Batch 2490/7032, Loss: 1.1007\n",
            "Batch 2500/7032, Loss: 1.2308\n",
            "Batch 2510/7032, Loss: 1.1545\n",
            "Batch 2520/7032, Loss: 1.0554\n",
            "Batch 2530/7032, Loss: 1.1276\n",
            "Batch 2540/7032, Loss: 1.0709\n",
            "Batch 2550/7032, Loss: 1.2765\n",
            "Batch 2560/7032, Loss: 1.1053\n",
            "Batch 2570/7032, Loss: 1.1152\n",
            "Batch 2580/7032, Loss: 1.1135\n",
            "Batch 2590/7032, Loss: 1.1600\n",
            "Batch 2600/7032, Loss: 1.0604\n",
            "Batch 2610/7032, Loss: 1.1185\n",
            "Batch 2620/7032, Loss: 1.1553\n",
            "Batch 2630/7032, Loss: 1.2736\n",
            "Batch 2640/7032, Loss: 1.1002\n",
            "Batch 2650/7032, Loss: 1.1643\n",
            "Batch 2660/7032, Loss: 1.2190\n",
            "Batch 2670/7032, Loss: 1.0677\n",
            "Batch 2680/7032, Loss: 1.2067\n",
            "Batch 2690/7032, Loss: 1.2504\n",
            "Batch 2700/7032, Loss: 1.1888\n",
            "Batch 2710/7032, Loss: 1.1078\n",
            "Batch 2720/7032, Loss: 1.2407\n",
            "Batch 2730/7032, Loss: 1.0592\n",
            "Batch 2740/7032, Loss: 1.1977\n",
            "Batch 2750/7032, Loss: 1.1312\n",
            "Batch 2760/7032, Loss: 1.0985\n",
            "Batch 2770/7032, Loss: 1.0983\n",
            "Batch 2780/7032, Loss: 1.2167\n",
            "Batch 2790/7032, Loss: 1.0267\n",
            "Batch 2800/7032, Loss: 1.0429\n",
            "Batch 2810/7032, Loss: 1.2321\n",
            "Batch 2820/7032, Loss: 1.0750\n",
            "Batch 2830/7032, Loss: 1.0320\n",
            "Batch 2840/7032, Loss: 1.2190\n",
            "Batch 2850/7032, Loss: 1.1610\n",
            "Batch 2860/7032, Loss: 1.0695\n",
            "Batch 2870/7032, Loss: 1.1834\n",
            "Batch 2880/7032, Loss: 1.1467\n",
            "Batch 2890/7032, Loss: 1.1344\n",
            "Batch 2900/7032, Loss: 1.2470\n",
            "Batch 2910/7032, Loss: 1.2967\n",
            "Batch 2920/7032, Loss: 1.0839\n",
            "Batch 2930/7032, Loss: 1.1335\n",
            "Batch 2940/7032, Loss: 1.1942\n",
            "Batch 2950/7032, Loss: 1.1920\n",
            "Batch 2960/7032, Loss: 1.2488\n",
            "Batch 2970/7032, Loss: 1.2188\n",
            "Batch 2980/7032, Loss: 1.1580\n",
            "Batch 2990/7032, Loss: 1.1706\n",
            "Batch 3000/7032, Loss: 1.0358\n",
            "Batch 3010/7032, Loss: 1.0123\n",
            "Batch 3020/7032, Loss: 1.2450\n",
            "Batch 3030/7032, Loss: 1.1581\n",
            "Batch 3040/7032, Loss: 0.9820\n",
            "Batch 3050/7032, Loss: 1.0379\n",
            "Batch 3060/7032, Loss: 1.0687\n",
            "Batch 3070/7032, Loss: 1.0524\n",
            "Batch 3080/7032, Loss: 1.0788\n",
            "Batch 3090/7032, Loss: 1.2196\n",
            "Batch 3100/7032, Loss: 1.1208\n",
            "Batch 3110/7032, Loss: 1.1694\n",
            "Batch 3120/7032, Loss: 1.1094\n",
            "Batch 3130/7032, Loss: 1.1744\n",
            "Batch 3140/7032, Loss: 1.0823\n",
            "Batch 3150/7032, Loss: 0.9866\n",
            "Batch 3160/7032, Loss: 1.2952\n",
            "Batch 3170/7032, Loss: 1.1657\n",
            "Batch 3180/7032, Loss: 1.2778\n",
            "Batch 3190/7032, Loss: 1.1028\n",
            "Batch 3200/7032, Loss: 1.0415\n",
            "Batch 3210/7032, Loss: 1.2210\n",
            "Batch 3220/7032, Loss: 1.1479\n",
            "Batch 3230/7032, Loss: 1.2586\n",
            "Batch 3240/7032, Loss: 0.9775\n",
            "Batch 3250/7032, Loss: 1.1255\n",
            "Batch 3260/7032, Loss: 1.2766\n",
            "Batch 3270/7032, Loss: 1.3178\n",
            "Batch 3280/7032, Loss: 1.1434\n",
            "Batch 3290/7032, Loss: 1.1313\n",
            "Batch 3300/7032, Loss: 1.2647\n",
            "Batch 3310/7032, Loss: 1.1110\n",
            "Batch 3320/7032, Loss: 1.1264\n",
            "Batch 3330/7032, Loss: 1.1831\n",
            "Batch 3340/7032, Loss: 1.2129\n",
            "Batch 3350/7032, Loss: 1.2396\n",
            "Batch 3360/7032, Loss: 1.2956\n",
            "Batch 3370/7032, Loss: 1.2792\n",
            "Batch 3380/7032, Loss: 1.0950\n",
            "Batch 3390/7032, Loss: 1.2409\n",
            "Batch 3400/7032, Loss: 1.2248\n",
            "Batch 3410/7032, Loss: 1.2428\n",
            "Batch 3420/7032, Loss: 1.0362\n",
            "Batch 3430/7032, Loss: 1.2654\n",
            "Batch 3440/7032, Loss: 1.1947\n",
            "Batch 3450/7032, Loss: 1.2046\n",
            "Batch 3460/7032, Loss: 1.0725\n",
            "Batch 3470/7032, Loss: 1.1546\n",
            "Batch 3480/7032, Loss: 1.2529\n",
            "Batch 3490/7032, Loss: 1.0705\n",
            "Batch 3500/7032, Loss: 1.1675\n",
            "Batch 3510/7032, Loss: 1.1002\n",
            "Batch 3520/7032, Loss: 1.0301\n",
            "Batch 3530/7032, Loss: 1.1570\n",
            "Batch 3540/7032, Loss: 1.1498\n",
            "Batch 3550/7032, Loss: 1.1738\n",
            "Batch 3560/7032, Loss: 1.0733\n",
            "Batch 3570/7032, Loss: 1.1246\n",
            "Batch 3580/7032, Loss: 1.0261\n",
            "Batch 3590/7032, Loss: 1.2094\n",
            "Batch 3600/7032, Loss: 1.2925\n",
            "Batch 3610/7032, Loss: 1.1118\n",
            "Batch 3620/7032, Loss: 1.1481\n",
            "Batch 3630/7032, Loss: 1.2133\n",
            "Batch 3640/7032, Loss: 1.0043\n",
            "Batch 3650/7032, Loss: 1.2585\n",
            "Batch 3660/7032, Loss: 1.1666\n",
            "Batch 3670/7032, Loss: 1.2022\n",
            "Batch 3680/7032, Loss: 1.1454\n",
            "Batch 3690/7032, Loss: 1.0940\n",
            "Batch 3700/7032, Loss: 1.0768\n",
            "Batch 3710/7032, Loss: 1.1084\n",
            "Batch 3720/7032, Loss: 1.1968\n",
            "Batch 3730/7032, Loss: 1.1808\n",
            "Batch 3740/7032, Loss: 1.0334\n",
            "Batch 3750/7032, Loss: 1.2343\n",
            "Batch 3760/7032, Loss: 1.1773\n",
            "Batch 3770/7032, Loss: 1.1494\n",
            "Batch 3780/7032, Loss: 1.1699\n",
            "Batch 3790/7032, Loss: 1.1746\n",
            "Batch 3800/7032, Loss: 1.1301\n",
            "Batch 3810/7032, Loss: 1.1290\n",
            "Batch 3820/7032, Loss: 1.0948\n",
            "Batch 3830/7032, Loss: 1.1716\n",
            "Batch 3840/7032, Loss: 1.1232\n",
            "Batch 3850/7032, Loss: 1.0749\n",
            "Batch 3860/7032, Loss: 1.1540\n",
            "Batch 3870/7032, Loss: 1.1054\n",
            "Batch 3880/7032, Loss: 1.1508\n",
            "Batch 3890/7032, Loss: 1.1629\n",
            "Batch 3900/7032, Loss: 1.2318\n",
            "Batch 3910/7032, Loss: 1.1673\n",
            "Batch 3920/7032, Loss: 1.1306\n",
            "Batch 3930/7032, Loss: 1.1152\n",
            "Batch 3940/7032, Loss: 1.1701\n",
            "Batch 3950/7032, Loss: 1.2018\n",
            "Batch 3960/7032, Loss: 1.0785\n",
            "Batch 3970/7032, Loss: 1.1753\n",
            "Batch 3980/7032, Loss: 1.2147\n",
            "Batch 3990/7032, Loss: 1.1768\n",
            "Batch 4000/7032, Loss: 1.0881\n",
            "Batch 4010/7032, Loss: 1.1708\n",
            "Batch 4020/7032, Loss: 1.1524\n",
            "Batch 4030/7032, Loss: 1.1344\n",
            "Batch 4040/7032, Loss: 1.2285\n",
            "Batch 4050/7032, Loss: 1.1736\n",
            "Batch 4060/7032, Loss: 1.0200\n",
            "Batch 4070/7032, Loss: 1.0379\n",
            "Batch 4080/7032, Loss: 1.1017\n",
            "Batch 4090/7032, Loss: 1.1271\n",
            "Batch 4100/7032, Loss: 1.2017\n",
            "Batch 4110/7032, Loss: 1.0804\n",
            "Batch 4120/7032, Loss: 1.1730\n",
            "Batch 4130/7032, Loss: 1.1286\n",
            "Batch 4140/7032, Loss: 1.1502\n",
            "Batch 4150/7032, Loss: 1.0542\n",
            "Batch 4160/7032, Loss: 1.1564\n",
            "Batch 4170/7032, Loss: 1.1599\n",
            "Batch 4180/7032, Loss: 1.0636\n",
            "Batch 4190/7032, Loss: 1.1512\n",
            "Batch 4200/7032, Loss: 1.1502\n",
            "Batch 4210/7032, Loss: 1.1184\n",
            "Batch 4220/7032, Loss: 1.1452\n",
            "Batch 4230/7032, Loss: 1.0705\n",
            "Batch 4240/7032, Loss: 1.1782\n",
            "Batch 4250/7032, Loss: 1.0627\n",
            "Batch 4260/7032, Loss: 1.0925\n",
            "Batch 4270/7032, Loss: 1.2527\n",
            "Batch 4280/7032, Loss: 1.1796\n",
            "Batch 4290/7032, Loss: 1.0622\n",
            "Batch 4300/7032, Loss: 1.2370\n",
            "Batch 4310/7032, Loss: 1.1228\n",
            "Batch 4320/7032, Loss: 1.2275\n",
            "Batch 4330/7032, Loss: 1.0565\n",
            "Batch 4340/7032, Loss: 1.0360\n",
            "Batch 4350/7032, Loss: 1.0931\n",
            "Batch 4360/7032, Loss: 1.3034\n",
            "Batch 4370/7032, Loss: 1.2248\n",
            "Batch 4380/7032, Loss: 1.1786\n",
            "Batch 4390/7032, Loss: 1.2010\n",
            "Batch 4400/7032, Loss: 1.0945\n",
            "Batch 4410/7032, Loss: 1.1533\n",
            "Batch 4420/7032, Loss: 1.1497\n",
            "Batch 4430/7032, Loss: 1.0636\n",
            "Batch 4440/7032, Loss: 1.2294\n",
            "Batch 4450/7032, Loss: 1.1874\n",
            "Batch 4460/7032, Loss: 0.9741\n",
            "Batch 4470/7032, Loss: 1.1459\n",
            "Batch 4480/7032, Loss: 1.1195\n",
            "Batch 4490/7032, Loss: 1.1813\n",
            "Batch 4500/7032, Loss: 1.1398\n",
            "Batch 4510/7032, Loss: 1.1515\n",
            "Batch 4520/7032, Loss: 1.1972\n",
            "Batch 4530/7032, Loss: 1.0541\n",
            "Batch 4540/7032, Loss: 1.1630\n",
            "Batch 4550/7032, Loss: 1.1440\n",
            "Batch 4560/7032, Loss: 1.0666\n",
            "Batch 4570/7032, Loss: 1.2040\n",
            "Batch 4580/7032, Loss: 1.0882\n",
            "Batch 4590/7032, Loss: 1.0934\n",
            "Batch 4600/7032, Loss: 1.1929\n",
            "Batch 4610/7032, Loss: 1.0660\n",
            "Batch 4620/7032, Loss: 1.0986\n",
            "Batch 4630/7032, Loss: 1.1224\n",
            "Batch 4640/7032, Loss: 1.1789\n",
            "Batch 4650/7032, Loss: 1.1794\n",
            "Batch 4660/7032, Loss: 1.0998\n",
            "Batch 4670/7032, Loss: 1.1216\n",
            "Batch 4680/7032, Loss: 1.0367\n",
            "Batch 4690/7032, Loss: 1.2345\n",
            "Batch 4700/7032, Loss: 1.0879\n",
            "Batch 4710/7032, Loss: 1.0208\n",
            "Batch 4720/7032, Loss: 1.2590\n",
            "Batch 4730/7032, Loss: 1.0845\n",
            "Batch 4740/7032, Loss: 1.0578\n",
            "Batch 4750/7032, Loss: 1.2121\n",
            "Batch 4760/7032, Loss: 1.0681\n",
            "Batch 4770/7032, Loss: 1.1035\n",
            "Batch 4780/7032, Loss: 1.2496\n",
            "Batch 4790/7032, Loss: 1.0220\n",
            "Batch 4800/7032, Loss: 1.0705\n",
            "Batch 4810/7032, Loss: 1.2264\n",
            "Batch 4820/7032, Loss: 1.0450\n",
            "Batch 4830/7032, Loss: 1.1626\n",
            "Batch 4840/7032, Loss: 0.9798\n",
            "Batch 4850/7032, Loss: 1.1938\n",
            "Batch 4860/7032, Loss: 1.1991\n",
            "Batch 4870/7032, Loss: 1.1332\n",
            "Batch 4880/7032, Loss: 1.1607\n",
            "Batch 4890/7032, Loss: 1.1778\n",
            "Batch 4900/7032, Loss: 0.9369\n",
            "Batch 4910/7032, Loss: 1.1533\n",
            "Batch 4920/7032, Loss: 1.0727\n",
            "Batch 4930/7032, Loss: 1.1537\n",
            "Batch 4940/7032, Loss: 1.2641\n",
            "Batch 4950/7032, Loss: 1.0778\n",
            "Batch 4960/7032, Loss: 1.0636\n",
            "Batch 4970/7032, Loss: 1.1451\n",
            "Batch 4980/7032, Loss: 1.0073\n",
            "Batch 4990/7032, Loss: 1.0830\n",
            "Batch 5000/7032, Loss: 1.0802\n",
            "Batch 5010/7032, Loss: 1.1720\n",
            "Batch 5020/7032, Loss: 1.2827\n",
            "Batch 5030/7032, Loss: 1.2080\n",
            "Batch 5040/7032, Loss: 1.0694\n",
            "Batch 5050/7032, Loss: 1.2090\n",
            "Batch 5060/7032, Loss: 1.3114\n",
            "Batch 5070/7032, Loss: 1.2581\n",
            "Batch 5080/7032, Loss: 1.1285\n",
            "Batch 5090/7032, Loss: 1.1344\n",
            "Batch 5100/7032, Loss: 1.1459\n",
            "Batch 5110/7032, Loss: 1.1077\n",
            "Batch 5120/7032, Loss: 1.2431\n",
            "Batch 5130/7032, Loss: 1.1112\n",
            "Batch 5140/7032, Loss: 1.1193\n",
            "Batch 5150/7032, Loss: 1.2068\n",
            "Batch 5160/7032, Loss: 1.1918\n",
            "Batch 5170/7032, Loss: 1.2659\n",
            "Batch 5180/7032, Loss: 1.2489\n",
            "Batch 5190/7032, Loss: 1.1231\n",
            "Batch 5200/7032, Loss: 1.1312\n",
            "Batch 5210/7032, Loss: 1.1081\n",
            "Batch 5220/7032, Loss: 1.0670\n",
            "Batch 5230/7032, Loss: 1.1479\n",
            "Batch 5240/7032, Loss: 1.1372\n",
            "Batch 5250/7032, Loss: 1.0759\n",
            "Batch 5260/7032, Loss: 1.1530\n",
            "Batch 5270/7032, Loss: 1.1998\n",
            "Batch 5280/7032, Loss: 1.1119\n",
            "Batch 5290/7032, Loss: 1.0245\n",
            "Batch 5300/7032, Loss: 1.1258\n",
            "Batch 5310/7032, Loss: 1.2319\n",
            "Batch 5320/7032, Loss: 1.0515\n",
            "Batch 5330/7032, Loss: 1.1956\n",
            "Batch 5340/7032, Loss: 1.1841\n",
            "Batch 5350/7032, Loss: 1.1445\n",
            "Batch 5360/7032, Loss: 1.0564\n",
            "Batch 5370/7032, Loss: 1.1562\n",
            "Batch 5380/7032, Loss: 1.0532\n",
            "Batch 5390/7032, Loss: 1.1562\n",
            "Batch 5400/7032, Loss: 1.1010\n",
            "Batch 5410/7032, Loss: 1.0618\n",
            "Batch 5420/7032, Loss: 1.0828\n",
            "Batch 5430/7032, Loss: 1.1598\n",
            "Batch 5440/7032, Loss: 1.1627\n",
            "Batch 5450/7032, Loss: 1.1372\n",
            "Batch 5460/7032, Loss: 1.0685\n",
            "Batch 5470/7032, Loss: 1.1854\n",
            "Batch 5480/7032, Loss: 1.2003\n",
            "Batch 5490/7032, Loss: 1.2404\n",
            "Batch 5500/7032, Loss: 1.0076\n",
            "Batch 5510/7032, Loss: 0.9940\n",
            "Batch 5520/7032, Loss: 1.0438\n",
            "Batch 5530/7032, Loss: 1.1269\n",
            "Batch 5540/7032, Loss: 1.1939\n",
            "Batch 5550/7032, Loss: 1.1454\n",
            "Batch 5560/7032, Loss: 1.0843\n",
            "Batch 5570/7032, Loss: 1.1506\n",
            "Batch 5580/7032, Loss: 1.1866\n",
            "Batch 5590/7032, Loss: 1.1373\n",
            "Batch 5600/7032, Loss: 1.2467\n",
            "Batch 5610/7032, Loss: 1.1014\n",
            "Batch 5620/7032, Loss: 1.0279\n",
            "Batch 5630/7032, Loss: 1.1700\n",
            "Batch 5640/7032, Loss: 1.1725\n",
            "Batch 5650/7032, Loss: 1.2630\n",
            "Batch 5660/7032, Loss: 1.2237\n",
            "Batch 5670/7032, Loss: 1.2652\n",
            "Batch 5680/7032, Loss: 1.1885\n",
            "Batch 5690/7032, Loss: 1.1402\n",
            "Batch 5700/7032, Loss: 1.0619\n",
            "Batch 5710/7032, Loss: 1.0614\n",
            "Batch 5720/7032, Loss: 1.0898\n",
            "Batch 5730/7032, Loss: 1.1803\n",
            "Batch 5740/7032, Loss: 1.1801\n",
            "Batch 5750/7032, Loss: 1.1548\n",
            "Batch 5760/7032, Loss: 1.1458\n",
            "Batch 5770/7032, Loss: 1.1189\n",
            "Batch 5780/7032, Loss: 1.0615\n",
            "Batch 5790/7032, Loss: 1.1909\n",
            "Batch 5800/7032, Loss: 1.1800\n",
            "Batch 5810/7032, Loss: 1.2557\n",
            "Batch 5820/7032, Loss: 1.1011\n",
            "Batch 5830/7032, Loss: 1.2316\n",
            "Batch 5840/7032, Loss: 1.1199\n",
            "Batch 5850/7032, Loss: 0.9787\n",
            "Batch 5860/7032, Loss: 1.2798\n",
            "Batch 5870/7032, Loss: 1.1259\n",
            "Batch 5880/7032, Loss: 1.1550\n",
            "Batch 5890/7032, Loss: 1.1257\n",
            "Batch 5900/7032, Loss: 1.0976\n",
            "Batch 5910/7032, Loss: 1.1512\n",
            "Batch 5920/7032, Loss: 0.9637\n",
            "Batch 5930/7032, Loss: 1.0387\n",
            "Batch 5940/7032, Loss: 1.2510\n",
            "Batch 5950/7032, Loss: 1.0950\n",
            "Batch 5960/7032, Loss: 1.0776\n",
            "Batch 5970/7032, Loss: 1.1573\n",
            "Batch 5980/7032, Loss: 1.1068\n",
            "Batch 5990/7032, Loss: 1.1120\n",
            "Batch 6000/7032, Loss: 1.1034\n",
            "Batch 6010/7032, Loss: 1.0755\n",
            "Batch 6020/7032, Loss: 1.1559\n",
            "Batch 6030/7032, Loss: 1.0333\n",
            "Batch 6040/7032, Loss: 0.9713\n",
            "Batch 6050/7032, Loss: 1.0336\n",
            "Batch 6060/7032, Loss: 1.2654\n",
            "Batch 6070/7032, Loss: 1.1136\n",
            "Batch 6080/7032, Loss: 1.1505\n",
            "Batch 6090/7032, Loss: 0.9851\n",
            "Batch 6100/7032, Loss: 0.9551\n",
            "Batch 6110/7032, Loss: 1.1422\n",
            "Batch 6120/7032, Loss: 0.9849\n",
            "Batch 6130/7032, Loss: 1.1674\n",
            "Batch 6140/7032, Loss: 1.2980\n",
            "Batch 6150/7032, Loss: 1.1179\n",
            "Batch 6160/7032, Loss: 1.1942\n",
            "Batch 6170/7032, Loss: 1.2198\n",
            "Batch 6180/7032, Loss: 1.0818\n",
            "Batch 6190/7032, Loss: 1.2506\n",
            "Batch 6200/7032, Loss: 1.1704\n",
            "Batch 6210/7032, Loss: 1.1688\n",
            "Batch 6220/7032, Loss: 1.1700\n",
            "Batch 6230/7032, Loss: 0.9462\n",
            "Batch 6240/7032, Loss: 1.1290\n",
            "Batch 6250/7032, Loss: 1.2012\n",
            "Batch 6260/7032, Loss: 1.1517\n",
            "Batch 6270/7032, Loss: 1.1339\n",
            "Batch 6280/7032, Loss: 1.0422\n",
            "Batch 6290/7032, Loss: 0.9862\n",
            "Batch 6300/7032, Loss: 1.0726\n",
            "Batch 6310/7032, Loss: 1.2150\n",
            "Batch 6320/7032, Loss: 1.2486\n",
            "Batch 6330/7032, Loss: 1.1507\n",
            "Batch 6340/7032, Loss: 0.9534\n",
            "Batch 6350/7032, Loss: 1.0743\n",
            "Batch 6360/7032, Loss: 1.1948\n",
            "Batch 6370/7032, Loss: 1.2400\n",
            "Batch 6380/7032, Loss: 1.1046\n",
            "Batch 6390/7032, Loss: 1.1024\n",
            "Batch 6400/7032, Loss: 1.0411\n",
            "Batch 6410/7032, Loss: 1.0395\n",
            "Batch 6420/7032, Loss: 1.0585\n",
            "Batch 6430/7032, Loss: 1.1264\n",
            "Batch 6440/7032, Loss: 1.0568\n",
            "Batch 6450/7032, Loss: 1.2626\n",
            "Batch 6460/7032, Loss: 1.1922\n",
            "Batch 6470/7032, Loss: 1.2095\n",
            "Batch 6480/7032, Loss: 1.0577\n",
            "Batch 6490/7032, Loss: 1.1260\n",
            "Batch 6500/7032, Loss: 1.1597\n",
            "Batch 6510/7032, Loss: 1.0258\n",
            "Batch 6520/7032, Loss: 1.1887\n",
            "Batch 6530/7032, Loss: 1.0570\n",
            "Batch 6540/7032, Loss: 1.1775\n",
            "Batch 6550/7032, Loss: 1.1258\n",
            "Batch 6560/7032, Loss: 1.1650\n",
            "Batch 6570/7032, Loss: 1.1774\n",
            "Batch 6580/7032, Loss: 1.0540\n",
            "Batch 6590/7032, Loss: 1.1303\n",
            "Batch 6600/7032, Loss: 1.2572\n",
            "Batch 6610/7032, Loss: 1.0357\n",
            "Batch 6620/7032, Loss: 1.2257\n",
            "Batch 6630/7032, Loss: 1.0730\n",
            "Batch 6640/7032, Loss: 1.0416\n",
            "Batch 6650/7032, Loss: 1.3128\n",
            "Batch 6660/7032, Loss: 1.1994\n",
            "Batch 6670/7032, Loss: 1.2338\n",
            "Batch 6680/7032, Loss: 1.1935\n",
            "Batch 6690/7032, Loss: 1.0407\n",
            "Batch 6700/7032, Loss: 1.0890\n",
            "Batch 6710/7032, Loss: 1.0894\n",
            "Batch 6720/7032, Loss: 1.3028\n",
            "Batch 6730/7032, Loss: 1.2447\n",
            "Batch 6740/7032, Loss: 1.1043\n",
            "Batch 6750/7032, Loss: 1.0242\n",
            "Batch 6760/7032, Loss: 1.0505\n",
            "Batch 6770/7032, Loss: 1.0201\n",
            "Batch 6780/7032, Loss: 0.9796\n",
            "Batch 6790/7032, Loss: 0.9897\n",
            "Batch 6800/7032, Loss: 1.0032\n",
            "Batch 6810/7032, Loss: 1.1137\n",
            "Batch 6820/7032, Loss: 1.1423\n",
            "Batch 6830/7032, Loss: 1.2701\n",
            "Batch 6840/7032, Loss: 1.0812\n",
            "Batch 6850/7032, Loss: 1.0224\n",
            "Batch 6860/7032, Loss: 1.1368\n",
            "Batch 6870/7032, Loss: 1.0030\n",
            "Batch 6880/7032, Loss: 0.9869\n",
            "Batch 6890/7032, Loss: 1.0284\n",
            "Batch 6900/7032, Loss: 1.1992\n",
            "Batch 6910/7032, Loss: 1.2378\n",
            "Batch 6920/7032, Loss: 1.1013\n",
            "Batch 6930/7032, Loss: 1.1527\n",
            "Batch 6940/7032, Loss: 1.1274\n",
            "Batch 6950/7032, Loss: 1.1634\n",
            "Batch 6960/7032, Loss: 1.2857\n",
            "Batch 6970/7032, Loss: 1.1141\n",
            "Batch 6980/7032, Loss: 1.1641\n",
            "Batch 6990/7032, Loss: 1.2315\n",
            "Batch 7000/7032, Loss: 1.0940\n",
            "Batch 7010/7032, Loss: 1.0894\n",
            "Batch 7020/7032, Loss: 1.0254\n",
            "Batch 7030/7032, Loss: 1.1237\n",
            "Batch 7032/7032, Loss: 1.1986\n",
            "Epoch 5/10\n",
            "Batch 10/7032, Loss: 1.0926\n",
            "Batch 20/7032, Loss: 1.1353\n",
            "Batch 30/7032, Loss: 1.1003\n",
            "Batch 40/7032, Loss: 1.0340\n",
            "Batch 50/7032, Loss: 1.0544\n",
            "Batch 60/7032, Loss: 1.1252\n",
            "Batch 70/7032, Loss: 1.0669\n",
            "Batch 80/7032, Loss: 1.1137\n",
            "Batch 90/7032, Loss: 1.0631\n",
            "Batch 100/7032, Loss: 1.1926\n",
            "Batch 110/7032, Loss: 1.1668\n",
            "Batch 120/7032, Loss: 1.1048\n",
            "Batch 130/7032, Loss: 1.1935\n",
            "Batch 140/7032, Loss: 1.1134\n",
            "Batch 150/7032, Loss: 1.1784\n",
            "Batch 160/7032, Loss: 1.1359\n",
            "Batch 170/7032, Loss: 1.0242\n",
            "Batch 180/7032, Loss: 1.1038\n",
            "Batch 190/7032, Loss: 1.0246\n",
            "Batch 200/7032, Loss: 1.1680\n",
            "Batch 210/7032, Loss: 1.0767\n",
            "Batch 220/7032, Loss: 1.1112\n",
            "Batch 230/7032, Loss: 1.0907\n",
            "Batch 240/7032, Loss: 1.0598\n",
            "Batch 250/7032, Loss: 1.0978\n",
            "Batch 260/7032, Loss: 1.1222\n",
            "Batch 270/7032, Loss: 1.1121\n",
            "Batch 280/7032, Loss: 1.1857\n",
            "Batch 290/7032, Loss: 1.1156\n",
            "Batch 300/7032, Loss: 0.9860\n",
            "Batch 310/7032, Loss: 1.1441\n",
            "Batch 320/7032, Loss: 1.1164\n",
            "Batch 330/7032, Loss: 1.0148\n",
            "Batch 340/7032, Loss: 1.1854\n",
            "Batch 350/7032, Loss: 1.2418\n",
            "Batch 360/7032, Loss: 1.0932\n",
            "Batch 370/7032, Loss: 1.0090\n",
            "Batch 380/7032, Loss: 1.0319\n",
            "Batch 390/7032, Loss: 1.0066\n",
            "Batch 400/7032, Loss: 1.1918\n",
            "Batch 410/7032, Loss: 1.1355\n",
            "Batch 420/7032, Loss: 1.1515\n",
            "Batch 430/7032, Loss: 1.1006\n",
            "Batch 440/7032, Loss: 1.1346\n",
            "Batch 450/7032, Loss: 1.0992\n",
            "Batch 460/7032, Loss: 1.1094\n",
            "Batch 470/7032, Loss: 1.1254\n",
            "Batch 480/7032, Loss: 1.0091\n",
            "Batch 490/7032, Loss: 1.1746\n",
            "Batch 500/7032, Loss: 1.1062\n",
            "Batch 510/7032, Loss: 1.1437\n",
            "Batch 520/7032, Loss: 1.1445\n",
            "Batch 530/7032, Loss: 0.9102\n",
            "Batch 540/7032, Loss: 0.9195\n",
            "Batch 550/7032, Loss: 1.2700\n",
            "Batch 560/7032, Loss: 1.2115\n",
            "Batch 570/7032, Loss: 1.1974\n",
            "Batch 580/7032, Loss: 1.0492\n",
            "Batch 590/7032, Loss: 1.0836\n",
            "Batch 600/7032, Loss: 1.1784\n",
            "Batch 610/7032, Loss: 1.0493\n",
            "Batch 620/7032, Loss: 1.2677\n",
            "Batch 630/7032, Loss: 1.0905\n",
            "Batch 640/7032, Loss: 1.1103\n",
            "Batch 650/7032, Loss: 1.0932\n",
            "Batch 660/7032, Loss: 1.0865\n",
            "Batch 670/7032, Loss: 1.1367\n",
            "Batch 680/7032, Loss: 1.0571\n",
            "Batch 690/7032, Loss: 1.0986\n",
            "Batch 700/7032, Loss: 1.2082\n",
            "Batch 710/7032, Loss: 1.1222\n",
            "Batch 720/7032, Loss: 1.1604\n",
            "Batch 730/7032, Loss: 1.1752\n",
            "Batch 740/7032, Loss: 1.0864\n",
            "Batch 750/7032, Loss: 1.0715\n",
            "Batch 760/7032, Loss: 1.0310\n",
            "Batch 770/7032, Loss: 1.1875\n",
            "Batch 780/7032, Loss: 1.1383\n",
            "Batch 790/7032, Loss: 0.9574\n",
            "Batch 800/7032, Loss: 1.2379\n",
            "Batch 810/7032, Loss: 1.0346\n",
            "Batch 820/7032, Loss: 1.0520\n",
            "Batch 830/7032, Loss: 1.0980\n",
            "Batch 840/7032, Loss: 1.1299\n",
            "Batch 850/7032, Loss: 1.1349\n",
            "Batch 860/7032, Loss: 1.0432\n",
            "Batch 870/7032, Loss: 1.1404\n",
            "Batch 880/7032, Loss: 1.1973\n",
            "Batch 890/7032, Loss: 1.1084\n",
            "Batch 900/7032, Loss: 1.1205\n",
            "Batch 910/7032, Loss: 1.2466\n",
            "Batch 920/7032, Loss: 0.9832\n",
            "Batch 930/7032, Loss: 1.1682\n",
            "Batch 940/7032, Loss: 1.0095\n",
            "Batch 950/7032, Loss: 1.0448\n",
            "Batch 960/7032, Loss: 1.1434\n",
            "Batch 970/7032, Loss: 1.2430\n",
            "Batch 980/7032, Loss: 1.0658\n",
            "Batch 990/7032, Loss: 1.1147\n",
            "Batch 1000/7032, Loss: 1.1955\n",
            "Batch 1010/7032, Loss: 0.9595\n",
            "Batch 1020/7032, Loss: 1.1337\n",
            "Batch 1030/7032, Loss: 1.0108\n",
            "Batch 1040/7032, Loss: 1.1418\n",
            "Batch 1050/7032, Loss: 1.2476\n",
            "Batch 1060/7032, Loss: 1.1517\n",
            "Batch 1070/7032, Loss: 1.0742\n",
            "Batch 1080/7032, Loss: 1.0536\n",
            "Batch 1090/7032, Loss: 1.0952\n",
            "Batch 1100/7032, Loss: 1.0128\n",
            "Batch 1110/7032, Loss: 1.1076\n",
            "Batch 1120/7032, Loss: 1.0274\n",
            "Batch 1130/7032, Loss: 1.1079\n",
            "Batch 1140/7032, Loss: 1.0554\n",
            "Batch 1150/7032, Loss: 1.2490\n",
            "Batch 1160/7032, Loss: 1.1833\n",
            "Batch 1170/7032, Loss: 1.1358\n",
            "Batch 1180/7032, Loss: 0.9912\n",
            "Batch 1190/7032, Loss: 1.1519\n",
            "Batch 1200/7032, Loss: 1.0766\n",
            "Batch 1210/7032, Loss: 1.0829\n",
            "Batch 1220/7032, Loss: 1.2234\n",
            "Batch 1230/7032, Loss: 1.0853\n",
            "Batch 1240/7032, Loss: 1.1241\n",
            "Batch 1250/7032, Loss: 1.1520\n",
            "Batch 1260/7032, Loss: 1.0475\n",
            "Batch 1270/7032, Loss: 1.3021\n",
            "Batch 1280/7032, Loss: 1.0918\n",
            "Batch 1290/7032, Loss: 1.1022\n",
            "Batch 1300/7032, Loss: 1.1678\n",
            "Batch 1310/7032, Loss: 1.0062\n",
            "Batch 1320/7032, Loss: 0.9501\n",
            "Batch 1330/7032, Loss: 1.1421\n",
            "Batch 1340/7032, Loss: 1.0576\n",
            "Batch 1350/7032, Loss: 1.0347\n",
            "Batch 1360/7032, Loss: 1.0161\n",
            "Batch 1370/7032, Loss: 1.0873\n",
            "Batch 1380/7032, Loss: 1.0305\n",
            "Batch 1390/7032, Loss: 1.0537\n",
            "Batch 1400/7032, Loss: 1.0377\n",
            "Batch 1410/7032, Loss: 1.0330\n",
            "Batch 1420/7032, Loss: 1.1409\n",
            "Batch 1430/7032, Loss: 1.0055\n",
            "Batch 1440/7032, Loss: 1.1730\n",
            "Batch 1450/7032, Loss: 0.9495\n",
            "Batch 1460/7032, Loss: 1.0668\n",
            "Batch 1470/7032, Loss: 0.9970\n",
            "Batch 1480/7032, Loss: 1.0128\n",
            "Batch 1490/7032, Loss: 1.0932\n",
            "Batch 1500/7032, Loss: 1.1874\n",
            "Batch 1510/7032, Loss: 1.1881\n",
            "Batch 1520/7032, Loss: 1.0739\n",
            "Batch 1530/7032, Loss: 1.1685\n",
            "Batch 1540/7032, Loss: 1.0705\n",
            "Batch 1550/7032, Loss: 1.1005\n",
            "Batch 1560/7032, Loss: 1.1553\n",
            "Batch 1570/7032, Loss: 1.1465\n",
            "Batch 1580/7032, Loss: 1.1145\n",
            "Batch 1590/7032, Loss: 1.1736\n",
            "Batch 1600/7032, Loss: 1.0579\n",
            "Batch 1610/7032, Loss: 1.0193\n",
            "Batch 1620/7032, Loss: 1.2359\n",
            "Batch 1630/7032, Loss: 1.0591\n",
            "Batch 1640/7032, Loss: 0.9877\n",
            "Batch 1650/7032, Loss: 1.0772\n",
            "Batch 1660/7032, Loss: 1.1147\n",
            "Batch 1670/7032, Loss: 1.1164\n",
            "Batch 1680/7032, Loss: 1.0932\n",
            "Batch 1690/7032, Loss: 1.0161\n",
            "Batch 1700/7032, Loss: 1.0951\n",
            "Batch 1710/7032, Loss: 1.1927\n",
            "Batch 1720/7032, Loss: 1.0029\n",
            "Batch 1730/7032, Loss: 1.1966\n",
            "Batch 1740/7032, Loss: 1.1461\n",
            "Batch 1750/7032, Loss: 1.0930\n",
            "Batch 1760/7032, Loss: 1.1479\n",
            "Batch 1770/7032, Loss: 1.0391\n",
            "Batch 1780/7032, Loss: 1.0865\n",
            "Batch 1790/7032, Loss: 1.1434\n",
            "Batch 1800/7032, Loss: 1.0235\n",
            "Batch 1810/7032, Loss: 1.1050\n",
            "Batch 1820/7032, Loss: 1.1447\n",
            "Batch 1830/7032, Loss: 1.1717\n",
            "Batch 1840/7032, Loss: 1.1708\n",
            "Batch 1850/7032, Loss: 1.0143\n",
            "Batch 1860/7032, Loss: 0.9668\n",
            "Batch 1870/7032, Loss: 1.1392\n",
            "Batch 1880/7032, Loss: 0.9908\n",
            "Batch 1890/7032, Loss: 1.0702\n",
            "Batch 1900/7032, Loss: 1.2811\n",
            "Batch 1910/7032, Loss: 1.0880\n",
            "Batch 1920/7032, Loss: 1.0558\n",
            "Batch 1930/7032, Loss: 1.2491\n",
            "Batch 1940/7032, Loss: 1.1436\n",
            "Batch 1950/7032, Loss: 1.1527\n",
            "Batch 1960/7032, Loss: 1.1347\n",
            "Batch 1970/7032, Loss: 1.0190\n",
            "Batch 1980/7032, Loss: 1.0992\n",
            "Batch 1990/7032, Loss: 1.0885\n",
            "Batch 2000/7032, Loss: 1.1181\n",
            "Batch 2010/7032, Loss: 1.0362\n",
            "Batch 2020/7032, Loss: 1.0994\n",
            "Batch 2030/7032, Loss: 1.1225\n",
            "Batch 2040/7032, Loss: 0.8660\n",
            "Batch 2050/7032, Loss: 1.1445\n",
            "Batch 2060/7032, Loss: 1.0473\n",
            "Batch 2070/7032, Loss: 1.1006\n",
            "Batch 2080/7032, Loss: 1.1683\n",
            "Batch 2090/7032, Loss: 1.0525\n",
            "Batch 2100/7032, Loss: 1.1217\n",
            "Batch 2110/7032, Loss: 0.9526\n",
            "Batch 2120/7032, Loss: 1.1124\n",
            "Batch 2130/7032, Loss: 1.1245\n",
            "Batch 2140/7032, Loss: 1.0108\n",
            "Batch 2150/7032, Loss: 1.0672\n",
            "Batch 2160/7032, Loss: 1.1529\n",
            "Batch 2170/7032, Loss: 1.2569\n",
            "Batch 2180/7032, Loss: 1.1602\n",
            "Batch 2190/7032, Loss: 1.1179\n",
            "Batch 2200/7032, Loss: 1.0822\n",
            "Batch 2210/7032, Loss: 1.1010\n",
            "Batch 2220/7032, Loss: 1.1007\n",
            "Batch 2230/7032, Loss: 1.1091\n",
            "Batch 2240/7032, Loss: 1.0442\n",
            "Batch 2250/7032, Loss: 0.9944\n",
            "Batch 2260/7032, Loss: 1.1212\n",
            "Batch 2270/7032, Loss: 1.1425\n",
            "Batch 2280/7032, Loss: 1.0436\n",
            "Batch 2290/7032, Loss: 1.1559\n",
            "Batch 2300/7032, Loss: 1.0158\n",
            "Batch 2310/7032, Loss: 1.0689\n",
            "Batch 2320/7032, Loss: 1.0882\n",
            "Batch 2330/7032, Loss: 1.1315\n",
            "Batch 2340/7032, Loss: 0.9604\n",
            "Batch 2350/7032, Loss: 1.3167\n",
            "Batch 2360/7032, Loss: 1.0569\n",
            "Batch 2370/7032, Loss: 1.0572\n",
            "Batch 2380/7032, Loss: 1.2197\n",
            "Batch 2390/7032, Loss: 0.9510\n",
            "Batch 2400/7032, Loss: 1.1363\n",
            "Batch 2410/7032, Loss: 1.1714\n",
            "Batch 2420/7032, Loss: 1.0946\n",
            "Batch 2430/7032, Loss: 1.0806\n",
            "Batch 2440/7032, Loss: 0.9768\n",
            "Batch 2450/7032, Loss: 0.9786\n",
            "Batch 2460/7032, Loss: 1.0707\n",
            "Batch 2470/7032, Loss: 1.2171\n",
            "Batch 2480/7032, Loss: 1.1567\n",
            "Batch 2490/7032, Loss: 1.0226\n",
            "Batch 2500/7032, Loss: 0.9047\n",
            "Batch 2510/7032, Loss: 1.1466\n",
            "Batch 2520/7032, Loss: 1.1592\n",
            "Batch 2530/7032, Loss: 1.0173\n",
            "Batch 2540/7032, Loss: 1.1897\n",
            "Batch 2550/7032, Loss: 1.2096\n",
            "Batch 2560/7032, Loss: 1.0174\n",
            "Batch 2570/7032, Loss: 0.9354\n",
            "Batch 2580/7032, Loss: 1.0435\n",
            "Batch 2590/7032, Loss: 1.0807\n",
            "Batch 2600/7032, Loss: 1.2008\n",
            "Batch 2610/7032, Loss: 1.0538\n",
            "Batch 2620/7032, Loss: 1.1488\n",
            "Batch 2630/7032, Loss: 1.0457\n",
            "Batch 2640/7032, Loss: 1.1488\n",
            "Batch 2650/7032, Loss: 1.1027\n",
            "Batch 2660/7032, Loss: 1.0828\n",
            "Batch 2670/7032, Loss: 1.1175\n",
            "Batch 2680/7032, Loss: 1.2498\n",
            "Batch 2690/7032, Loss: 1.2406\n",
            "Batch 2700/7032, Loss: 1.1417\n",
            "Batch 2710/7032, Loss: 1.0311\n",
            "Batch 2720/7032, Loss: 1.0680\n",
            "Batch 2730/7032, Loss: 1.0844\n",
            "Batch 2740/7032, Loss: 1.2043\n",
            "Batch 2750/7032, Loss: 1.1617\n",
            "Batch 2760/7032, Loss: 1.1217\n",
            "Batch 2770/7032, Loss: 1.0500\n",
            "Batch 2780/7032, Loss: 1.1162\n",
            "Batch 2790/7032, Loss: 1.0667\n",
            "Batch 2800/7032, Loss: 1.1962\n",
            "Batch 2810/7032, Loss: 0.9393\n",
            "Batch 2820/7032, Loss: 1.0591\n",
            "Batch 2830/7032, Loss: 1.1223\n",
            "Batch 2840/7032, Loss: 1.1742\n",
            "Batch 2850/7032, Loss: 0.9892\n",
            "Batch 2860/7032, Loss: 1.1119\n",
            "Batch 2870/7032, Loss: 1.0849\n",
            "Batch 2880/7032, Loss: 1.0287\n",
            "Batch 2890/7032, Loss: 1.0647\n",
            "Batch 2900/7032, Loss: 1.0997\n",
            "Batch 2910/7032, Loss: 1.1087\n",
            "Batch 2920/7032, Loss: 1.1144\n",
            "Batch 2930/7032, Loss: 1.0149\n",
            "Batch 2940/7032, Loss: 1.0235\n",
            "Batch 2950/7032, Loss: 1.0915\n",
            "Batch 2960/7032, Loss: 1.0337\n",
            "Batch 2970/7032, Loss: 1.1532\n",
            "Batch 2980/7032, Loss: 1.0773\n",
            "Batch 2990/7032, Loss: 1.0420\n",
            "Batch 3000/7032, Loss: 1.1295\n",
            "Batch 3010/7032, Loss: 1.0740\n",
            "Batch 3020/7032, Loss: 1.0534\n",
            "Batch 3030/7032, Loss: 0.9777\n",
            "Batch 3040/7032, Loss: 1.0592\n",
            "Batch 3050/7032, Loss: 1.0693\n",
            "Batch 3060/7032, Loss: 1.0586\n",
            "Batch 3070/7032, Loss: 1.0095\n",
            "Batch 3080/7032, Loss: 1.1597\n",
            "Batch 3090/7032, Loss: 1.1288\n",
            "Batch 3100/7032, Loss: 1.0034\n",
            "Batch 3110/7032, Loss: 1.1082\n",
            "Batch 3120/7032, Loss: 1.1011\n",
            "Batch 3130/7032, Loss: 1.0997\n",
            "Batch 3140/7032, Loss: 1.0913\n",
            "Batch 3150/7032, Loss: 1.1600\n",
            "Batch 3160/7032, Loss: 1.1555\n",
            "Batch 3170/7032, Loss: 0.9591\n",
            "Batch 3180/7032, Loss: 1.0835\n",
            "Batch 3190/7032, Loss: 1.0528\n",
            "Batch 3200/7032, Loss: 1.2109\n",
            "Batch 3210/7032, Loss: 1.0386\n",
            "Batch 3220/7032, Loss: 1.0304\n",
            "Batch 3230/7032, Loss: 1.0987\n",
            "Batch 3240/7032, Loss: 1.0833\n",
            "Batch 3250/7032, Loss: 1.1522\n",
            "Batch 3260/7032, Loss: 1.2165\n",
            "Batch 3270/7032, Loss: 1.1152\n",
            "Batch 3280/7032, Loss: 1.0777\n",
            "Batch 3290/7032, Loss: 1.0087\n",
            "Batch 3300/7032, Loss: 1.2441\n",
            "Batch 3310/7032, Loss: 1.2148\n",
            "Batch 3320/7032, Loss: 1.0907\n",
            "Batch 3330/7032, Loss: 1.0952\n",
            "Batch 3340/7032, Loss: 1.1665\n",
            "Batch 3350/7032, Loss: 1.0871\n",
            "Batch 3360/7032, Loss: 1.2006\n",
            "Batch 3370/7032, Loss: 1.0230\n",
            "Batch 3380/7032, Loss: 1.1761\n",
            "Batch 3390/7032, Loss: 1.0507\n",
            "Batch 3400/7032, Loss: 1.0798\n",
            "Batch 3410/7032, Loss: 1.1088\n",
            "Batch 3420/7032, Loss: 1.1171\n",
            "Batch 3430/7032, Loss: 1.1529\n",
            "Batch 3440/7032, Loss: 1.1245\n",
            "Batch 3450/7032, Loss: 1.1274\n",
            "Batch 3460/7032, Loss: 1.0660\n",
            "Batch 3470/7032, Loss: 1.1265\n",
            "Batch 3480/7032, Loss: 1.2221\n",
            "Batch 3490/7032, Loss: 1.0645\n",
            "Batch 3500/7032, Loss: 1.0180\n",
            "Batch 3510/7032, Loss: 1.1774\n",
            "Batch 3520/7032, Loss: 1.1537\n",
            "Batch 3530/7032, Loss: 1.0999\n",
            "Batch 3540/7032, Loss: 1.0894\n",
            "Batch 3550/7032, Loss: 1.1049\n",
            "Batch 3560/7032, Loss: 1.1453\n",
            "Batch 3570/7032, Loss: 0.9438\n",
            "Batch 3580/7032, Loss: 1.1054\n",
            "Batch 3590/7032, Loss: 0.9816\n",
            "Batch 3600/7032, Loss: 1.1042\n",
            "Batch 3610/7032, Loss: 1.1480\n",
            "Batch 3620/7032, Loss: 1.0244\n",
            "Batch 3630/7032, Loss: 0.9979\n",
            "Batch 3640/7032, Loss: 1.1743\n",
            "Batch 3650/7032, Loss: 1.0534\n",
            "Batch 3660/7032, Loss: 1.0640\n",
            "Batch 3670/7032, Loss: 1.1126\n",
            "Batch 3680/7032, Loss: 0.9417\n",
            "Batch 3690/7032, Loss: 0.9580\n",
            "Batch 3700/7032, Loss: 1.2119\n",
            "Batch 3710/7032, Loss: 1.1648\n",
            "Batch 3720/7032, Loss: 1.0897\n",
            "Batch 3730/7032, Loss: 1.1321\n",
            "Batch 3740/7032, Loss: 1.1025\n",
            "Batch 3750/7032, Loss: 1.0525\n",
            "Batch 3760/7032, Loss: 1.0528\n",
            "Batch 3770/7032, Loss: 1.1679\n",
            "Batch 3780/7032, Loss: 1.0594\n",
            "Batch 3790/7032, Loss: 1.1458\n",
            "Batch 3800/7032, Loss: 1.0973\n",
            "Batch 3810/7032, Loss: 1.1527\n",
            "Batch 3820/7032, Loss: 1.0455\n",
            "Batch 3830/7032, Loss: 1.0169\n",
            "Batch 3840/7032, Loss: 1.0574\n",
            "Batch 3850/7032, Loss: 0.9961\n",
            "Batch 3860/7032, Loss: 1.0580\n",
            "Batch 3870/7032, Loss: 1.0493\n",
            "Batch 3880/7032, Loss: 1.1294\n",
            "Batch 3890/7032, Loss: 1.0674\n",
            "Batch 3900/7032, Loss: 1.1763\n",
            "Batch 3910/7032, Loss: 1.0855\n",
            "Batch 3920/7032, Loss: 1.0976\n",
            "Batch 3930/7032, Loss: 1.0216\n",
            "Batch 3940/7032, Loss: 1.0391\n",
            "Batch 3950/7032, Loss: 1.0182\n",
            "Batch 3960/7032, Loss: 1.0548\n",
            "Batch 3970/7032, Loss: 1.1118\n",
            "Batch 3980/7032, Loss: 1.0876\n",
            "Batch 3990/7032, Loss: 0.9713\n",
            "Batch 4000/7032, Loss: 1.0547\n",
            "Batch 4010/7032, Loss: 1.1923\n",
            "Batch 4020/7032, Loss: 1.0925\n",
            "Batch 4030/7032, Loss: 1.0902\n",
            "Batch 4040/7032, Loss: 0.9880\n",
            "Batch 4050/7032, Loss: 0.9845\n",
            "Batch 4060/7032, Loss: 1.0535\n",
            "Batch 4070/7032, Loss: 1.0744\n",
            "Batch 4080/7032, Loss: 1.0425\n",
            "Batch 4090/7032, Loss: 1.0891\n",
            "Batch 4100/7032, Loss: 1.2244\n",
            "Batch 4110/7032, Loss: 0.9102\n",
            "Batch 4120/7032, Loss: 0.9681\n",
            "Batch 4130/7032, Loss: 1.1872\n",
            "Batch 4140/7032, Loss: 1.0694\n",
            "Batch 4150/7032, Loss: 1.1204\n",
            "Batch 4160/7032, Loss: 0.9873\n",
            "Batch 4170/7032, Loss: 1.0666\n",
            "Batch 4180/7032, Loss: 1.1468\n",
            "Batch 4190/7032, Loss: 1.1496\n",
            "Batch 4200/7032, Loss: 1.0719\n",
            "Batch 4210/7032, Loss: 1.0220\n",
            "Batch 4220/7032, Loss: 1.0877\n",
            "Batch 4230/7032, Loss: 1.1235\n",
            "Batch 4240/7032, Loss: 1.0980\n",
            "Batch 4250/7032, Loss: 1.1417\n",
            "Batch 4260/7032, Loss: 1.1136\n",
            "Batch 4270/7032, Loss: 1.0560\n",
            "Batch 4280/7032, Loss: 1.0099\n",
            "Batch 4290/7032, Loss: 1.0643\n",
            "Batch 4300/7032, Loss: 1.0622\n",
            "Batch 4310/7032, Loss: 1.0496\n",
            "Batch 4320/7032, Loss: 1.1076\n",
            "Batch 4330/7032, Loss: 0.9928\n",
            "Batch 4340/7032, Loss: 1.0974\n",
            "Batch 4350/7032, Loss: 0.9324\n",
            "Batch 4360/7032, Loss: 1.1210\n",
            "Batch 4370/7032, Loss: 1.0790\n",
            "Batch 4380/7032, Loss: 1.0759\n",
            "Batch 4390/7032, Loss: 1.0105\n",
            "Batch 4400/7032, Loss: 1.1405\n",
            "Batch 4410/7032, Loss: 1.0985\n",
            "Batch 4420/7032, Loss: 0.9633\n",
            "Batch 4430/7032, Loss: 1.0342\n",
            "Batch 4440/7032, Loss: 1.0702\n",
            "Batch 4450/7032, Loss: 1.0838\n",
            "Batch 4460/7032, Loss: 1.0135\n",
            "Batch 4470/7032, Loss: 1.0515\n",
            "Batch 4480/7032, Loss: 0.9442\n",
            "Batch 4490/7032, Loss: 1.1325\n",
            "Batch 4500/7032, Loss: 1.1223\n",
            "Batch 4510/7032, Loss: 1.0666\n",
            "Batch 4520/7032, Loss: 1.0246\n",
            "Batch 4530/7032, Loss: 1.0996\n",
            "Batch 4540/7032, Loss: 1.0739\n",
            "Batch 4550/7032, Loss: 1.0378\n",
            "Batch 4560/7032, Loss: 1.0295\n",
            "Batch 4570/7032, Loss: 1.1893\n",
            "Batch 4580/7032, Loss: 0.9711\n",
            "Batch 4590/7032, Loss: 0.9812\n",
            "Batch 4600/7032, Loss: 1.1536\n",
            "Batch 4610/7032, Loss: 1.1852\n",
            "Batch 4620/7032, Loss: 1.2558\n",
            "Batch 4630/7032, Loss: 1.0473\n",
            "Batch 4640/7032, Loss: 1.0527\n",
            "Batch 4650/7032, Loss: 1.1676\n",
            "Batch 4660/7032, Loss: 0.9218\n",
            "Batch 4670/7032, Loss: 1.1545\n",
            "Batch 4680/7032, Loss: 0.9865\n",
            "Batch 4690/7032, Loss: 1.1892\n",
            "Batch 4700/7032, Loss: 1.0913\n",
            "Batch 4710/7032, Loss: 1.1090\n",
            "Batch 4720/7032, Loss: 1.0657\n",
            "Batch 4730/7032, Loss: 1.1266\n",
            "Batch 4740/7032, Loss: 1.1164\n",
            "Batch 4750/7032, Loss: 1.1447\n",
            "Batch 4760/7032, Loss: 1.2267\n",
            "Batch 4770/7032, Loss: 1.0690\n",
            "Batch 4780/7032, Loss: 1.0600\n",
            "Batch 4790/7032, Loss: 1.1891\n",
            "Batch 4800/7032, Loss: 1.0295\n",
            "Batch 4810/7032, Loss: 0.8809\n",
            "Batch 4820/7032, Loss: 1.2525\n",
            "Batch 4830/7032, Loss: 1.0549\n",
            "Batch 4840/7032, Loss: 1.1013\n",
            "Batch 4850/7032, Loss: 1.1300\n",
            "Batch 4860/7032, Loss: 1.0100\n",
            "Batch 4870/7032, Loss: 1.1171\n",
            "Batch 4880/7032, Loss: 1.1064\n",
            "Batch 4890/7032, Loss: 1.0525\n",
            "Batch 4900/7032, Loss: 1.0411\n",
            "Batch 4910/7032, Loss: 1.0490\n",
            "Batch 4920/7032, Loss: 1.1136\n",
            "Batch 4930/7032, Loss: 1.0311\n",
            "Batch 4940/7032, Loss: 1.0865\n",
            "Batch 4950/7032, Loss: 1.0881\n",
            "Batch 4960/7032, Loss: 1.1116\n",
            "Batch 4970/7032, Loss: 1.0491\n",
            "Batch 4980/7032, Loss: 1.2301\n",
            "Batch 4990/7032, Loss: 0.9767\n",
            "Batch 5000/7032, Loss: 1.1176\n",
            "Batch 5010/7032, Loss: 1.1595\n",
            "Batch 5020/7032, Loss: 1.1836\n",
            "Batch 5030/7032, Loss: 1.0419\n",
            "Batch 5040/7032, Loss: 1.0947\n",
            "Batch 5050/7032, Loss: 0.9703\n",
            "Batch 5060/7032, Loss: 1.2141\n",
            "Batch 5070/7032, Loss: 0.9783\n",
            "Batch 5080/7032, Loss: 1.0652\n",
            "Batch 5090/7032, Loss: 1.0266\n",
            "Batch 5100/7032, Loss: 0.9651\n",
            "Batch 5110/7032, Loss: 1.2028\n",
            "Batch 5120/7032, Loss: 1.0167\n",
            "Batch 5130/7032, Loss: 0.9743\n",
            "Batch 5140/7032, Loss: 1.0767\n",
            "Batch 5150/7032, Loss: 1.0506\n",
            "Batch 5160/7032, Loss: 1.0687\n",
            "Batch 5170/7032, Loss: 1.0916\n",
            "Batch 5180/7032, Loss: 1.0208\n",
            "Batch 5190/7032, Loss: 1.0470\n",
            "Batch 5200/7032, Loss: 0.9177\n",
            "Batch 5210/7032, Loss: 0.9672\n",
            "Batch 5220/7032, Loss: 0.9837\n",
            "Batch 5230/7032, Loss: 1.0412\n",
            "Batch 5240/7032, Loss: 0.9606\n",
            "Batch 5250/7032, Loss: 1.2066\n",
            "Batch 5260/7032, Loss: 1.0562\n",
            "Batch 5270/7032, Loss: 1.0415\n",
            "Batch 5280/7032, Loss: 1.0142\n",
            "Batch 5290/7032, Loss: 1.0209\n",
            "Batch 5300/7032, Loss: 1.0094\n",
            "Batch 5310/7032, Loss: 0.9311\n",
            "Batch 5320/7032, Loss: 0.9883\n",
            "Batch 5330/7032, Loss: 1.0669\n",
            "Batch 5340/7032, Loss: 1.0608\n",
            "Batch 5350/7032, Loss: 0.9510\n",
            "Batch 5360/7032, Loss: 1.0026\n",
            "Batch 5370/7032, Loss: 1.0295\n",
            "Batch 5380/7032, Loss: 1.1035\n",
            "Batch 5390/7032, Loss: 1.0726\n",
            "Batch 5400/7032, Loss: 1.0622\n",
            "Batch 5410/7032, Loss: 1.1196\n",
            "Batch 5420/7032, Loss: 0.9463\n",
            "Batch 5430/7032, Loss: 0.9274\n",
            "Batch 5440/7032, Loss: 1.0640\n",
            "Batch 5450/7032, Loss: 1.0196\n",
            "Batch 5460/7032, Loss: 1.0644\n",
            "Batch 5470/7032, Loss: 1.1237\n",
            "Batch 5480/7032, Loss: 1.1416\n",
            "Batch 5490/7032, Loss: 0.9907\n",
            "Batch 5500/7032, Loss: 1.1926\n",
            "Batch 5510/7032, Loss: 1.1163\n",
            "Batch 5520/7032, Loss: 1.0862\n",
            "Batch 5530/7032, Loss: 1.0270\n",
            "Batch 5540/7032, Loss: 0.9806\n",
            "Batch 5550/7032, Loss: 1.0246\n",
            "Batch 5560/7032, Loss: 1.0647\n",
            "Batch 5570/7032, Loss: 1.1143\n",
            "Batch 5580/7032, Loss: 1.0474\n",
            "Batch 5590/7032, Loss: 1.0427\n",
            "Batch 5600/7032, Loss: 1.0240\n",
            "Batch 5610/7032, Loss: 1.0454\n",
            "Batch 5620/7032, Loss: 1.0069\n",
            "Batch 5630/7032, Loss: 0.9969\n",
            "Batch 5640/7032, Loss: 1.0346\n",
            "Batch 5650/7032, Loss: 0.8813\n",
            "Batch 5660/7032, Loss: 1.0808\n",
            "Batch 5670/7032, Loss: 0.9729\n",
            "Batch 5680/7032, Loss: 0.9855\n",
            "Batch 5690/7032, Loss: 1.0413\n",
            "Batch 5700/7032, Loss: 1.0030\n",
            "Batch 5710/7032, Loss: 1.0200\n",
            "Batch 5720/7032, Loss: 1.0770\n",
            "Batch 5730/7032, Loss: 1.0673\n",
            "Batch 5740/7032, Loss: 1.0225\n",
            "Batch 5750/7032, Loss: 1.0496\n",
            "Batch 5760/7032, Loss: 1.0273\n",
            "Batch 5770/7032, Loss: 1.0484\n",
            "Batch 5780/7032, Loss: 1.0343\n",
            "Batch 5790/7032, Loss: 1.0083\n",
            "Batch 5800/7032, Loss: 1.1426\n",
            "Batch 5810/7032, Loss: 1.0534\n",
            "Batch 5820/7032, Loss: 0.9389\n",
            "Batch 5830/7032, Loss: 1.1916\n",
            "Batch 5840/7032, Loss: 1.0465\n",
            "Batch 5850/7032, Loss: 1.0404\n",
            "Batch 5860/7032, Loss: 1.0693\n",
            "Batch 5870/7032, Loss: 1.0094\n",
            "Batch 5880/7032, Loss: 1.1770\n",
            "Batch 5890/7032, Loss: 1.1576\n",
            "Batch 5900/7032, Loss: 1.0284\n",
            "Batch 5910/7032, Loss: 1.1729\n",
            "Batch 5920/7032, Loss: 1.1731\n",
            "Batch 5930/7032, Loss: 1.1750\n",
            "Batch 5940/7032, Loss: 1.0389\n",
            "Batch 5950/7032, Loss: 1.1125\n",
            "Batch 5960/7032, Loss: 1.0160\n",
            "Batch 5970/7032, Loss: 1.0861\n",
            "Batch 5980/7032, Loss: 0.9548\n",
            "Batch 5990/7032, Loss: 1.0494\n",
            "Batch 6000/7032, Loss: 1.0644\n",
            "Batch 6010/7032, Loss: 1.1155\n",
            "Batch 6020/7032, Loss: 1.1911\n",
            "Batch 6030/7032, Loss: 1.1189\n",
            "Batch 6040/7032, Loss: 0.9950\n",
            "Batch 6050/7032, Loss: 1.1490\n",
            "Batch 6060/7032, Loss: 1.0494\n",
            "Batch 6070/7032, Loss: 1.0916\n",
            "Batch 6080/7032, Loss: 1.1509\n",
            "Batch 6090/7032, Loss: 0.9381\n",
            "Batch 6100/7032, Loss: 1.0509\n",
            "Batch 6110/7032, Loss: 1.0325\n",
            "Batch 6120/7032, Loss: 1.0733\n",
            "Batch 6130/7032, Loss: 1.1014\n",
            "Batch 6140/7032, Loss: 1.0520\n",
            "Batch 6150/7032, Loss: 1.1236\n",
            "Batch 6160/7032, Loss: 1.0682\n",
            "Batch 6170/7032, Loss: 1.1524\n",
            "Batch 6180/7032, Loss: 1.1057\n",
            "Batch 6190/7032, Loss: 1.0845\n",
            "Batch 6200/7032, Loss: 1.1001\n",
            "Batch 6210/7032, Loss: 1.1430\n",
            "Batch 6220/7032, Loss: 1.1026\n",
            "Batch 6230/7032, Loss: 1.0694\n",
            "Batch 6240/7032, Loss: 1.0314\n",
            "Batch 6250/7032, Loss: 0.9535\n",
            "Batch 6260/7032, Loss: 1.0308\n",
            "Batch 6270/7032, Loss: 1.0543\n",
            "Batch 6280/7032, Loss: 0.9866\n",
            "Batch 6290/7032, Loss: 1.1578\n",
            "Batch 6300/7032, Loss: 1.1280\n",
            "Batch 6310/7032, Loss: 1.1514\n",
            "Batch 6320/7032, Loss: 1.1454\n",
            "Batch 6330/7032, Loss: 1.0256\n",
            "Batch 6340/7032, Loss: 1.0634\n",
            "Batch 6350/7032, Loss: 1.0185\n",
            "Batch 6360/7032, Loss: 0.9247\n",
            "Batch 6370/7032, Loss: 1.0163\n",
            "Batch 6380/7032, Loss: 1.0720\n",
            "Batch 6390/7032, Loss: 0.9732\n",
            "Batch 6400/7032, Loss: 1.0702\n",
            "Batch 6410/7032, Loss: 1.0789\n",
            "Batch 6420/7032, Loss: 1.1325\n",
            "Batch 6430/7032, Loss: 1.1385\n",
            "Batch 6440/7032, Loss: 1.0185\n",
            "Batch 6450/7032, Loss: 1.0350\n",
            "Batch 6460/7032, Loss: 1.1014\n",
            "Batch 6470/7032, Loss: 0.9891\n",
            "Batch 6480/7032, Loss: 1.0742\n",
            "Batch 6490/7032, Loss: 1.1009\n",
            "Batch 6500/7032, Loss: 1.0273\n",
            "Batch 6510/7032, Loss: 0.9358\n",
            "Batch 6520/7032, Loss: 1.0565\n",
            "Batch 6530/7032, Loss: 0.9918\n",
            "Batch 6540/7032, Loss: 1.0679\n",
            "Batch 6550/7032, Loss: 1.1463\n",
            "Batch 6560/7032, Loss: 1.0815\n",
            "Batch 6570/7032, Loss: 1.0584\n",
            "Batch 6580/7032, Loss: 1.1772\n",
            "Batch 6590/7032, Loss: 1.0706\n",
            "Batch 6600/7032, Loss: 1.1196\n",
            "Batch 6610/7032, Loss: 1.1965\n",
            "Batch 6620/7032, Loss: 1.1180\n",
            "Batch 6630/7032, Loss: 1.0377\n",
            "Batch 6640/7032, Loss: 1.1013\n",
            "Batch 6650/7032, Loss: 1.0702\n",
            "Batch 6660/7032, Loss: 1.0366\n",
            "Batch 6670/7032, Loss: 1.1252\n",
            "Batch 6680/7032, Loss: 1.0300\n",
            "Batch 6690/7032, Loss: 1.1658\n",
            "Batch 6700/7032, Loss: 0.9698\n",
            "Batch 6710/7032, Loss: 1.0235\n",
            "Batch 6720/7032, Loss: 1.1151\n",
            "Batch 6730/7032, Loss: 1.1240\n",
            "Batch 6740/7032, Loss: 0.9843\n",
            "Batch 6750/7032, Loss: 1.0361\n",
            "Batch 6760/7032, Loss: 1.1430\n",
            "Batch 6770/7032, Loss: 1.0261\n",
            "Batch 6780/7032, Loss: 1.0544\n",
            "Batch 6790/7032, Loss: 0.9926\n",
            "Batch 6800/7032, Loss: 1.0678\n",
            "Batch 6810/7032, Loss: 1.1438\n",
            "Batch 6820/7032, Loss: 1.0226\n",
            "Batch 6830/7032, Loss: 1.0236\n",
            "Batch 6840/7032, Loss: 0.9891\n",
            "Batch 6850/7032, Loss: 1.0288\n",
            "Batch 6860/7032, Loss: 1.1279\n",
            "Batch 6870/7032, Loss: 1.1251\n",
            "Batch 6880/7032, Loss: 0.9811\n",
            "Batch 6890/7032, Loss: 1.0502\n",
            "Batch 6900/7032, Loss: 1.0103\n",
            "Batch 6910/7032, Loss: 1.0732\n",
            "Batch 6920/7032, Loss: 1.0408\n",
            "Batch 6930/7032, Loss: 1.0484\n",
            "Batch 6940/7032, Loss: 1.2310\n",
            "Batch 6950/7032, Loss: 0.9824\n",
            "Batch 6960/7032, Loss: 0.9299\n",
            "Batch 6970/7032, Loss: 1.0102\n",
            "Batch 6980/7032, Loss: 1.0649\n",
            "Batch 6990/7032, Loss: 1.0233\n",
            "Batch 7000/7032, Loss: 1.1180\n",
            "Batch 7010/7032, Loss: 1.0463\n",
            "Batch 7020/7032, Loss: 1.2186\n",
            "Batch 7030/7032, Loss: 0.8563\n",
            "Batch 7032/7032, Loss: 0.9453\n",
            "Epoch 6/10\n",
            "Batch 10/7032, Loss: 1.1260\n",
            "Batch 20/7032, Loss: 1.0658\n",
            "Batch 30/7032, Loss: 1.0561\n",
            "Batch 40/7032, Loss: 1.0853\n",
            "Batch 50/7032, Loss: 1.1166\n",
            "Batch 60/7032, Loss: 0.8412\n",
            "Batch 70/7032, Loss: 0.9856\n",
            "Batch 80/7032, Loss: 1.1033\n",
            "Batch 90/7032, Loss: 1.1202\n",
            "Batch 100/7032, Loss: 1.0754\n",
            "Batch 110/7032, Loss: 1.0721\n",
            "Batch 120/7032, Loss: 1.1060\n",
            "Batch 130/7032, Loss: 1.0344\n",
            "Batch 140/7032, Loss: 1.0939\n",
            "Batch 150/7032, Loss: 1.0029\n",
            "Batch 160/7032, Loss: 0.8873\n",
            "Batch 170/7032, Loss: 1.1618\n",
            "Batch 180/7032, Loss: 0.9627\n",
            "Batch 190/7032, Loss: 1.0118\n",
            "Batch 200/7032, Loss: 0.9407\n",
            "Batch 210/7032, Loss: 1.1095\n",
            "Batch 220/7032, Loss: 0.9110\n",
            "Batch 230/7032, Loss: 0.8916\n",
            "Batch 240/7032, Loss: 1.1207\n",
            "Batch 250/7032, Loss: 1.0887\n",
            "Batch 260/7032, Loss: 1.0612\n",
            "Batch 270/7032, Loss: 1.1962\n",
            "Batch 280/7032, Loss: 1.0747\n",
            "Batch 290/7032, Loss: 1.0354\n",
            "Batch 300/7032, Loss: 1.0325\n",
            "Batch 310/7032, Loss: 1.0225\n",
            "Batch 320/7032, Loss: 1.1023\n",
            "Batch 330/7032, Loss: 1.0948\n",
            "Batch 340/7032, Loss: 0.8667\n",
            "Batch 350/7032, Loss: 1.0573\n",
            "Batch 360/7032, Loss: 1.0688\n",
            "Batch 370/7032, Loss: 1.0017\n",
            "Batch 380/7032, Loss: 0.9594\n",
            "Batch 390/7032, Loss: 1.0295\n",
            "Batch 400/7032, Loss: 1.1704\n",
            "Batch 410/7032, Loss: 1.0559\n",
            "Batch 420/7032, Loss: 0.9305\n",
            "Batch 430/7032, Loss: 1.0597\n",
            "Batch 440/7032, Loss: 0.9051\n",
            "Batch 450/7032, Loss: 1.1721\n",
            "Batch 460/7032, Loss: 1.1802\n",
            "Batch 470/7032, Loss: 1.0359\n",
            "Batch 480/7032, Loss: 1.0887\n",
            "Batch 490/7032, Loss: 1.1447\n",
            "Batch 500/7032, Loss: 1.1021\n",
            "Batch 510/7032, Loss: 1.0196\n",
            "Batch 520/7032, Loss: 1.1350\n",
            "Batch 530/7032, Loss: 1.2438\n",
            "Batch 540/7032, Loss: 1.0795\n",
            "Batch 550/7032, Loss: 1.1169\n",
            "Batch 560/7032, Loss: 1.1686\n",
            "Batch 570/7032, Loss: 1.1321\n",
            "Batch 580/7032, Loss: 1.0304\n",
            "Batch 590/7032, Loss: 1.0137\n",
            "Batch 600/7032, Loss: 0.9784\n",
            "Batch 610/7032, Loss: 1.0035\n",
            "Batch 620/7032, Loss: 1.0005\n",
            "Batch 630/7032, Loss: 0.9342\n",
            "Batch 640/7032, Loss: 0.9339\n",
            "Batch 650/7032, Loss: 1.1101\n",
            "Batch 660/7032, Loss: 1.0536\n",
            "Batch 670/7032, Loss: 0.9414\n",
            "Batch 680/7032, Loss: 0.9412\n",
            "Batch 690/7032, Loss: 1.0754\n",
            "Batch 700/7032, Loss: 1.1569\n",
            "Batch 710/7032, Loss: 1.0630\n",
            "Batch 720/7032, Loss: 1.0621\n",
            "Batch 730/7032, Loss: 0.9127\n",
            "Batch 740/7032, Loss: 1.1396\n",
            "Batch 750/7032, Loss: 0.9932\n",
            "Batch 760/7032, Loss: 1.1966\n",
            "Batch 770/7032, Loss: 1.0128\n",
            "Batch 780/7032, Loss: 1.1410\n",
            "Batch 790/7032, Loss: 0.9590\n",
            "Batch 800/7032, Loss: 1.0575\n",
            "Batch 810/7032, Loss: 1.0196\n",
            "Batch 820/7032, Loss: 1.0305\n",
            "Batch 830/7032, Loss: 1.0261\n",
            "Batch 840/7032, Loss: 1.1105\n",
            "Batch 850/7032, Loss: 0.9189\n",
            "Batch 860/7032, Loss: 1.0633\n",
            "Batch 870/7032, Loss: 0.9170\n",
            "Batch 880/7032, Loss: 1.1091\n",
            "Batch 890/7032, Loss: 1.0673\n",
            "Batch 900/7032, Loss: 1.0744\n",
            "Batch 910/7032, Loss: 1.0597\n",
            "Batch 920/7032, Loss: 0.9599\n",
            "Batch 930/7032, Loss: 0.8757\n",
            "Batch 940/7032, Loss: 1.0413\n",
            "Batch 950/7032, Loss: 1.0586\n",
            "Batch 960/7032, Loss: 0.9641\n",
            "Batch 970/7032, Loss: 1.0966\n",
            "Batch 980/7032, Loss: 1.0461\n",
            "Batch 990/7032, Loss: 1.0918\n",
            "Batch 1000/7032, Loss: 1.0538\n",
            "Batch 1010/7032, Loss: 0.9470\n",
            "Batch 1020/7032, Loss: 1.0402\n",
            "Batch 1030/7032, Loss: 1.0766\n",
            "Batch 1040/7032, Loss: 0.9878\n",
            "Batch 1050/7032, Loss: 0.9562\n",
            "Batch 1060/7032, Loss: 1.0844\n",
            "Batch 1070/7032, Loss: 1.0807\n",
            "Batch 1080/7032, Loss: 1.0500\n",
            "Batch 1090/7032, Loss: 1.0212\n",
            "Batch 1100/7032, Loss: 0.9622\n",
            "Batch 1110/7032, Loss: 1.0512\n",
            "Batch 1120/7032, Loss: 1.0158\n",
            "Batch 1130/7032, Loss: 1.0510\n",
            "Batch 1140/7032, Loss: 1.0224\n",
            "Batch 1150/7032, Loss: 1.0496\n",
            "Batch 1160/7032, Loss: 0.9188\n",
            "Batch 1170/7032, Loss: 1.0517\n",
            "Batch 1180/7032, Loss: 1.1857\n",
            "Batch 1190/7032, Loss: 0.9673\n",
            "Batch 1200/7032, Loss: 1.0853\n",
            "Batch 1210/7032, Loss: 1.0267\n",
            "Batch 1220/7032, Loss: 0.9632\n",
            "Batch 1230/7032, Loss: 1.1542\n",
            "Batch 1240/7032, Loss: 1.0419\n",
            "Batch 1250/7032, Loss: 1.0893\n",
            "Batch 1260/7032, Loss: 1.0794\n",
            "Batch 1270/7032, Loss: 1.0689\n",
            "Batch 1280/7032, Loss: 1.0437\n",
            "Batch 1290/7032, Loss: 1.1473\n",
            "Batch 1300/7032, Loss: 0.9221\n",
            "Batch 1310/7032, Loss: 0.9677\n",
            "Batch 1320/7032, Loss: 0.9498\n",
            "Batch 1330/7032, Loss: 1.0168\n",
            "Batch 1340/7032, Loss: 1.0703\n",
            "Batch 1350/7032, Loss: 0.9190\n",
            "Batch 1360/7032, Loss: 1.0263\n",
            "Batch 1370/7032, Loss: 1.0805\n",
            "Batch 1380/7032, Loss: 1.0582\n",
            "Batch 1390/7032, Loss: 1.0855\n",
            "Batch 1400/7032, Loss: 1.0438\n",
            "Batch 1410/7032, Loss: 1.0265\n",
            "Batch 1420/7032, Loss: 0.9659\n",
            "Batch 1430/7032, Loss: 1.0444\n",
            "Batch 1440/7032, Loss: 1.0771\n",
            "Batch 1450/7032, Loss: 1.0475\n",
            "Batch 1460/7032, Loss: 0.8943\n",
            "Batch 1470/7032, Loss: 1.0817\n",
            "Batch 1480/7032, Loss: 1.0469\n",
            "Batch 1490/7032, Loss: 0.9591\n",
            "Batch 1500/7032, Loss: 0.9699\n",
            "Batch 1510/7032, Loss: 1.1717\n",
            "Batch 1520/7032, Loss: 1.0213\n",
            "Batch 1530/7032, Loss: 0.8637\n",
            "Batch 1540/7032, Loss: 1.0501\n",
            "Batch 1550/7032, Loss: 0.9870\n",
            "Batch 1560/7032, Loss: 1.0887\n",
            "Batch 1570/7032, Loss: 1.0538\n",
            "Batch 1580/7032, Loss: 0.9975\n",
            "Batch 1590/7032, Loss: 1.1303\n",
            "Batch 1600/7032, Loss: 0.9097\n",
            "Batch 1610/7032, Loss: 1.0173\n",
            "Batch 1620/7032, Loss: 0.9688\n",
            "Batch 1630/7032, Loss: 1.1515\n",
            "Batch 1640/7032, Loss: 0.9337\n",
            "Batch 1650/7032, Loss: 0.9143\n",
            "Batch 1660/7032, Loss: 1.0602\n",
            "Batch 1670/7032, Loss: 1.0398\n",
            "Batch 1680/7032, Loss: 0.9215\n",
            "Batch 1690/7032, Loss: 1.0336\n",
            "Batch 1700/7032, Loss: 0.9531\n",
            "Batch 1710/7032, Loss: 1.0729\n",
            "Batch 1720/7032, Loss: 1.0383\n",
            "Batch 1730/7032, Loss: 0.9021\n",
            "Batch 1740/7032, Loss: 0.9224\n",
            "Batch 1750/7032, Loss: 1.0199\n",
            "Batch 1760/7032, Loss: 1.0703\n",
            "Batch 1770/7032, Loss: 1.1366\n",
            "Batch 1780/7032, Loss: 1.0450\n",
            "Batch 1790/7032, Loss: 1.1305\n",
            "Batch 1800/7032, Loss: 0.9442\n",
            "Batch 1810/7032, Loss: 1.0139\n",
            "Batch 1820/7032, Loss: 1.1206\n",
            "Batch 1830/7032, Loss: 0.9696\n",
            "Batch 1840/7032, Loss: 1.0670\n",
            "Batch 1850/7032, Loss: 0.9905\n",
            "Batch 1860/7032, Loss: 1.0201\n",
            "Batch 1870/7032, Loss: 1.0575\n",
            "Batch 1880/7032, Loss: 1.0562\n",
            "Batch 1890/7032, Loss: 1.0592\n",
            "Batch 1900/7032, Loss: 1.0018\n",
            "Batch 1910/7032, Loss: 1.0117\n",
            "Batch 1920/7032, Loss: 1.1191\n",
            "Batch 1930/7032, Loss: 0.9576\n",
            "Batch 1940/7032, Loss: 1.0408\n",
            "Batch 1950/7032, Loss: 1.0536\n",
            "Batch 1960/7032, Loss: 1.0312\n",
            "Batch 1970/7032, Loss: 1.1314\n",
            "Batch 1980/7032, Loss: 1.0415\n",
            "Batch 1990/7032, Loss: 1.0424\n",
            "Batch 2000/7032, Loss: 1.0608\n",
            "Batch 2010/7032, Loss: 1.0048\n",
            "Batch 2020/7032, Loss: 0.9721\n",
            "Batch 2030/7032, Loss: 0.9302\n",
            "Batch 2040/7032, Loss: 1.0373\n",
            "Batch 2050/7032, Loss: 1.0784\n",
            "Batch 2060/7032, Loss: 1.0293\n",
            "Batch 2070/7032, Loss: 1.0473\n",
            "Batch 2080/7032, Loss: 1.0390\n",
            "Batch 2090/7032, Loss: 0.9079\n",
            "Batch 2100/7032, Loss: 0.9840\n",
            "Batch 2110/7032, Loss: 1.2011\n",
            "Batch 2120/7032, Loss: 0.9586\n",
            "Batch 2130/7032, Loss: 1.0464\n",
            "Batch 2140/7032, Loss: 1.0212\n",
            "Batch 2150/7032, Loss: 1.0164\n",
            "Batch 2160/7032, Loss: 1.0539\n",
            "Batch 2170/7032, Loss: 0.9900\n",
            "Batch 2180/7032, Loss: 0.9494\n",
            "Batch 2190/7032, Loss: 0.8572\n",
            "Batch 2200/7032, Loss: 0.9821\n",
            "Batch 2210/7032, Loss: 1.0130\n",
            "Batch 2220/7032, Loss: 1.0178\n",
            "Batch 2230/7032, Loss: 1.1043\n",
            "Batch 2240/7032, Loss: 0.9770\n",
            "Batch 2250/7032, Loss: 1.0064\n",
            "Batch 2260/7032, Loss: 1.0490\n",
            "Batch 2270/7032, Loss: 1.0110\n",
            "Batch 2280/7032, Loss: 1.0900\n",
            "Batch 2290/7032, Loss: 1.0755\n",
            "Batch 2300/7032, Loss: 1.1577\n",
            "Batch 2310/7032, Loss: 1.1548\n",
            "Batch 2320/7032, Loss: 1.0780\n",
            "Batch 2330/7032, Loss: 1.0896\n",
            "Batch 2340/7032, Loss: 0.8973\n",
            "Batch 2350/7032, Loss: 1.0750\n",
            "Batch 2360/7032, Loss: 1.0212\n",
            "Batch 2370/7032, Loss: 0.9963\n",
            "Batch 2380/7032, Loss: 0.9711\n",
            "Batch 2390/7032, Loss: 0.9752\n",
            "Batch 2400/7032, Loss: 1.0477\n",
            "Batch 2410/7032, Loss: 0.9529\n",
            "Batch 2420/7032, Loss: 0.9820\n",
            "Batch 2430/7032, Loss: 1.0636\n",
            "Batch 2440/7032, Loss: 1.1403\n",
            "Batch 2450/7032, Loss: 1.1411\n",
            "Batch 2460/7032, Loss: 1.0952\n",
            "Batch 2470/7032, Loss: 1.0537\n",
            "Batch 2480/7032, Loss: 0.9938\n",
            "Batch 2490/7032, Loss: 0.9143\n",
            "Batch 2500/7032, Loss: 1.0851\n",
            "Batch 2510/7032, Loss: 1.0406\n",
            "Batch 2520/7032, Loss: 1.1012\n",
            "Batch 2530/7032, Loss: 0.9849\n",
            "Batch 2540/7032, Loss: 0.9588\n",
            "Batch 2550/7032, Loss: 1.0604\n",
            "Batch 2560/7032, Loss: 1.1485\n",
            "Batch 2570/7032, Loss: 1.0969\n",
            "Batch 2580/7032, Loss: 1.0669\n",
            "Batch 2590/7032, Loss: 1.0137\n",
            "Batch 2600/7032, Loss: 1.1121\n",
            "Batch 2610/7032, Loss: 0.9724\n",
            "Batch 2620/7032, Loss: 0.9269\n",
            "Batch 2630/7032, Loss: 1.0707\n",
            "Batch 2640/7032, Loss: 1.1394\n",
            "Batch 2650/7032, Loss: 1.0370\n",
            "Batch 2660/7032, Loss: 1.0522\n",
            "Batch 2670/7032, Loss: 0.9412\n",
            "Batch 2680/7032, Loss: 0.9777\n",
            "Batch 2690/7032, Loss: 1.1163\n",
            "Batch 2700/7032, Loss: 1.2155\n",
            "Batch 2710/7032, Loss: 1.0598\n",
            "Batch 2720/7032, Loss: 1.1243\n",
            "Batch 2730/7032, Loss: 1.1245\n",
            "Batch 2740/7032, Loss: 1.0870\n",
            "Batch 2750/7032, Loss: 1.1023\n",
            "Batch 2760/7032, Loss: 1.0628\n",
            "Batch 2770/7032, Loss: 1.0077\n",
            "Batch 2780/7032, Loss: 1.1496\n",
            "Batch 2790/7032, Loss: 1.0982\n",
            "Batch 2800/7032, Loss: 1.0716\n",
            "Batch 2810/7032, Loss: 0.9633\n",
            "Batch 2820/7032, Loss: 1.0111\n",
            "Batch 2830/7032, Loss: 1.0680\n",
            "Batch 2840/7032, Loss: 0.9715\n",
            "Batch 2850/7032, Loss: 1.0816\n",
            "Batch 2860/7032, Loss: 1.0558\n",
            "Batch 2870/7032, Loss: 0.9384\n",
            "Batch 2880/7032, Loss: 1.0451\n",
            "Batch 2890/7032, Loss: 0.9975\n",
            "Batch 2900/7032, Loss: 1.1729\n",
            "Batch 2910/7032, Loss: 1.0912\n",
            "Batch 2920/7032, Loss: 1.0944\n",
            "Batch 2930/7032, Loss: 1.0641\n",
            "Batch 2940/7032, Loss: 1.1356\n",
            "Batch 2950/7032, Loss: 1.0439\n",
            "Batch 2960/7032, Loss: 0.9607\n",
            "Batch 2970/7032, Loss: 1.0946\n",
            "Batch 2980/7032, Loss: 1.0139\n",
            "Batch 2990/7032, Loss: 1.0068\n",
            "Batch 3000/7032, Loss: 1.0168\n",
            "Batch 3010/7032, Loss: 1.0177\n",
            "Batch 3020/7032, Loss: 1.0183\n",
            "Batch 3030/7032, Loss: 1.0636\n",
            "Batch 3040/7032, Loss: 1.0106\n",
            "Batch 3050/7032, Loss: 0.9982\n",
            "Batch 3060/7032, Loss: 1.1576\n",
            "Batch 3070/7032, Loss: 1.0999\n",
            "Batch 3080/7032, Loss: 1.0687\n",
            "Batch 3090/7032, Loss: 1.0087\n",
            "Batch 3100/7032, Loss: 1.0457\n",
            "Batch 3110/7032, Loss: 1.0910\n",
            "Batch 3120/7032, Loss: 1.0515\n",
            "Batch 3130/7032, Loss: 0.9948\n",
            "Batch 3140/7032, Loss: 1.0486\n",
            "Batch 3150/7032, Loss: 1.0447\n",
            "Batch 3160/7032, Loss: 1.1077\n",
            "Batch 3170/7032, Loss: 1.1037\n",
            "Batch 3180/7032, Loss: 1.0398\n",
            "Batch 3190/7032, Loss: 0.9072\n",
            "Batch 3200/7032, Loss: 0.9781\n",
            "Batch 3210/7032, Loss: 0.9917\n",
            "Batch 3220/7032, Loss: 1.0810\n",
            "Batch 3230/7032, Loss: 0.9757\n",
            "Batch 3240/7032, Loss: 1.0984\n",
            "Batch 3250/7032, Loss: 1.0352\n",
            "Batch 3260/7032, Loss: 1.0452\n",
            "Batch 3270/7032, Loss: 1.0465\n",
            "Batch 3280/7032, Loss: 1.0227\n",
            "Batch 3290/7032, Loss: 1.0995\n",
            "Batch 3300/7032, Loss: 1.0104\n",
            "Batch 3310/7032, Loss: 1.0089\n",
            "Batch 3320/7032, Loss: 1.0711\n",
            "Batch 3330/7032, Loss: 1.0259\n",
            "Batch 3340/7032, Loss: 1.1057\n",
            "Batch 3350/7032, Loss: 1.1877\n",
            "Batch 3360/7032, Loss: 0.9616\n",
            "Batch 3370/7032, Loss: 0.9025\n",
            "Batch 3380/7032, Loss: 1.0886\n",
            "Batch 3390/7032, Loss: 0.9533\n",
            "Batch 3400/7032, Loss: 0.9664\n",
            "Batch 3410/7032, Loss: 0.9531\n",
            "Batch 3420/7032, Loss: 0.9440\n",
            "Batch 3430/7032, Loss: 1.0340\n",
            "Batch 3440/7032, Loss: 1.0740\n",
            "Batch 3450/7032, Loss: 1.1405\n",
            "Batch 3460/7032, Loss: 1.0406\n",
            "Batch 3470/7032, Loss: 1.1142\n",
            "Batch 3480/7032, Loss: 0.9452\n",
            "Batch 3490/7032, Loss: 0.9648\n",
            "Batch 3500/7032, Loss: 1.0456\n",
            "Batch 3510/7032, Loss: 1.1374\n",
            "Batch 3520/7032, Loss: 0.9905\n",
            "Batch 3530/7032, Loss: 0.9075\n",
            "Batch 3540/7032, Loss: 1.0544\n",
            "Batch 3550/7032, Loss: 1.0767\n",
            "Batch 3560/7032, Loss: 1.0971\n",
            "Batch 3570/7032, Loss: 0.9948\n",
            "Batch 3580/7032, Loss: 1.0655\n",
            "Batch 3590/7032, Loss: 1.1000\n",
            "Batch 3600/7032, Loss: 1.0805\n",
            "Batch 3610/7032, Loss: 1.0134\n",
            "Batch 3620/7032, Loss: 1.0644\n",
            "Batch 3630/7032, Loss: 1.0947\n",
            "Batch 3640/7032, Loss: 1.0499\n",
            "Batch 3650/7032, Loss: 1.0535\n",
            "Batch 3660/7032, Loss: 0.9902\n",
            "Batch 3670/7032, Loss: 0.8947\n",
            "Batch 3680/7032, Loss: 1.0274\n",
            "Batch 3690/7032, Loss: 1.1380\n",
            "Batch 3700/7032, Loss: 1.0284\n",
            "Batch 3710/7032, Loss: 1.0842\n",
            "Batch 3720/7032, Loss: 1.1296\n",
            "Batch 3730/7032, Loss: 1.0520\n",
            "Batch 3740/7032, Loss: 0.9789\n",
            "Batch 3750/7032, Loss: 1.1792\n",
            "Batch 3760/7032, Loss: 1.0102\n",
            "Batch 3770/7032, Loss: 0.9123\n",
            "Batch 3780/7032, Loss: 1.1526\n",
            "Batch 3790/7032, Loss: 0.9378\n",
            "Batch 3800/7032, Loss: 1.1194\n",
            "Batch 3810/7032, Loss: 1.0074\n",
            "Batch 3820/7032, Loss: 1.0435\n",
            "Batch 3830/7032, Loss: 1.0888\n",
            "Batch 3840/7032, Loss: 1.0501\n",
            "Batch 3850/7032, Loss: 1.0552\n",
            "Batch 3860/7032, Loss: 1.0394\n",
            "Batch 3870/7032, Loss: 1.0908\n",
            "Batch 3880/7032, Loss: 0.9082\n",
            "Batch 3890/7032, Loss: 1.0946\n",
            "Batch 3900/7032, Loss: 1.0455\n",
            "Batch 3910/7032, Loss: 0.9738\n",
            "Batch 3920/7032, Loss: 1.0503\n",
            "Batch 3930/7032, Loss: 0.9567\n",
            "Batch 3940/7032, Loss: 1.0004\n",
            "Batch 3950/7032, Loss: 1.1324\n",
            "Batch 3960/7032, Loss: 0.9763\n",
            "Batch 3970/7032, Loss: 0.9964\n",
            "Batch 3980/7032, Loss: 1.0614\n",
            "Batch 3990/7032, Loss: 1.0395\n",
            "Batch 4000/7032, Loss: 1.1784\n",
            "Batch 4010/7032, Loss: 1.0924\n",
            "Batch 4020/7032, Loss: 0.9518\n",
            "Batch 4030/7032, Loss: 1.0165\n",
            "Batch 4040/7032, Loss: 1.0139\n",
            "Batch 4050/7032, Loss: 1.0805\n",
            "Batch 4060/7032, Loss: 0.9874\n",
            "Batch 4070/7032, Loss: 1.0116\n",
            "Batch 4080/7032, Loss: 1.0178\n",
            "Batch 4090/7032, Loss: 1.0684\n",
            "Batch 4100/7032, Loss: 0.9750\n",
            "Batch 4110/7032, Loss: 1.0042\n",
            "Batch 4120/7032, Loss: 0.9944\n",
            "Batch 4130/7032, Loss: 1.0007\n",
            "Batch 4140/7032, Loss: 0.9737\n",
            "Batch 4150/7032, Loss: 1.0057\n",
            "Batch 4160/7032, Loss: 1.0361\n",
            "Batch 4170/7032, Loss: 1.0011\n",
            "Batch 4180/7032, Loss: 1.0553\n",
            "Batch 4190/7032, Loss: 1.0562\n",
            "Batch 4200/7032, Loss: 1.0816\n",
            "Batch 4210/7032, Loss: 1.1234\n",
            "Batch 4220/7032, Loss: 1.0075\n",
            "Batch 4230/7032, Loss: 1.0525\n",
            "Batch 4240/7032, Loss: 0.9169\n",
            "Batch 4250/7032, Loss: 1.0409\n",
            "Batch 4260/7032, Loss: 0.9794\n",
            "Batch 4270/7032, Loss: 1.0923\n",
            "Batch 4280/7032, Loss: 1.0174\n",
            "Batch 4290/7032, Loss: 0.9964\n",
            "Batch 4300/7032, Loss: 1.1815\n",
            "Batch 4310/7032, Loss: 0.9678\n",
            "Batch 4320/7032, Loss: 1.1459\n",
            "Batch 4330/7032, Loss: 1.0968\n",
            "Batch 4340/7032, Loss: 1.0826\n",
            "Batch 4350/7032, Loss: 0.9739\n",
            "Batch 4360/7032, Loss: 0.9825\n",
            "Batch 4370/7032, Loss: 0.9689\n",
            "Batch 4380/7032, Loss: 0.9976\n",
            "Batch 4390/7032, Loss: 1.1537\n",
            "Batch 4400/7032, Loss: 1.1411\n",
            "Batch 4410/7032, Loss: 1.0401\n",
            "Batch 4420/7032, Loss: 1.1215\n",
            "Batch 4430/7032, Loss: 0.9586\n",
            "Batch 4440/7032, Loss: 1.1202\n",
            "Batch 4450/7032, Loss: 0.9903\n",
            "Batch 4460/7032, Loss: 1.0217\n",
            "Batch 4470/7032, Loss: 1.0981\n",
            "Batch 4480/7032, Loss: 1.0076\n",
            "Batch 4490/7032, Loss: 0.9847\n",
            "Batch 4500/7032, Loss: 1.1107\n",
            "Batch 4510/7032, Loss: 1.0411\n",
            "Batch 4520/7032, Loss: 0.8622\n",
            "Batch 4530/7032, Loss: 1.0843\n",
            "Batch 4540/7032, Loss: 1.1377\n",
            "Batch 4550/7032, Loss: 1.0816\n",
            "Batch 4560/7032, Loss: 1.0279\n",
            "Batch 4570/7032, Loss: 0.9451\n",
            "Batch 4580/7032, Loss: 0.9875\n",
            "Batch 4590/7032, Loss: 0.9744\n",
            "Batch 4600/7032, Loss: 1.0043\n",
            "Batch 4610/7032, Loss: 1.0721\n",
            "Batch 4620/7032, Loss: 1.0534\n",
            "Batch 4630/7032, Loss: 1.1291\n",
            "Batch 4640/7032, Loss: 1.0154\n",
            "Batch 4650/7032, Loss: 1.1238\n",
            "Batch 4660/7032, Loss: 1.1041\n",
            "Batch 4670/7032, Loss: 0.9680\n",
            "Batch 4680/7032, Loss: 1.1597\n",
            "Batch 4690/7032, Loss: 1.0428\n",
            "Batch 4700/7032, Loss: 1.0964\n",
            "Batch 4710/7032, Loss: 1.1191\n",
            "Batch 4720/7032, Loss: 0.9973\n",
            "Batch 4730/7032, Loss: 1.0419\n",
            "Batch 4740/7032, Loss: 1.0621\n",
            "Batch 4750/7032, Loss: 1.0315\n",
            "Batch 4760/7032, Loss: 1.0593\n",
            "Batch 4770/7032, Loss: 0.9535\n",
            "Batch 4780/7032, Loss: 0.9976\n",
            "Batch 4790/7032, Loss: 0.9696\n",
            "Batch 4800/7032, Loss: 0.8890\n",
            "Batch 4810/7032, Loss: 1.0129\n",
            "Batch 4820/7032, Loss: 0.9740\n",
            "Batch 4830/7032, Loss: 0.8885\n",
            "Batch 4840/7032, Loss: 0.9921\n",
            "Batch 4850/7032, Loss: 1.0030\n",
            "Batch 4860/7032, Loss: 1.0307\n",
            "Batch 4870/7032, Loss: 0.9599\n",
            "Batch 4880/7032, Loss: 1.0910\n",
            "Batch 4890/7032, Loss: 1.0517\n",
            "Batch 4900/7032, Loss: 1.1289\n",
            "Batch 4910/7032, Loss: 1.0877\n",
            "Batch 4920/7032, Loss: 0.9899\n",
            "Batch 4930/7032, Loss: 0.9302\n",
            "Batch 4940/7032, Loss: 1.0421\n",
            "Batch 4950/7032, Loss: 1.0128\n",
            "Batch 4960/7032, Loss: 0.9824\n",
            "Batch 4970/7032, Loss: 1.0570\n",
            "Batch 4980/7032, Loss: 1.0036\n",
            "Batch 4990/7032, Loss: 1.0510\n",
            "Batch 5000/7032, Loss: 0.9842\n",
            "Batch 5010/7032, Loss: 1.0325\n",
            "Batch 5020/7032, Loss: 1.0319\n",
            "Batch 5030/7032, Loss: 1.0305\n",
            "Batch 5040/7032, Loss: 1.0610\n",
            "Batch 5050/7032, Loss: 1.0968\n",
            "Batch 5060/7032, Loss: 0.9721\n",
            "Batch 5070/7032, Loss: 1.0239\n",
            "Batch 5080/7032, Loss: 1.0590\n",
            "Batch 5090/7032, Loss: 1.0894\n",
            "Batch 5100/7032, Loss: 0.8238\n",
            "Batch 5110/7032, Loss: 1.1515\n",
            "Batch 5120/7032, Loss: 0.9635\n",
            "Batch 5130/7032, Loss: 1.0460\n",
            "Batch 5140/7032, Loss: 1.1212\n",
            "Batch 5150/7032, Loss: 1.1728\n",
            "Batch 5160/7032, Loss: 1.0913\n",
            "Batch 5170/7032, Loss: 0.9263\n",
            "Batch 5180/7032, Loss: 1.0293\n",
            "Batch 5190/7032, Loss: 0.9402\n",
            "Batch 5200/7032, Loss: 1.0412\n",
            "Batch 5210/7032, Loss: 1.1034\n",
            "Batch 5220/7032, Loss: 1.0252\n",
            "Batch 5230/7032, Loss: 1.0641\n",
            "Batch 5240/7032, Loss: 1.0888\n",
            "Batch 5250/7032, Loss: 0.9333\n",
            "Batch 5260/7032, Loss: 1.0012\n",
            "Batch 5270/7032, Loss: 0.9632\n",
            "Batch 5280/7032, Loss: 0.9750\n",
            "Batch 5290/7032, Loss: 0.8233\n",
            "Batch 5300/7032, Loss: 0.9609\n",
            "Batch 5310/7032, Loss: 0.9852\n",
            "Batch 5320/7032, Loss: 0.9639\n",
            "Batch 5330/7032, Loss: 1.0347\n",
            "Batch 5340/7032, Loss: 1.0861\n",
            "Batch 5350/7032, Loss: 0.9492\n",
            "Batch 5360/7032, Loss: 1.0367\n",
            "Batch 5370/7032, Loss: 1.0148\n",
            "Batch 5380/7032, Loss: 1.1337\n",
            "Batch 5390/7032, Loss: 1.0624\n",
            "Batch 5400/7032, Loss: 1.0797\n",
            "Batch 5410/7032, Loss: 0.9781\n",
            "Batch 5420/7032, Loss: 1.0387\n",
            "Batch 5430/7032, Loss: 1.0315\n",
            "Batch 5440/7032, Loss: 0.8564\n",
            "Batch 5450/7032, Loss: 1.0789\n",
            "Batch 5460/7032, Loss: 1.1573\n",
            "Batch 5470/7032, Loss: 0.9677\n",
            "Batch 5480/7032, Loss: 0.9260\n",
            "Batch 5490/7032, Loss: 0.9650\n",
            "Batch 5500/7032, Loss: 0.9985\n",
            "Batch 5510/7032, Loss: 1.0113\n",
            "Batch 5520/7032, Loss: 0.9925\n",
            "Batch 5530/7032, Loss: 1.1138\n",
            "Batch 5540/7032, Loss: 1.1158\n",
            "Batch 5550/7032, Loss: 1.0927\n",
            "Batch 5560/7032, Loss: 1.0536\n",
            "Batch 5570/7032, Loss: 1.0314\n",
            "Batch 5580/7032, Loss: 1.0694\n",
            "Batch 5590/7032, Loss: 1.1942\n",
            "Batch 5600/7032, Loss: 1.0986\n",
            "Batch 5610/7032, Loss: 0.9418\n",
            "Batch 5620/7032, Loss: 1.0778\n",
            "Batch 5630/7032, Loss: 1.0568\n",
            "Batch 5640/7032, Loss: 0.9283\n",
            "Batch 5650/7032, Loss: 0.8678\n",
            "Batch 5660/7032, Loss: 1.0625\n",
            "Batch 5670/7032, Loss: 0.9664\n",
            "Batch 5680/7032, Loss: 1.0510\n",
            "Batch 5690/7032, Loss: 0.9323\n",
            "Batch 5700/7032, Loss: 0.9834\n",
            "Batch 5710/7032, Loss: 1.0656\n",
            "Batch 5720/7032, Loss: 1.0591\n",
            "Batch 5730/7032, Loss: 0.9960\n",
            "Batch 5740/7032, Loss: 1.0671\n",
            "Batch 5750/7032, Loss: 0.8416\n",
            "Batch 5760/7032, Loss: 1.0089\n",
            "Batch 5770/7032, Loss: 0.9970\n",
            "Batch 5780/7032, Loss: 1.1231\n",
            "Batch 5790/7032, Loss: 1.0687\n",
            "Batch 5800/7032, Loss: 0.9568\n",
            "Batch 5810/7032, Loss: 1.0244\n",
            "Batch 5820/7032, Loss: 1.1229\n",
            "Batch 5830/7032, Loss: 1.0148\n",
            "Batch 5840/7032, Loss: 0.9728\n",
            "Batch 5850/7032, Loss: 1.0189\n",
            "Batch 5860/7032, Loss: 1.0608\n",
            "Batch 5870/7032, Loss: 0.9123\n",
            "Batch 5880/7032, Loss: 1.1018\n",
            "Batch 5890/7032, Loss: 1.1425\n",
            "Batch 5900/7032, Loss: 1.0229\n",
            "Batch 5910/7032, Loss: 1.0833\n",
            "Batch 5920/7032, Loss: 1.1930\n",
            "Batch 5930/7032, Loss: 0.9637\n",
            "Batch 5940/7032, Loss: 1.0306\n",
            "Batch 5950/7032, Loss: 1.0280\n",
            "Batch 5960/7032, Loss: 1.0230\n",
            "Batch 5970/7032, Loss: 1.0385\n",
            "Batch 5980/7032, Loss: 0.9521\n",
            "Batch 5990/7032, Loss: 0.9431\n",
            "Batch 6000/7032, Loss: 0.9852\n",
            "Batch 6010/7032, Loss: 1.0753\n",
            "Batch 6020/7032, Loss: 1.1459\n",
            "Batch 6030/7032, Loss: 1.0284\n",
            "Batch 6040/7032, Loss: 0.8555\n",
            "Batch 6050/7032, Loss: 1.0265\n",
            "Batch 6060/7032, Loss: 0.9839\n",
            "Batch 6070/7032, Loss: 0.9380\n",
            "Batch 6080/7032, Loss: 1.0607\n",
            "Batch 6090/7032, Loss: 1.0685\n",
            "Batch 6100/7032, Loss: 1.0528\n",
            "Batch 6110/7032, Loss: 0.9361\n",
            "Batch 6120/7032, Loss: 1.0491\n",
            "Batch 6130/7032, Loss: 1.0834\n",
            "Batch 6140/7032, Loss: 1.0358\n",
            "Batch 6150/7032, Loss: 1.0964\n",
            "Batch 6160/7032, Loss: 0.9337\n",
            "Batch 6170/7032, Loss: 1.0506\n",
            "Batch 6180/7032, Loss: 0.9765\n",
            "Batch 6190/7032, Loss: 1.0666\n",
            "Batch 6200/7032, Loss: 0.9676\n",
            "Batch 6210/7032, Loss: 1.0133\n",
            "Batch 6220/7032, Loss: 0.9838\n",
            "Batch 6230/7032, Loss: 0.9728\n",
            "Batch 6240/7032, Loss: 0.9412\n",
            "Batch 6250/7032, Loss: 1.0443\n",
            "Batch 6260/7032, Loss: 1.0206\n",
            "Batch 6270/7032, Loss: 1.0693\n",
            "Batch 6280/7032, Loss: 1.0377\n",
            "Batch 6290/7032, Loss: 1.1402\n",
            "Batch 6300/7032, Loss: 1.0999\n",
            "Batch 6310/7032, Loss: 1.0354\n",
            "Batch 6320/7032, Loss: 1.0334\n",
            "Batch 6330/7032, Loss: 1.1950\n",
            "Batch 6340/7032, Loss: 1.0002\n",
            "Batch 6350/7032, Loss: 1.0784\n",
            "Batch 6360/7032, Loss: 1.1057\n",
            "Batch 6370/7032, Loss: 1.0100\n",
            "Batch 6380/7032, Loss: 1.1829\n",
            "Batch 6390/7032, Loss: 0.9900\n",
            "Batch 6400/7032, Loss: 1.0614\n",
            "Batch 6410/7032, Loss: 0.9744\n",
            "Batch 6420/7032, Loss: 0.9758\n",
            "Batch 6430/7032, Loss: 1.1922\n",
            "Batch 6440/7032, Loss: 0.9910\n",
            "Batch 6450/7032, Loss: 1.1712\n",
            "Batch 6460/7032, Loss: 1.0587\n",
            "Batch 6470/7032, Loss: 1.0560\n",
            "Batch 6480/7032, Loss: 1.0261\n",
            "Batch 6490/7032, Loss: 0.9535\n",
            "Batch 6500/7032, Loss: 1.0352\n",
            "Batch 6510/7032, Loss: 1.0419\n",
            "Batch 6520/7032, Loss: 0.9498\n",
            "Batch 6530/7032, Loss: 1.0841\n",
            "Batch 6540/7032, Loss: 0.9781\n",
            "Batch 6550/7032, Loss: 0.8757\n",
            "Batch 6560/7032, Loss: 0.7323\n",
            "Batch 6570/7032, Loss: 1.0656\n",
            "Batch 6580/7032, Loss: 1.0097\n",
            "Batch 6590/7032, Loss: 1.1236\n",
            "Batch 6600/7032, Loss: 1.0836\n",
            "Batch 6610/7032, Loss: 1.0546\n",
            "Batch 6620/7032, Loss: 0.9647\n",
            "Batch 6630/7032, Loss: 1.0481\n",
            "Batch 6640/7032, Loss: 1.0113\n",
            "Batch 6650/7032, Loss: 0.9943\n",
            "Batch 6660/7032, Loss: 1.0166\n",
            "Batch 6670/7032, Loss: 0.9210\n",
            "Batch 6680/7032, Loss: 0.9691\n",
            "Batch 6690/7032, Loss: 1.0911\n",
            "Batch 6700/7032, Loss: 0.9867\n",
            "Batch 6710/7032, Loss: 1.0954\n",
            "Batch 6720/7032, Loss: 1.1375\n",
            "Batch 6730/7032, Loss: 1.0145\n",
            "Batch 6740/7032, Loss: 1.0756\n",
            "Batch 6750/7032, Loss: 1.0309\n",
            "Batch 6760/7032, Loss: 0.9370\n",
            "Batch 6770/7032, Loss: 1.1571\n",
            "Batch 6780/7032, Loss: 0.9933\n",
            "Batch 6790/7032, Loss: 1.0404\n",
            "Batch 6800/7032, Loss: 1.0999\n",
            "Batch 6810/7032, Loss: 0.9389\n",
            "Batch 6820/7032, Loss: 1.0104\n",
            "Batch 6830/7032, Loss: 1.0685\n",
            "Batch 6840/7032, Loss: 1.0306\n",
            "Batch 6850/7032, Loss: 0.9180\n",
            "Batch 6860/7032, Loss: 0.8972\n",
            "Batch 6870/7032, Loss: 1.1329\n",
            "Batch 6880/7032, Loss: 0.9645\n",
            "Batch 6890/7032, Loss: 0.9955\n",
            "Batch 6900/7032, Loss: 0.9877\n",
            "Batch 6910/7032, Loss: 0.9487\n",
            "Batch 6920/7032, Loss: 1.0665\n",
            "Batch 6930/7032, Loss: 1.0202\n",
            "Batch 6940/7032, Loss: 1.0369\n",
            "Batch 6950/7032, Loss: 1.0802\n",
            "Batch 6960/7032, Loss: 0.9820\n",
            "Batch 6970/7032, Loss: 1.0725\n",
            "Batch 6980/7032, Loss: 0.9831\n",
            "Batch 6990/7032, Loss: 1.1849\n",
            "Batch 7000/7032, Loss: 0.8935\n",
            "Batch 7010/7032, Loss: 1.1397\n",
            "Batch 7020/7032, Loss: 1.0948\n",
            "Batch 7030/7032, Loss: 1.1316\n",
            "Batch 7032/7032, Loss: 1.2432\n",
            "Epoch 7/10\n",
            "Batch 10/7032, Loss: 0.8858\n",
            "Batch 20/7032, Loss: 1.1214\n",
            "Batch 30/7032, Loss: 0.9901\n",
            "Batch 40/7032, Loss: 0.9425\n",
            "Batch 50/7032, Loss: 0.9861\n",
            "Batch 60/7032, Loss: 0.9661\n",
            "Batch 70/7032, Loss: 1.1142\n",
            "Batch 80/7032, Loss: 1.0831\n",
            "Batch 90/7032, Loss: 0.8763\n",
            "Batch 100/7032, Loss: 1.0086\n",
            "Batch 110/7032, Loss: 1.0216\n",
            "Batch 120/7032, Loss: 0.9788\n",
            "Batch 130/7032, Loss: 1.0074\n",
            "Batch 140/7032, Loss: 1.0640\n",
            "Batch 150/7032, Loss: 1.0383\n",
            "Batch 160/7032, Loss: 1.0457\n",
            "Batch 170/7032, Loss: 0.8686\n",
            "Batch 180/7032, Loss: 0.9948\n",
            "Batch 190/7032, Loss: 0.8950\n",
            "Batch 200/7032, Loss: 1.0772\n",
            "Batch 210/7032, Loss: 1.0202\n",
            "Batch 220/7032, Loss: 1.1465\n",
            "Batch 230/7032, Loss: 1.0264\n",
            "Batch 240/7032, Loss: 1.0041\n",
            "Batch 250/7032, Loss: 0.9699\n",
            "Batch 260/7032, Loss: 0.9704\n",
            "Batch 270/7032, Loss: 0.9568\n",
            "Batch 280/7032, Loss: 0.8870\n",
            "Batch 290/7032, Loss: 1.0107\n",
            "Batch 300/7032, Loss: 1.0412\n",
            "Batch 310/7032, Loss: 0.8621\n",
            "Batch 320/7032, Loss: 1.0091\n",
            "Batch 330/7032, Loss: 1.0556\n",
            "Batch 340/7032, Loss: 1.0259\n",
            "Batch 350/7032, Loss: 0.9556\n",
            "Batch 360/7032, Loss: 0.9238\n",
            "Batch 370/7032, Loss: 1.0974\n",
            "Batch 380/7032, Loss: 0.9437\n",
            "Batch 390/7032, Loss: 0.9570\n",
            "Batch 400/7032, Loss: 1.0762\n",
            "Batch 410/7032, Loss: 1.0157\n",
            "Batch 420/7032, Loss: 0.9952\n",
            "Batch 430/7032, Loss: 0.9643\n",
            "Batch 440/7032, Loss: 1.0745\n",
            "Batch 450/7032, Loss: 0.9588\n",
            "Batch 460/7032, Loss: 1.0285\n",
            "Batch 470/7032, Loss: 0.9899\n",
            "Batch 480/7032, Loss: 1.0598\n",
            "Batch 490/7032, Loss: 1.0273\n",
            "Batch 500/7032, Loss: 1.0816\n",
            "Batch 510/7032, Loss: 0.9352\n",
            "Batch 520/7032, Loss: 1.0594\n",
            "Batch 530/7032, Loss: 1.0592\n",
            "Batch 540/7032, Loss: 0.8244\n",
            "Batch 550/7032, Loss: 0.9746\n",
            "Batch 560/7032, Loss: 0.9781\n",
            "Batch 570/7032, Loss: 1.0431\n",
            "Batch 580/7032, Loss: 1.0687\n",
            "Batch 590/7032, Loss: 0.8893\n",
            "Batch 600/7032, Loss: 0.9225\n",
            "Batch 610/7032, Loss: 1.1178\n",
            "Batch 620/7032, Loss: 0.9555\n",
            "Batch 630/7032, Loss: 0.9861\n",
            "Batch 640/7032, Loss: 0.9976\n",
            "Batch 650/7032, Loss: 0.9913\n",
            "Batch 660/7032, Loss: 1.0617\n",
            "Batch 670/7032, Loss: 0.8054\n",
            "Batch 680/7032, Loss: 1.0367\n",
            "Batch 690/7032, Loss: 0.9590\n",
            "Batch 700/7032, Loss: 0.9178\n",
            "Batch 710/7032, Loss: 1.0143\n",
            "Batch 720/7032, Loss: 1.1636\n",
            "Batch 730/7032, Loss: 1.0290\n",
            "Batch 740/7032, Loss: 1.0553\n",
            "Batch 750/7032, Loss: 1.0295\n",
            "Batch 760/7032, Loss: 1.0192\n",
            "Batch 770/7032, Loss: 0.9438\n",
            "Batch 780/7032, Loss: 0.9107\n",
            "Batch 790/7032, Loss: 0.9775\n",
            "Batch 800/7032, Loss: 1.1071\n",
            "Batch 810/7032, Loss: 1.0964\n",
            "Batch 820/7032, Loss: 0.9452\n",
            "Batch 830/7032, Loss: 0.9229\n",
            "Batch 840/7032, Loss: 1.0217\n",
            "Batch 850/7032, Loss: 1.0748\n",
            "Batch 860/7032, Loss: 0.9017\n",
            "Batch 870/7032, Loss: 1.1009\n",
            "Batch 880/7032, Loss: 1.0345\n",
            "Batch 890/7032, Loss: 0.9768\n",
            "Batch 900/7032, Loss: 0.9241\n",
            "Batch 910/7032, Loss: 1.0000\n",
            "Batch 920/7032, Loss: 1.0539\n",
            "Batch 930/7032, Loss: 1.0626\n",
            "Batch 940/7032, Loss: 1.0979\n",
            "Batch 950/7032, Loss: 0.9467\n",
            "Batch 960/7032, Loss: 0.9649\n",
            "Batch 970/7032, Loss: 1.0011\n",
            "Batch 980/7032, Loss: 1.0322\n",
            "Batch 990/7032, Loss: 1.1050\n",
            "Batch 1000/7032, Loss: 0.8918\n",
            "Batch 1010/7032, Loss: 1.0403\n",
            "Batch 1020/7032, Loss: 1.0660\n",
            "Batch 1030/7032, Loss: 1.0180\n",
            "Batch 1040/7032, Loss: 0.9381\n",
            "Batch 1050/7032, Loss: 0.9126\n",
            "Batch 1060/7032, Loss: 1.0809\n",
            "Batch 1070/7032, Loss: 1.1130\n",
            "Batch 1080/7032, Loss: 1.0590\n",
            "Batch 1090/7032, Loss: 1.0083\n",
            "Batch 1100/7032, Loss: 1.2184\n",
            "Batch 1110/7032, Loss: 0.9333\n",
            "Batch 1120/7032, Loss: 1.0174\n",
            "Batch 1130/7032, Loss: 1.0132\n",
            "Batch 1140/7032, Loss: 0.9695\n",
            "Batch 1150/7032, Loss: 1.0370\n",
            "Batch 1160/7032, Loss: 0.9052\n",
            "Batch 1170/7032, Loss: 1.0093\n",
            "Batch 1180/7032, Loss: 1.0299\n",
            "Batch 1190/7032, Loss: 1.0255\n",
            "Batch 1200/7032, Loss: 0.8553\n",
            "Batch 1210/7032, Loss: 0.9781\n",
            "Batch 1220/7032, Loss: 1.0403\n",
            "Batch 1230/7032, Loss: 0.9427\n",
            "Batch 1240/7032, Loss: 1.0057\n",
            "Batch 1250/7032, Loss: 0.9633\n",
            "Batch 1260/7032, Loss: 0.9728\n",
            "Batch 1270/7032, Loss: 0.9228\n",
            "Batch 1280/7032, Loss: 1.0756\n",
            "Batch 1290/7032, Loss: 0.9776\n",
            "Batch 1300/7032, Loss: 1.0077\n",
            "Batch 1310/7032, Loss: 0.9967\n",
            "Batch 1320/7032, Loss: 0.9652\n",
            "Batch 1330/7032, Loss: 1.0801\n",
            "Batch 1340/7032, Loss: 1.0766\n",
            "Batch 1350/7032, Loss: 1.0467\n",
            "Batch 1360/7032, Loss: 0.9416\n",
            "Batch 1370/7032, Loss: 1.0516\n",
            "Batch 1380/7032, Loss: 1.1069\n",
            "Batch 1390/7032, Loss: 1.0138\n",
            "Batch 1400/7032, Loss: 1.0400\n",
            "Batch 1410/7032, Loss: 0.8588\n",
            "Batch 1420/7032, Loss: 1.0091\n",
            "Batch 1430/7032, Loss: 0.9561\n",
            "Batch 1440/7032, Loss: 0.9590\n",
            "Batch 1450/7032, Loss: 1.0650\n",
            "Batch 1460/7032, Loss: 0.9215\n",
            "Batch 1470/7032, Loss: 0.8809\n",
            "Batch 1480/7032, Loss: 1.0188\n",
            "Batch 1490/7032, Loss: 0.9241\n",
            "Batch 1500/7032, Loss: 1.0710\n",
            "Batch 1510/7032, Loss: 0.9365\n",
            "Batch 1520/7032, Loss: 1.0566\n",
            "Batch 1530/7032, Loss: 0.9177\n",
            "Batch 1540/7032, Loss: 1.0069\n",
            "Batch 1550/7032, Loss: 0.9806\n",
            "Batch 1560/7032, Loss: 0.9721\n",
            "Batch 1570/7032, Loss: 1.0558\n",
            "Batch 1580/7032, Loss: 1.0075\n",
            "Batch 1590/7032, Loss: 0.9495\n",
            "Batch 1600/7032, Loss: 0.9515\n",
            "Batch 1610/7032, Loss: 1.0509\n",
            "Batch 1620/7032, Loss: 0.9685\n",
            "Batch 1630/7032, Loss: 0.9244\n",
            "Batch 1640/7032, Loss: 0.9912\n",
            "Batch 1650/7032, Loss: 1.0370\n",
            "Batch 1660/7032, Loss: 1.0648\n",
            "Batch 1670/7032, Loss: 1.0130\n",
            "Batch 1680/7032, Loss: 0.9959\n",
            "Batch 1690/7032, Loss: 1.0172\n",
            "Batch 1700/7032, Loss: 1.0101\n",
            "Batch 1710/7032, Loss: 1.0453\n",
            "Batch 1720/7032, Loss: 0.9506\n",
            "Batch 1730/7032, Loss: 1.0003\n",
            "Batch 1740/7032, Loss: 0.9693\n",
            "Batch 1750/7032, Loss: 0.8798\n",
            "Batch 1760/7032, Loss: 0.9757\n",
            "Batch 1770/7032, Loss: 1.0706\n",
            "Batch 1780/7032, Loss: 0.9157\n",
            "Batch 1790/7032, Loss: 0.9835\n",
            "Batch 1800/7032, Loss: 1.0244\n",
            "Batch 1810/7032, Loss: 1.1139\n",
            "Batch 1820/7032, Loss: 1.0833\n",
            "Batch 1830/7032, Loss: 1.0115\n",
            "Batch 1840/7032, Loss: 0.9785\n",
            "Batch 1850/7032, Loss: 0.9740\n",
            "Batch 1860/7032, Loss: 0.9410\n",
            "Batch 1870/7032, Loss: 0.9224\n",
            "Batch 1880/7032, Loss: 0.8644\n",
            "Batch 1890/7032, Loss: 1.0399\n",
            "Batch 1900/7032, Loss: 0.9317\n",
            "Batch 1910/7032, Loss: 0.9941\n",
            "Batch 1920/7032, Loss: 0.8867\n",
            "Batch 1930/7032, Loss: 0.9151\n",
            "Batch 1940/7032, Loss: 0.9057\n",
            "Batch 1950/7032, Loss: 1.0772\n",
            "Batch 1960/7032, Loss: 1.0229\n",
            "Batch 1970/7032, Loss: 1.1660\n",
            "Batch 1980/7032, Loss: 1.1216\n",
            "Batch 1990/7032, Loss: 1.0754\n",
            "Batch 2000/7032, Loss: 0.9889\n",
            "Batch 2010/7032, Loss: 0.9262\n",
            "Batch 2020/7032, Loss: 0.9935\n",
            "Batch 2030/7032, Loss: 0.9806\n",
            "Batch 2040/7032, Loss: 1.0233\n",
            "Batch 2050/7032, Loss: 0.9269\n",
            "Batch 2060/7032, Loss: 1.0940\n",
            "Batch 2070/7032, Loss: 0.9857\n",
            "Batch 2080/7032, Loss: 0.9759\n",
            "Batch 2090/7032, Loss: 1.0005\n",
            "Batch 2100/7032, Loss: 0.9779\n",
            "Batch 2110/7032, Loss: 0.9979\n",
            "Batch 2120/7032, Loss: 0.9351\n",
            "Batch 2130/7032, Loss: 0.9522\n",
            "Batch 2140/7032, Loss: 1.0788\n",
            "Batch 2150/7032, Loss: 1.1060\n",
            "Batch 2160/7032, Loss: 0.9795\n",
            "Batch 2170/7032, Loss: 1.0092\n",
            "Batch 2180/7032, Loss: 0.9992\n",
            "Batch 2190/7032, Loss: 1.0004\n",
            "Batch 2200/7032, Loss: 1.0942\n",
            "Batch 2210/7032, Loss: 1.1742\n",
            "Batch 2220/7032, Loss: 0.9020\n",
            "Batch 2230/7032, Loss: 1.0309\n",
            "Batch 2240/7032, Loss: 1.0072\n",
            "Batch 2250/7032, Loss: 0.9985\n",
            "Batch 2260/7032, Loss: 1.0235\n",
            "Batch 2270/7032, Loss: 0.9474\n",
            "Batch 2280/7032, Loss: 0.9925\n",
            "Batch 2290/7032, Loss: 1.0349\n",
            "Batch 2300/7032, Loss: 1.0093\n",
            "Batch 2310/7032, Loss: 1.0488\n",
            "Batch 2320/7032, Loss: 1.0513\n",
            "Batch 2330/7032, Loss: 1.0770\n",
            "Batch 2340/7032, Loss: 1.1092\n",
            "Batch 2350/7032, Loss: 0.9109\n",
            "Batch 2360/7032, Loss: 0.9826\n",
            "Batch 2370/7032, Loss: 1.0229\n",
            "Batch 2380/7032, Loss: 1.1750\n",
            "Batch 2390/7032, Loss: 0.9869\n",
            "Batch 2400/7032, Loss: 0.8891\n",
            "Batch 2410/7032, Loss: 0.9174\n",
            "Batch 2420/7032, Loss: 0.9825\n",
            "Batch 2430/7032, Loss: 0.9777\n",
            "Batch 2440/7032, Loss: 1.0997\n",
            "Batch 2450/7032, Loss: 1.0060\n",
            "Batch 2460/7032, Loss: 0.9661\n",
            "Batch 2470/7032, Loss: 0.9753\n",
            "Batch 2480/7032, Loss: 0.9535\n",
            "Batch 2490/7032, Loss: 1.0660\n",
            "Batch 2500/7032, Loss: 0.8562\n",
            "Batch 2510/7032, Loss: 0.9723\n",
            "Batch 2520/7032, Loss: 0.9890\n",
            "Batch 2530/7032, Loss: 0.9811\n",
            "Batch 2540/7032, Loss: 0.8246\n",
            "Batch 2550/7032, Loss: 0.9777\n",
            "Batch 2560/7032, Loss: 1.0173\n",
            "Batch 2570/7032, Loss: 0.9834\n",
            "Batch 2580/7032, Loss: 1.0354\n",
            "Batch 2590/7032, Loss: 1.0383\n",
            "Batch 2600/7032, Loss: 0.8630\n",
            "Batch 2610/7032, Loss: 0.9757\n",
            "Batch 2620/7032, Loss: 0.9548\n",
            "Batch 2630/7032, Loss: 0.9185\n",
            "Batch 2640/7032, Loss: 0.9897\n",
            "Batch 2650/7032, Loss: 0.8770\n",
            "Batch 2660/7032, Loss: 1.1436\n",
            "Batch 2670/7032, Loss: 1.1131\n",
            "Batch 2680/7032, Loss: 0.9257\n",
            "Batch 2690/7032, Loss: 1.0301\n",
            "Batch 2700/7032, Loss: 0.8745\n",
            "Batch 2710/7032, Loss: 0.9745\n",
            "Batch 2720/7032, Loss: 0.9592\n",
            "Batch 2730/7032, Loss: 1.0113\n",
            "Batch 2740/7032, Loss: 0.9802\n",
            "Batch 2750/7032, Loss: 1.0076\n",
            "Batch 2760/7032, Loss: 1.0709\n",
            "Batch 2770/7032, Loss: 0.9968\n",
            "Batch 2780/7032, Loss: 1.0315\n",
            "Batch 2790/7032, Loss: 0.9830\n",
            "Batch 2800/7032, Loss: 1.0141\n",
            "Batch 2810/7032, Loss: 0.9950\n",
            "Batch 2820/7032, Loss: 0.9928\n",
            "Batch 2830/7032, Loss: 1.0175\n",
            "Batch 2840/7032, Loss: 0.8864\n",
            "Batch 2850/7032, Loss: 1.0625\n",
            "Batch 2860/7032, Loss: 0.8589\n",
            "Batch 2870/7032, Loss: 0.8782\n",
            "Batch 2880/7032, Loss: 0.9327\n",
            "Batch 2890/7032, Loss: 1.0257\n",
            "Batch 2900/7032, Loss: 0.9527\n",
            "Batch 2910/7032, Loss: 0.9323\n",
            "Batch 2920/7032, Loss: 0.9533\n",
            "Batch 2930/7032, Loss: 1.0267\n",
            "Batch 2940/7032, Loss: 0.9123\n",
            "Batch 2950/7032, Loss: 0.9595\n",
            "Batch 2960/7032, Loss: 0.9705\n",
            "Batch 2970/7032, Loss: 0.9110\n",
            "Batch 2980/7032, Loss: 0.9336\n",
            "Batch 2990/7032, Loss: 0.9785\n",
            "Batch 3000/7032, Loss: 0.9016\n",
            "Batch 3010/7032, Loss: 0.9619\n",
            "Batch 3020/7032, Loss: 1.0544\n",
            "Batch 3030/7032, Loss: 1.0428\n",
            "Batch 3040/7032, Loss: 1.0617\n",
            "Batch 3050/7032, Loss: 1.0345\n",
            "Batch 3060/7032, Loss: 0.9236\n",
            "Batch 3070/7032, Loss: 1.0200\n",
            "Batch 3080/7032, Loss: 1.0687\n",
            "Batch 3090/7032, Loss: 1.0352\n",
            "Batch 3100/7032, Loss: 1.0319\n",
            "Batch 3110/7032, Loss: 0.8352\n",
            "Batch 3120/7032, Loss: 1.1231\n",
            "Batch 3130/7032, Loss: 0.9814\n",
            "Batch 3140/7032, Loss: 1.0440\n",
            "Batch 3150/7032, Loss: 0.9124\n",
            "Batch 3160/7032, Loss: 0.9946\n",
            "Batch 3170/7032, Loss: 1.0867\n",
            "Batch 3180/7032, Loss: 0.9129\n",
            "Batch 3190/7032, Loss: 0.8881\n",
            "Batch 3200/7032, Loss: 1.0940\n",
            "Batch 3210/7032, Loss: 0.9658\n",
            "Batch 3220/7032, Loss: 0.9896\n",
            "Batch 3230/7032, Loss: 0.8867\n",
            "Batch 3240/7032, Loss: 1.0317\n",
            "Batch 3250/7032, Loss: 1.0411\n",
            "Batch 3260/7032, Loss: 0.9462\n",
            "Batch 3270/7032, Loss: 0.9643\n",
            "Batch 3280/7032, Loss: 0.8019\n",
            "Batch 3290/7032, Loss: 1.0594\n",
            "Batch 3300/7032, Loss: 1.0410\n",
            "Batch 3310/7032, Loss: 0.9763\n",
            "Batch 3320/7032, Loss: 1.0069\n",
            "Batch 3330/7032, Loss: 1.0480\n",
            "Batch 3340/7032, Loss: 1.0294\n",
            "Batch 3350/7032, Loss: 0.9977\n",
            "Batch 3360/7032, Loss: 1.0062\n",
            "Batch 3370/7032, Loss: 1.0387\n",
            "Batch 3380/7032, Loss: 0.8883\n",
            "Batch 3390/7032, Loss: 0.9075\n",
            "Batch 3400/7032, Loss: 0.9533\n",
            "Batch 3410/7032, Loss: 0.8590\n",
            "Batch 3420/7032, Loss: 1.0090\n",
            "Batch 3430/7032, Loss: 0.9969\n",
            "Batch 3440/7032, Loss: 0.9977\n",
            "Batch 3450/7032, Loss: 0.9946\n",
            "Batch 3460/7032, Loss: 0.9483\n",
            "Batch 3470/7032, Loss: 1.0553\n",
            "Batch 3480/7032, Loss: 1.0594\n",
            "Batch 3490/7032, Loss: 0.9598\n",
            "Batch 3500/7032, Loss: 1.1019\n",
            "Batch 3510/7032, Loss: 1.0025\n",
            "Batch 3520/7032, Loss: 0.8997\n",
            "Batch 3530/7032, Loss: 1.0426\n",
            "Batch 3540/7032, Loss: 1.0689\n",
            "Batch 3550/7032, Loss: 0.9839\n",
            "Batch 3560/7032, Loss: 1.0365\n",
            "Batch 3570/7032, Loss: 1.0289\n",
            "Batch 3580/7032, Loss: 1.0739\n",
            "Batch 3590/7032, Loss: 1.0780\n",
            "Batch 3600/7032, Loss: 0.8795\n",
            "Batch 3610/7032, Loss: 0.9459\n",
            "Batch 3620/7032, Loss: 1.0000\n",
            "Batch 3630/7032, Loss: 0.9883\n",
            "Batch 3640/7032, Loss: 0.9419\n",
            "Batch 3650/7032, Loss: 0.9354\n",
            "Batch 3660/7032, Loss: 1.0510\n",
            "Batch 3670/7032, Loss: 0.9867\n",
            "Batch 3680/7032, Loss: 1.0178\n",
            "Batch 3690/7032, Loss: 1.0733\n",
            "Batch 3700/7032, Loss: 0.8876\n",
            "Batch 3710/7032, Loss: 0.9844\n",
            "Batch 3720/7032, Loss: 0.9248\n",
            "Batch 3730/7032, Loss: 1.0562\n",
            "Batch 3740/7032, Loss: 1.0590\n",
            "Batch 3750/7032, Loss: 1.0249\n",
            "Batch 3760/7032, Loss: 1.1202\n",
            "Batch 3770/7032, Loss: 1.0121\n",
            "Batch 3780/7032, Loss: 0.9868\n",
            "Batch 3790/7032, Loss: 1.0492\n",
            "Batch 3800/7032, Loss: 1.0669\n",
            "Batch 3810/7032, Loss: 1.0472\n",
            "Batch 3820/7032, Loss: 0.9614\n",
            "Batch 3830/7032, Loss: 1.0285\n",
            "Batch 3840/7032, Loss: 0.9967\n",
            "Batch 3850/7032, Loss: 0.8670\n",
            "Batch 3860/7032, Loss: 1.1417\n",
            "Batch 3870/7032, Loss: 1.0187\n",
            "Batch 3880/7032, Loss: 0.9212\n",
            "Batch 3890/7032, Loss: 0.9872\n",
            "Batch 3900/7032, Loss: 1.0957\n",
            "Batch 3910/7032, Loss: 1.0155\n",
            "Batch 3920/7032, Loss: 0.9714\n",
            "Batch 3930/7032, Loss: 1.0469\n",
            "Batch 3940/7032, Loss: 0.9235\n",
            "Batch 3950/7032, Loss: 0.9015\n",
            "Batch 3960/7032, Loss: 1.0245\n",
            "Batch 3970/7032, Loss: 1.1157\n",
            "Batch 3980/7032, Loss: 1.0077\n",
            "Batch 3990/7032, Loss: 0.9641\n",
            "Batch 4000/7032, Loss: 1.0416\n",
            "Batch 4010/7032, Loss: 1.0728\n",
            "Batch 4020/7032, Loss: 0.9781\n",
            "Batch 4030/7032, Loss: 1.0345\n",
            "Batch 4040/7032, Loss: 1.0373\n",
            "Batch 4050/7032, Loss: 1.0030\n",
            "Batch 4060/7032, Loss: 1.1248\n",
            "Batch 4070/7032, Loss: 0.9406\n",
            "Batch 4080/7032, Loss: 0.9737\n",
            "Batch 4090/7032, Loss: 0.9541\n",
            "Batch 4100/7032, Loss: 0.9726\n",
            "Batch 4110/7032, Loss: 1.0253\n",
            "Batch 4120/7032, Loss: 0.9979\n",
            "Batch 4130/7032, Loss: 0.9996\n",
            "Batch 4140/7032, Loss: 0.9562\n",
            "Batch 4150/7032, Loss: 0.9801\n",
            "Batch 4160/7032, Loss: 0.9699\n",
            "Batch 4170/7032, Loss: 0.9161\n",
            "Batch 4180/7032, Loss: 1.0610\n",
            "Batch 4190/7032, Loss: 0.9894\n",
            "Batch 4200/7032, Loss: 1.0347\n",
            "Batch 4210/7032, Loss: 0.9869\n",
            "Batch 4220/7032, Loss: 0.9606\n",
            "Batch 4230/7032, Loss: 0.9026\n",
            "Batch 4240/7032, Loss: 0.9743\n",
            "Batch 4250/7032, Loss: 1.1978\n",
            "Batch 4260/7032, Loss: 1.0325\n",
            "Batch 4270/7032, Loss: 1.1091\n",
            "Batch 4280/7032, Loss: 0.8788\n",
            "Batch 4290/7032, Loss: 0.9730\n",
            "Batch 4300/7032, Loss: 0.8732\n",
            "Batch 4310/7032, Loss: 0.9187\n",
            "Batch 4320/7032, Loss: 0.8884\n",
            "Batch 4330/7032, Loss: 0.9747\n",
            "Batch 4340/7032, Loss: 0.9686\n",
            "Batch 4350/7032, Loss: 1.1015\n",
            "Batch 4360/7032, Loss: 0.9621\n",
            "Batch 4370/7032, Loss: 1.0027\n",
            "Batch 4380/7032, Loss: 0.9702\n",
            "Batch 4390/7032, Loss: 1.0527\n",
            "Batch 4400/7032, Loss: 1.0730\n",
            "Batch 4410/7032, Loss: 0.9924\n",
            "Batch 4420/7032, Loss: 1.0534\n",
            "Batch 4430/7032, Loss: 1.0918\n",
            "Batch 4440/7032, Loss: 1.1431\n",
            "Batch 4450/7032, Loss: 0.9596\n",
            "Batch 4460/7032, Loss: 1.0194\n",
            "Batch 4470/7032, Loss: 1.0202\n",
            "Batch 4480/7032, Loss: 1.0580\n",
            "Batch 4490/7032, Loss: 1.1330\n",
            "Batch 4500/7032, Loss: 0.8861\n",
            "Batch 4510/7032, Loss: 0.9942\n",
            "Batch 4520/7032, Loss: 0.9604\n",
            "Batch 4530/7032, Loss: 1.0533\n",
            "Batch 4540/7032, Loss: 1.0508\n",
            "Batch 4550/7032, Loss: 0.9524\n",
            "Batch 4560/7032, Loss: 0.9373\n",
            "Batch 4570/7032, Loss: 0.9897\n",
            "Batch 4580/7032, Loss: 1.0273\n",
            "Batch 4590/7032, Loss: 0.9268\n",
            "Batch 4600/7032, Loss: 1.0457\n",
            "Batch 4610/7032, Loss: 1.0829\n",
            "Batch 4620/7032, Loss: 0.9969\n",
            "Batch 4630/7032, Loss: 1.0702\n",
            "Batch 4640/7032, Loss: 0.8962\n",
            "Batch 4650/7032, Loss: 0.8529\n",
            "Batch 4660/7032, Loss: 0.8380\n",
            "Batch 4670/7032, Loss: 0.9957\n",
            "Batch 4680/7032, Loss: 0.8514\n",
            "Batch 4690/7032, Loss: 0.9663\n",
            "Batch 4700/7032, Loss: 0.9166\n",
            "Batch 4710/7032, Loss: 1.0156\n",
            "Batch 4720/7032, Loss: 0.9669\n",
            "Batch 4730/7032, Loss: 0.9659\n",
            "Batch 4740/7032, Loss: 0.9517\n",
            "Batch 4750/7032, Loss: 0.9121\n",
            "Batch 4760/7032, Loss: 1.0022\n",
            "Batch 4770/7032, Loss: 1.0257\n",
            "Batch 4780/7032, Loss: 0.9125\n",
            "Batch 4790/7032, Loss: 0.9834\n",
            "Batch 4800/7032, Loss: 1.0199\n",
            "Batch 4810/7032, Loss: 1.0028\n",
            "Batch 4820/7032, Loss: 0.8590\n",
            "Batch 4830/7032, Loss: 1.0009\n",
            "Batch 4840/7032, Loss: 0.9742\n",
            "Batch 4850/7032, Loss: 0.9610\n",
            "Batch 4860/7032, Loss: 1.0767\n",
            "Batch 4870/7032, Loss: 0.8821\n",
            "Batch 4880/7032, Loss: 1.0634\n",
            "Batch 4890/7032, Loss: 1.0592\n",
            "Batch 4900/7032, Loss: 0.9753\n",
            "Batch 4910/7032, Loss: 1.0266\n",
            "Batch 4920/7032, Loss: 1.0274\n",
            "Batch 4930/7032, Loss: 0.9916\n",
            "Batch 4940/7032, Loss: 1.0236\n",
            "Batch 4950/7032, Loss: 0.9986\n",
            "Batch 4960/7032, Loss: 0.9238\n",
            "Batch 4970/7032, Loss: 1.1180\n",
            "Batch 4980/7032, Loss: 1.0093\n",
            "Batch 4990/7032, Loss: 1.1004\n",
            "Batch 5000/7032, Loss: 0.9909\n",
            "Batch 5010/7032, Loss: 1.0103\n",
            "Batch 5020/7032, Loss: 0.9858\n",
            "Batch 5030/7032, Loss: 0.9784\n",
            "Batch 5040/7032, Loss: 1.0615\n",
            "Batch 5050/7032, Loss: 1.0049\n",
            "Batch 5060/7032, Loss: 1.0219\n",
            "Batch 5070/7032, Loss: 0.9866\n",
            "Batch 5080/7032, Loss: 0.8177\n",
            "Batch 5090/7032, Loss: 0.9550\n",
            "Batch 5100/7032, Loss: 0.8057\n",
            "Batch 5110/7032, Loss: 1.1181\n",
            "Batch 5120/7032, Loss: 0.9746\n",
            "Batch 5130/7032, Loss: 0.9549\n",
            "Batch 5140/7032, Loss: 0.9194\n",
            "Batch 5150/7032, Loss: 0.9836\n",
            "Batch 5160/7032, Loss: 1.1525\n",
            "Batch 5170/7032, Loss: 0.9680\n",
            "Batch 5180/7032, Loss: 1.0830\n",
            "Batch 5190/7032, Loss: 1.0778\n",
            "Batch 5200/7032, Loss: 0.9372\n",
            "Batch 5210/7032, Loss: 1.0796\n",
            "Batch 5220/7032, Loss: 0.9242\n",
            "Batch 5230/7032, Loss: 1.0275\n",
            "Batch 5240/7032, Loss: 0.7944\n",
            "Batch 5250/7032, Loss: 0.9078\n",
            "Batch 5260/7032, Loss: 0.9699\n",
            "Batch 5270/7032, Loss: 1.0020\n",
            "Batch 5280/7032, Loss: 0.8796\n",
            "Batch 5290/7032, Loss: 0.9137\n",
            "Batch 5300/7032, Loss: 1.0326\n",
            "Batch 5310/7032, Loss: 0.9577\n",
            "Batch 5320/7032, Loss: 0.9823\n",
            "Batch 5330/7032, Loss: 0.9603\n",
            "Batch 5340/7032, Loss: 0.9913\n",
            "Batch 5350/7032, Loss: 0.9370\n",
            "Batch 5360/7032, Loss: 1.0089\n",
            "Batch 5370/7032, Loss: 1.0626\n",
            "Batch 5380/7032, Loss: 0.9455\n",
            "Batch 5390/7032, Loss: 1.0094\n",
            "Batch 5400/7032, Loss: 0.9388\n",
            "Batch 5410/7032, Loss: 0.9924\n",
            "Batch 5420/7032, Loss: 0.9441\n",
            "Batch 5430/7032, Loss: 0.8758\n",
            "Batch 5440/7032, Loss: 0.8604\n",
            "Batch 5450/7032, Loss: 1.0498\n",
            "Batch 5460/7032, Loss: 1.0328\n",
            "Batch 5470/7032, Loss: 0.9475\n",
            "Batch 5480/7032, Loss: 1.0177\n",
            "Batch 5490/7032, Loss: 0.9062\n",
            "Batch 5500/7032, Loss: 0.9461\n",
            "Batch 5510/7032, Loss: 1.0141\n",
            "Batch 5520/7032, Loss: 1.0314\n",
            "Batch 5530/7032, Loss: 0.9584\n",
            "Batch 5540/7032, Loss: 1.0422\n",
            "Batch 5550/7032, Loss: 0.9402\n",
            "Batch 5560/7032, Loss: 0.9635\n",
            "Batch 5570/7032, Loss: 0.9227\n",
            "Batch 5580/7032, Loss: 0.9965\n",
            "Batch 5590/7032, Loss: 1.0219\n",
            "Batch 5600/7032, Loss: 0.9339\n",
            "Batch 5610/7032, Loss: 0.9243\n",
            "Batch 5620/7032, Loss: 1.1128\n",
            "Batch 5630/7032, Loss: 0.9220\n",
            "Batch 5640/7032, Loss: 0.9535\n",
            "Batch 5650/7032, Loss: 0.8681\n",
            "Batch 5660/7032, Loss: 1.0126\n",
            "Batch 5670/7032, Loss: 1.0321\n",
            "Batch 5680/7032, Loss: 0.9284\n",
            "Batch 5690/7032, Loss: 0.9325\n",
            "Batch 5700/7032, Loss: 1.0321\n",
            "Batch 5710/7032, Loss: 1.0038\n",
            "Batch 5720/7032, Loss: 0.8835\n",
            "Batch 5730/7032, Loss: 1.0159\n",
            "Batch 5740/7032, Loss: 0.9971\n",
            "Batch 5750/7032, Loss: 0.9132\n",
            "Batch 5760/7032, Loss: 0.9407\n",
            "Batch 5770/7032, Loss: 0.9255\n",
            "Batch 5780/7032, Loss: 0.9760\n",
            "Batch 5790/7032, Loss: 1.0592\n",
            "Batch 5800/7032, Loss: 1.0656\n",
            "Batch 5810/7032, Loss: 0.9541\n",
            "Batch 5820/7032, Loss: 1.0082\n",
            "Batch 5830/7032, Loss: 1.0248\n",
            "Batch 5840/7032, Loss: 1.0161\n",
            "Batch 5850/7032, Loss: 0.8967\n",
            "Batch 5860/7032, Loss: 0.8936\n",
            "Batch 5870/7032, Loss: 0.9546\n",
            "Batch 5880/7032, Loss: 0.8965\n",
            "Batch 5890/7032, Loss: 0.9969\n",
            "Batch 5900/7032, Loss: 1.0862\n",
            "Batch 5910/7032, Loss: 1.0677\n",
            "Batch 5920/7032, Loss: 1.0201\n",
            "Batch 5930/7032, Loss: 0.8606\n",
            "Batch 5940/7032, Loss: 1.0284\n",
            "Batch 5950/7032, Loss: 1.0814\n",
            "Batch 5960/7032, Loss: 0.9578\n",
            "Batch 5970/7032, Loss: 0.9439\n",
            "Batch 5980/7032, Loss: 1.0022\n",
            "Batch 5990/7032, Loss: 0.8775\n",
            "Batch 6000/7032, Loss: 0.8771\n",
            "Batch 6010/7032, Loss: 0.9436\n",
            "Batch 6020/7032, Loss: 0.9807\n",
            "Batch 6030/7032, Loss: 0.8909\n",
            "Batch 6040/7032, Loss: 1.0133\n",
            "Batch 6050/7032, Loss: 1.0441\n",
            "Batch 6060/7032, Loss: 0.9422\n",
            "Batch 6070/7032, Loss: 1.0127\n",
            "Batch 6080/7032, Loss: 1.0074\n",
            "Batch 6090/7032, Loss: 0.9532\n",
            "Batch 6100/7032, Loss: 0.9883\n",
            "Batch 6110/7032, Loss: 1.0650\n",
            "Batch 6120/7032, Loss: 0.9402\n",
            "Batch 6130/7032, Loss: 0.9892\n",
            "Batch 6140/7032, Loss: 0.9783\n",
            "Batch 6150/7032, Loss: 0.8646\n",
            "Batch 6160/7032, Loss: 1.0542\n",
            "Batch 6170/7032, Loss: 1.0612\n",
            "Batch 6180/7032, Loss: 0.9259\n",
            "Batch 6190/7032, Loss: 0.9751\n",
            "Batch 6200/7032, Loss: 0.9382\n",
            "Batch 6210/7032, Loss: 0.9235\n",
            "Batch 6220/7032, Loss: 0.9019\n",
            "Batch 6230/7032, Loss: 0.9763\n",
            "Batch 6240/7032, Loss: 0.9734\n",
            "Batch 6250/7032, Loss: 0.8616\n",
            "Batch 6260/7032, Loss: 1.0828\n",
            "Batch 6270/7032, Loss: 1.0007\n",
            "Batch 6280/7032, Loss: 1.0697\n",
            "Batch 6290/7032, Loss: 0.8186\n",
            "Batch 6300/7032, Loss: 1.0610\n",
            "Batch 6310/7032, Loss: 0.9420\n",
            "Batch 6320/7032, Loss: 1.0201\n",
            "Batch 6330/7032, Loss: 0.9580\n",
            "Batch 6340/7032, Loss: 1.0650\n",
            "Batch 6350/7032, Loss: 0.8402\n",
            "Batch 6360/7032, Loss: 0.9769\n",
            "Batch 6370/7032, Loss: 0.9762\n",
            "Batch 6380/7032, Loss: 0.9097\n",
            "Batch 6390/7032, Loss: 1.0645\n",
            "Batch 6400/7032, Loss: 1.0070\n",
            "Batch 6410/7032, Loss: 1.0255\n",
            "Batch 6420/7032, Loss: 0.9096\n",
            "Batch 6430/7032, Loss: 1.0130\n",
            "Batch 6440/7032, Loss: 0.9652\n",
            "Batch 6450/7032, Loss: 0.9587\n",
            "Batch 6460/7032, Loss: 1.0433\n",
            "Batch 6470/7032, Loss: 0.9877\n",
            "Batch 6480/7032, Loss: 0.8840\n",
            "Batch 6490/7032, Loss: 0.9131\n",
            "Batch 6500/7032, Loss: 1.0410\n",
            "Batch 6510/7032, Loss: 0.9218\n",
            "Batch 6520/7032, Loss: 0.8571\n",
            "Batch 6530/7032, Loss: 1.0096\n",
            "Batch 6540/7032, Loss: 0.9544\n",
            "Batch 6550/7032, Loss: 0.8648\n",
            "Batch 6560/7032, Loss: 0.9788\n",
            "Batch 6570/7032, Loss: 0.9048\n",
            "Batch 6580/7032, Loss: 1.0134\n",
            "Batch 6590/7032, Loss: 1.0269\n",
            "Batch 6600/7032, Loss: 0.9921\n",
            "Batch 6610/7032, Loss: 1.0051\n",
            "Batch 6620/7032, Loss: 1.0439\n",
            "Batch 6630/7032, Loss: 0.9450\n",
            "Batch 6640/7032, Loss: 1.1621\n",
            "Batch 6650/7032, Loss: 0.8750\n",
            "Batch 6660/7032, Loss: 0.9968\n",
            "Batch 6670/7032, Loss: 0.8793\n",
            "Batch 6680/7032, Loss: 1.0184\n",
            "Batch 6690/7032, Loss: 1.0102\n",
            "Batch 6700/7032, Loss: 0.9876\n",
            "Batch 6710/7032, Loss: 1.0517\n",
            "Batch 6720/7032, Loss: 0.9210\n",
            "Batch 6730/7032, Loss: 1.0439\n",
            "Batch 6740/7032, Loss: 0.9027\n",
            "Batch 6750/7032, Loss: 1.0690\n",
            "Batch 6760/7032, Loss: 1.0121\n",
            "Batch 6770/7032, Loss: 1.0069\n",
            "Batch 6780/7032, Loss: 1.0002\n",
            "Batch 6790/7032, Loss: 0.9027\n",
            "Batch 6800/7032, Loss: 0.9685\n",
            "Batch 6810/7032, Loss: 0.9940\n",
            "Batch 6820/7032, Loss: 1.0681\n",
            "Batch 6830/7032, Loss: 1.0008\n",
            "Batch 6840/7032, Loss: 0.8671\n",
            "Batch 6850/7032, Loss: 1.0316\n",
            "Batch 6860/7032, Loss: 0.9181\n",
            "Batch 6870/7032, Loss: 0.9889\n",
            "Batch 6880/7032, Loss: 0.8597\n",
            "Batch 6890/7032, Loss: 1.0233\n",
            "Batch 6900/7032, Loss: 0.9910\n",
            "Batch 6910/7032, Loss: 1.0297\n",
            "Batch 6920/7032, Loss: 0.9677\n",
            "Batch 6930/7032, Loss: 1.0249\n",
            "Batch 6940/7032, Loss: 0.9531\n",
            "Batch 6950/7032, Loss: 1.0253\n",
            "Batch 6960/7032, Loss: 0.9980\n",
            "Batch 6970/7032, Loss: 1.0443\n",
            "Batch 6980/7032, Loss: 0.9627\n",
            "Batch 6990/7032, Loss: 0.9299\n",
            "Batch 7000/7032, Loss: 0.8541\n",
            "Batch 7010/7032, Loss: 0.8755\n",
            "Batch 7020/7032, Loss: 0.9036\n",
            "Batch 7030/7032, Loss: 1.0325\n",
            "Batch 7032/7032, Loss: 1.1445\n",
            "Epoch 8/10\n",
            "Batch 10/7032, Loss: 0.8008\n",
            "Batch 20/7032, Loss: 0.8900\n",
            "Batch 30/7032, Loss: 0.9456\n",
            "Batch 40/7032, Loss: 0.9925\n",
            "Batch 50/7032, Loss: 0.8755\n",
            "Batch 60/7032, Loss: 1.0337\n",
            "Batch 70/7032, Loss: 0.9040\n",
            "Batch 80/7032, Loss: 0.8794\n",
            "Batch 90/7032, Loss: 0.9275\n",
            "Batch 100/7032, Loss: 1.0711\n",
            "Batch 110/7032, Loss: 0.9529\n",
            "Batch 120/7032, Loss: 0.9808\n",
            "Batch 130/7032, Loss: 1.1836\n",
            "Batch 140/7032, Loss: 0.9465\n",
            "Batch 150/7032, Loss: 1.0464\n",
            "Batch 160/7032, Loss: 1.0587\n",
            "Batch 170/7032, Loss: 1.0311\n",
            "Batch 180/7032, Loss: 0.9847\n",
            "Batch 190/7032, Loss: 0.9243\n",
            "Batch 200/7032, Loss: 1.0536\n",
            "Batch 210/7032, Loss: 0.9565\n",
            "Batch 220/7032, Loss: 0.8839\n",
            "Batch 230/7032, Loss: 0.9407\n",
            "Batch 240/7032, Loss: 0.8997\n",
            "Batch 250/7032, Loss: 0.8798\n",
            "Batch 260/7032, Loss: 0.9530\n",
            "Batch 270/7032, Loss: 0.8959\n",
            "Batch 280/7032, Loss: 0.9684\n",
            "Batch 290/7032, Loss: 0.8399\n",
            "Batch 300/7032, Loss: 0.8396\n",
            "Batch 310/7032, Loss: 1.0283\n",
            "Batch 320/7032, Loss: 0.8271\n",
            "Batch 330/7032, Loss: 0.9208\n",
            "Batch 340/7032, Loss: 0.9616\n",
            "Batch 350/7032, Loss: 1.0385\n",
            "Batch 360/7032, Loss: 0.8916\n",
            "Batch 370/7032, Loss: 0.9054\n",
            "Batch 380/7032, Loss: 0.9707\n",
            "Batch 390/7032, Loss: 0.9788\n",
            "Batch 400/7032, Loss: 1.0254\n",
            "Batch 410/7032, Loss: 1.0391\n",
            "Batch 420/7032, Loss: 0.9408\n",
            "Batch 430/7032, Loss: 0.9252\n",
            "Batch 440/7032, Loss: 0.8462\n",
            "Batch 450/7032, Loss: 0.8952\n",
            "Batch 460/7032, Loss: 1.0054\n",
            "Batch 470/7032, Loss: 0.8918\n",
            "Batch 480/7032, Loss: 0.9616\n",
            "Batch 490/7032, Loss: 1.0449\n",
            "Batch 500/7032, Loss: 0.9846\n",
            "Batch 510/7032, Loss: 0.9596\n",
            "Batch 520/7032, Loss: 0.8876\n",
            "Batch 530/7032, Loss: 0.9294\n",
            "Batch 540/7032, Loss: 0.9636\n",
            "Batch 550/7032, Loss: 1.0870\n",
            "Batch 560/7032, Loss: 0.9961\n",
            "Batch 570/7032, Loss: 0.9437\n",
            "Batch 580/7032, Loss: 0.9272\n",
            "Batch 590/7032, Loss: 1.0090\n",
            "Batch 600/7032, Loss: 1.0240\n",
            "Batch 610/7032, Loss: 1.0613\n",
            "Batch 620/7032, Loss: 0.9558\n",
            "Batch 630/7032, Loss: 0.9446\n",
            "Batch 640/7032, Loss: 0.8749\n",
            "Batch 650/7032, Loss: 1.0844\n",
            "Batch 660/7032, Loss: 1.0350\n",
            "Batch 670/7032, Loss: 0.9332\n",
            "Batch 680/7032, Loss: 1.0048\n",
            "Batch 690/7032, Loss: 0.9297\n",
            "Batch 700/7032, Loss: 0.8050\n",
            "Batch 710/7032, Loss: 0.9204\n",
            "Batch 720/7032, Loss: 0.9090\n",
            "Batch 730/7032, Loss: 0.9022\n",
            "Batch 740/7032, Loss: 1.0265\n",
            "Batch 750/7032, Loss: 0.9128\n",
            "Batch 760/7032, Loss: 0.9723\n",
            "Batch 770/7032, Loss: 0.9966\n",
            "Batch 780/7032, Loss: 1.0420\n",
            "Batch 790/7032, Loss: 0.9989\n",
            "Batch 800/7032, Loss: 0.9547\n",
            "Batch 810/7032, Loss: 0.8916\n",
            "Batch 820/7032, Loss: 0.9192\n",
            "Batch 830/7032, Loss: 0.9756\n",
            "Batch 840/7032, Loss: 1.0354\n",
            "Batch 850/7032, Loss: 0.9039\n",
            "Batch 860/7032, Loss: 0.7940\n",
            "Batch 870/7032, Loss: 1.0906\n",
            "Batch 880/7032, Loss: 1.0585\n",
            "Batch 890/7032, Loss: 1.0139\n",
            "Batch 900/7032, Loss: 1.0070\n",
            "Batch 910/7032, Loss: 0.9178\n",
            "Batch 920/7032, Loss: 0.8950\n",
            "Batch 930/7032, Loss: 0.9488\n",
            "Batch 940/7032, Loss: 0.8693\n",
            "Batch 950/7032, Loss: 0.9086\n",
            "Batch 960/7032, Loss: 0.8579\n",
            "Batch 970/7032, Loss: 0.8972\n",
            "Batch 980/7032, Loss: 1.0447\n",
            "Batch 990/7032, Loss: 0.9843\n",
            "Batch 1000/7032, Loss: 0.9567\n",
            "Batch 1010/7032, Loss: 0.9315\n",
            "Batch 1020/7032, Loss: 0.9116\n",
            "Batch 1030/7032, Loss: 0.9824\n",
            "Batch 1040/7032, Loss: 0.9733\n",
            "Batch 1050/7032, Loss: 1.0253\n",
            "Batch 1060/7032, Loss: 0.8347\n",
            "Batch 1070/7032, Loss: 0.9402\n",
            "Batch 1080/7032, Loss: 0.9611\n",
            "Batch 1090/7032, Loss: 0.9874\n",
            "Batch 1100/7032, Loss: 0.9790\n",
            "Batch 1110/7032, Loss: 0.9998\n",
            "Batch 1120/7032, Loss: 0.9089\n",
            "Batch 1130/7032, Loss: 0.8688\n",
            "Batch 1140/7032, Loss: 0.9873\n",
            "Batch 1150/7032, Loss: 0.9585\n",
            "Batch 1160/7032, Loss: 1.0263\n",
            "Batch 1170/7032, Loss: 1.0120\n",
            "Batch 1180/7032, Loss: 1.0015\n",
            "Batch 1190/7032, Loss: 0.8328\n",
            "Batch 1200/7032, Loss: 0.9437\n",
            "Batch 1210/7032, Loss: 0.8577\n",
            "Batch 1220/7032, Loss: 1.0238\n",
            "Batch 1230/7032, Loss: 1.0232\n",
            "Batch 1240/7032, Loss: 0.9516\n",
            "Batch 1250/7032, Loss: 0.9952\n",
            "Batch 1260/7032, Loss: 0.9053\n",
            "Batch 1270/7032, Loss: 0.9645\n",
            "Batch 1280/7032, Loss: 0.9384\n",
            "Batch 1290/7032, Loss: 0.9682\n",
            "Batch 1300/7032, Loss: 0.8729\n",
            "Batch 1310/7032, Loss: 1.0616\n",
            "Batch 1320/7032, Loss: 0.9344\n",
            "Batch 1330/7032, Loss: 0.8077\n",
            "Batch 1340/7032, Loss: 0.9952\n",
            "Batch 1350/7032, Loss: 0.9931\n",
            "Batch 1360/7032, Loss: 0.7985\n",
            "Batch 1370/7032, Loss: 0.8853\n",
            "Batch 1380/7032, Loss: 1.0254\n",
            "Batch 1390/7032, Loss: 1.1280\n",
            "Batch 1400/7032, Loss: 0.8751\n",
            "Batch 1410/7032, Loss: 0.9902\n",
            "Batch 1420/7032, Loss: 1.0562\n",
            "Batch 1430/7032, Loss: 0.9309\n",
            "Batch 1440/7032, Loss: 1.0014\n",
            "Batch 1450/7032, Loss: 1.0643\n",
            "Batch 1460/7032, Loss: 0.8936\n",
            "Batch 1470/7032, Loss: 0.8342\n",
            "Batch 1480/7032, Loss: 0.9953\n",
            "Batch 1490/7032, Loss: 0.8710\n",
            "Batch 1500/7032, Loss: 0.9254\n",
            "Batch 1510/7032, Loss: 0.9120\n",
            "Batch 1520/7032, Loss: 0.9704\n",
            "Batch 1530/7032, Loss: 0.9304\n",
            "Batch 1540/7032, Loss: 0.8830\n",
            "Batch 1550/7032, Loss: 1.0366\n",
            "Batch 1560/7032, Loss: 1.0339\n",
            "Batch 1570/7032, Loss: 0.9860\n",
            "Batch 1580/7032, Loss: 0.9865\n",
            "Batch 1590/7032, Loss: 1.1184\n",
            "Batch 1600/7032, Loss: 0.9080\n",
            "Batch 1610/7032, Loss: 0.9904\n",
            "Batch 1620/7032, Loss: 0.8842\n",
            "Batch 1630/7032, Loss: 0.9978\n",
            "Batch 1640/7032, Loss: 0.8923\n",
            "Batch 1650/7032, Loss: 0.9261\n",
            "Batch 1660/7032, Loss: 1.0778\n",
            "Batch 1670/7032, Loss: 0.8635\n",
            "Batch 1680/7032, Loss: 1.0741\n",
            "Batch 1690/7032, Loss: 0.8957\n",
            "Batch 1700/7032, Loss: 0.9574\n",
            "Batch 1710/7032, Loss: 0.8574\n",
            "Batch 1720/7032, Loss: 0.7790\n",
            "Batch 1730/7032, Loss: 1.0021\n",
            "Batch 1740/7032, Loss: 0.9125\n",
            "Batch 1750/7032, Loss: 0.9778\n",
            "Batch 1760/7032, Loss: 0.9107\n",
            "Batch 1770/7032, Loss: 1.0451\n",
            "Batch 1780/7032, Loss: 0.9333\n",
            "Batch 1790/7032, Loss: 0.9284\n",
            "Batch 1800/7032, Loss: 1.0799\n",
            "Batch 1810/7032, Loss: 1.1044\n",
            "Batch 1820/7032, Loss: 0.9610\n",
            "Batch 1830/7032, Loss: 0.9891\n",
            "Batch 1840/7032, Loss: 0.8695\n",
            "Batch 1850/7032, Loss: 0.9820\n",
            "Batch 1860/7032, Loss: 1.0104\n",
            "Batch 1870/7032, Loss: 0.9735\n",
            "Batch 1880/7032, Loss: 0.8269\n",
            "Batch 1890/7032, Loss: 0.8946\n",
            "Batch 1900/7032, Loss: 1.0014\n",
            "Batch 1910/7032, Loss: 1.0240\n",
            "Batch 1920/7032, Loss: 1.0002\n",
            "Batch 1930/7032, Loss: 1.0115\n",
            "Batch 1940/7032, Loss: 1.0700\n",
            "Batch 1950/7032, Loss: 1.0522\n",
            "Batch 1960/7032, Loss: 1.0473\n",
            "Batch 1970/7032, Loss: 0.8990\n",
            "Batch 1980/7032, Loss: 0.9713\n",
            "Batch 1990/7032, Loss: 0.8695\n",
            "Batch 2000/7032, Loss: 0.9398\n",
            "Batch 2010/7032, Loss: 1.1079\n",
            "Batch 2020/7032, Loss: 0.9457\n",
            "Batch 2030/7032, Loss: 0.9815\n",
            "Batch 2040/7032, Loss: 1.1089\n",
            "Batch 2050/7032, Loss: 0.8927\n",
            "Batch 2060/7032, Loss: 0.9467\n",
            "Batch 2070/7032, Loss: 1.0488\n",
            "Batch 2080/7032, Loss: 0.9996\n",
            "Batch 2090/7032, Loss: 0.9707\n",
            "Batch 2100/7032, Loss: 0.9750\n",
            "Batch 2110/7032, Loss: 0.9586\n",
            "Batch 2120/7032, Loss: 0.8822\n",
            "Batch 2130/7032, Loss: 0.9109\n",
            "Batch 2140/7032, Loss: 0.9179\n",
            "Batch 2150/7032, Loss: 0.9499\n",
            "Batch 2160/7032, Loss: 0.9104\n",
            "Batch 2170/7032, Loss: 0.9749\n",
            "Batch 2180/7032, Loss: 1.0160\n",
            "Batch 2190/7032, Loss: 1.0218\n",
            "Batch 2200/7032, Loss: 0.9064\n",
            "Batch 2210/7032, Loss: 0.9867\n",
            "Batch 2220/7032, Loss: 1.0166\n",
            "Batch 2230/7032, Loss: 0.9703\n",
            "Batch 2240/7032, Loss: 0.8877\n",
            "Batch 2250/7032, Loss: 1.0274\n",
            "Batch 2260/7032, Loss: 0.9512\n",
            "Batch 2270/7032, Loss: 0.9416\n",
            "Batch 2280/7032, Loss: 0.9867\n",
            "Batch 2290/7032, Loss: 0.9964\n",
            "Batch 2300/7032, Loss: 1.0133\n",
            "Batch 2310/7032, Loss: 0.9635\n",
            "Batch 2320/7032, Loss: 1.0970\n",
            "Batch 2330/7032, Loss: 0.9760\n",
            "Batch 2340/7032, Loss: 0.9405\n",
            "Batch 2350/7032, Loss: 0.9353\n",
            "Batch 2360/7032, Loss: 0.9895\n",
            "Batch 2370/7032, Loss: 0.9943\n",
            "Batch 2380/7032, Loss: 0.8111\n",
            "Batch 2390/7032, Loss: 0.9630\n",
            "Batch 2400/7032, Loss: 0.8382\n",
            "Batch 2410/7032, Loss: 0.9779\n",
            "Batch 2420/7032, Loss: 0.9545\n",
            "Batch 2430/7032, Loss: 0.8496\n",
            "Batch 2440/7032, Loss: 1.0046\n",
            "Batch 2450/7032, Loss: 1.1105\n",
            "Batch 2460/7032, Loss: 0.8243\n",
            "Batch 2470/7032, Loss: 0.9361\n",
            "Batch 2480/7032, Loss: 0.9940\n",
            "Batch 2490/7032, Loss: 0.9227\n",
            "Batch 2500/7032, Loss: 0.8725\n",
            "Batch 2510/7032, Loss: 0.9813\n",
            "Batch 2520/7032, Loss: 0.9829\n",
            "Batch 2530/7032, Loss: 1.0257\n",
            "Batch 2540/7032, Loss: 0.9318\n",
            "Batch 2550/7032, Loss: 1.0975\n",
            "Batch 2560/7032, Loss: 0.9655\n",
            "Batch 2570/7032, Loss: 0.9705\n",
            "Batch 2580/7032, Loss: 0.9492\n",
            "Batch 2590/7032, Loss: 0.9478\n",
            "Batch 2600/7032, Loss: 0.9711\n",
            "Batch 2610/7032, Loss: 0.8663\n",
            "Batch 2620/7032, Loss: 1.0577\n",
            "Batch 2630/7032, Loss: 1.0518\n",
            "Batch 2640/7032, Loss: 0.9466\n",
            "Batch 2650/7032, Loss: 0.9643\n",
            "Batch 2660/7032, Loss: 0.8813\n",
            "Batch 2670/7032, Loss: 1.0038\n",
            "Batch 2680/7032, Loss: 0.9363\n",
            "Batch 2690/7032, Loss: 1.0598\n",
            "Batch 2700/7032, Loss: 0.9873\n",
            "Batch 2710/7032, Loss: 0.9892\n",
            "Batch 2720/7032, Loss: 1.0230\n",
            "Batch 2730/7032, Loss: 0.9472\n",
            "Batch 2740/7032, Loss: 0.9572\n",
            "Batch 2750/7032, Loss: 1.0245\n",
            "Batch 2760/7032, Loss: 0.9795\n",
            "Batch 2770/7032, Loss: 1.0269\n",
            "Batch 2780/7032, Loss: 0.9716\n",
            "Batch 2790/7032, Loss: 0.9710\n",
            "Batch 2800/7032, Loss: 1.0412\n",
            "Batch 2810/7032, Loss: 0.8688\n",
            "Batch 2820/7032, Loss: 1.0925\n",
            "Batch 2830/7032, Loss: 0.9287\n",
            "Batch 2840/7032, Loss: 0.9816\n",
            "Batch 2850/7032, Loss: 0.8989\n",
            "Batch 2860/7032, Loss: 0.9547\n",
            "Batch 2870/7032, Loss: 0.9399\n",
            "Batch 2880/7032, Loss: 0.9822\n",
            "Batch 2890/7032, Loss: 0.9668\n",
            "Batch 2900/7032, Loss: 0.9628\n",
            "Batch 2910/7032, Loss: 0.9733\n",
            "Batch 2920/7032, Loss: 0.8506\n",
            "Batch 2930/7032, Loss: 0.9938\n",
            "Batch 2940/7032, Loss: 0.9646\n",
            "Batch 2950/7032, Loss: 0.8517\n",
            "Batch 2960/7032, Loss: 0.9648\n",
            "Batch 2970/7032, Loss: 0.9402\n",
            "Batch 2980/7032, Loss: 0.9623\n",
            "Batch 2990/7032, Loss: 0.8827\n",
            "Batch 3000/7032, Loss: 0.9301\n",
            "Batch 3010/7032, Loss: 0.9619\n",
            "Batch 3020/7032, Loss: 0.9030\n",
            "Batch 3030/7032, Loss: 1.0282\n",
            "Batch 3040/7032, Loss: 0.9761\n",
            "Batch 3050/7032, Loss: 0.9808\n",
            "Batch 3060/7032, Loss: 0.9433\n",
            "Batch 3070/7032, Loss: 0.9310\n",
            "Batch 3080/7032, Loss: 1.1158\n",
            "Batch 3090/7032, Loss: 0.8875\n",
            "Batch 3100/7032, Loss: 0.8613\n",
            "Batch 3110/7032, Loss: 1.0078\n",
            "Batch 3120/7032, Loss: 0.8975\n",
            "Batch 3130/7032, Loss: 0.9287\n",
            "Batch 3140/7032, Loss: 0.9721\n",
            "Batch 3150/7032, Loss: 0.9611\n",
            "Batch 3160/7032, Loss: 0.9293\n",
            "Batch 3170/7032, Loss: 1.0150\n",
            "Batch 3180/7032, Loss: 1.0314\n",
            "Batch 3190/7032, Loss: 0.8980\n",
            "Batch 3200/7032, Loss: 1.0369\n",
            "Batch 3210/7032, Loss: 0.9511\n",
            "Batch 3220/7032, Loss: 0.8572\n",
            "Batch 3230/7032, Loss: 0.9034\n",
            "Batch 3240/7032, Loss: 0.9890\n",
            "Batch 3250/7032, Loss: 1.0339\n",
            "Batch 3260/7032, Loss: 1.0074\n",
            "Batch 3270/7032, Loss: 0.9792\n",
            "Batch 3280/7032, Loss: 0.9712\n",
            "Batch 3290/7032, Loss: 1.0441\n",
            "Batch 3300/7032, Loss: 0.9675\n",
            "Batch 3310/7032, Loss: 1.0156\n",
            "Batch 3320/7032, Loss: 1.0282\n",
            "Batch 3330/7032, Loss: 0.8914\n",
            "Batch 3340/7032, Loss: 0.8653\n",
            "Batch 3350/7032, Loss: 0.8991\n",
            "Batch 3360/7032, Loss: 1.0211\n",
            "Batch 3370/7032, Loss: 0.9711\n",
            "Batch 3380/7032, Loss: 1.0159\n",
            "Batch 3390/7032, Loss: 0.8927\n",
            "Batch 3400/7032, Loss: 0.9423\n",
            "Batch 3410/7032, Loss: 0.8017\n",
            "Batch 3420/7032, Loss: 1.0095\n",
            "Batch 3430/7032, Loss: 0.9617\n",
            "Batch 3440/7032, Loss: 0.9473\n",
            "Batch 3450/7032, Loss: 0.9438\n",
            "Batch 3460/7032, Loss: 0.9919\n",
            "Batch 3470/7032, Loss: 0.9970\n",
            "Batch 3480/7032, Loss: 1.0032\n",
            "Batch 3490/7032, Loss: 0.9339\n",
            "Batch 3500/7032, Loss: 0.8831\n",
            "Batch 3510/7032, Loss: 0.9741\n",
            "Batch 3520/7032, Loss: 0.9144\n",
            "Batch 3530/7032, Loss: 0.8750\n",
            "Batch 3540/7032, Loss: 1.0105\n",
            "Batch 3550/7032, Loss: 0.8960\n",
            "Batch 3560/7032, Loss: 0.9938\n",
            "Batch 3570/7032, Loss: 0.9129\n",
            "Batch 3580/7032, Loss: 1.0544\n",
            "Batch 3590/7032, Loss: 0.9053\n",
            "Batch 3600/7032, Loss: 0.7911\n",
            "Batch 3610/7032, Loss: 0.8999\n",
            "Batch 3620/7032, Loss: 0.8040\n",
            "Batch 3630/7032, Loss: 0.9316\n",
            "Batch 3640/7032, Loss: 0.9012\n",
            "Batch 3650/7032, Loss: 0.9528\n",
            "Batch 3660/7032, Loss: 1.0300\n",
            "Batch 3670/7032, Loss: 1.0272\n",
            "Batch 3680/7032, Loss: 0.9358\n",
            "Batch 3690/7032, Loss: 0.9805\n",
            "Batch 3700/7032, Loss: 0.8898\n",
            "Batch 3710/7032, Loss: 0.9458\n",
            "Batch 3720/7032, Loss: 1.0021\n",
            "Batch 3730/7032, Loss: 0.9250\n",
            "Batch 3740/7032, Loss: 1.1411\n",
            "Batch 3750/7032, Loss: 0.9098\n",
            "Batch 3760/7032, Loss: 1.0604\n",
            "Batch 3770/7032, Loss: 0.9160\n",
            "Batch 3780/7032, Loss: 1.0028\n",
            "Batch 3790/7032, Loss: 0.8860\n",
            "Batch 3800/7032, Loss: 0.9985\n",
            "Batch 3810/7032, Loss: 0.9094\n",
            "Batch 3820/7032, Loss: 0.9339\n",
            "Batch 3830/7032, Loss: 0.9351\n",
            "Batch 3840/7032, Loss: 0.9177\n",
            "Batch 3850/7032, Loss: 1.0050\n",
            "Batch 3860/7032, Loss: 0.8171\n",
            "Batch 3870/7032, Loss: 0.9059\n",
            "Batch 3880/7032, Loss: 0.8259\n",
            "Batch 3890/7032, Loss: 0.9117\n",
            "Batch 3900/7032, Loss: 0.8328\n",
            "Batch 3910/7032, Loss: 0.9178\n",
            "Batch 3920/7032, Loss: 0.9935\n",
            "Batch 3930/7032, Loss: 0.9469\n",
            "Batch 3940/7032, Loss: 0.8783\n",
            "Batch 3950/7032, Loss: 0.9902\n",
            "Batch 3960/7032, Loss: 0.9946\n",
            "Batch 3970/7032, Loss: 1.0094\n",
            "Batch 3980/7032, Loss: 0.8193\n",
            "Batch 3990/7032, Loss: 0.8367\n",
            "Batch 4000/7032, Loss: 0.8947\n",
            "Batch 4010/7032, Loss: 0.9344\n",
            "Batch 4020/7032, Loss: 0.8796\n",
            "Batch 4030/7032, Loss: 1.0018\n",
            "Batch 4040/7032, Loss: 0.8822\n",
            "Batch 4050/7032, Loss: 0.9597\n",
            "Batch 4060/7032, Loss: 0.9234\n",
            "Batch 4070/7032, Loss: 0.8623\n",
            "Batch 4080/7032, Loss: 0.9430\n",
            "Batch 4090/7032, Loss: 1.0244\n",
            "Batch 4100/7032, Loss: 0.9628\n",
            "Batch 4110/7032, Loss: 1.0636\n",
            "Batch 4120/7032, Loss: 0.9709\n",
            "Batch 4130/7032, Loss: 0.9711\n",
            "Batch 4140/7032, Loss: 0.9786\n",
            "Batch 4150/7032, Loss: 1.0923\n",
            "Batch 4160/7032, Loss: 0.9058\n",
            "Batch 4170/7032, Loss: 0.9460\n",
            "Batch 4180/7032, Loss: 1.0125\n",
            "Batch 4190/7032, Loss: 0.9050\n",
            "Batch 4200/7032, Loss: 0.9014\n",
            "Batch 4210/7032, Loss: 1.0209\n",
            "Batch 4220/7032, Loss: 0.9497\n",
            "Batch 4230/7032, Loss: 0.8866\n",
            "Batch 4240/7032, Loss: 1.0418\n",
            "Batch 4250/7032, Loss: 0.8843\n",
            "Batch 4260/7032, Loss: 1.0112\n",
            "Batch 4270/7032, Loss: 0.8852\n",
            "Batch 4280/7032, Loss: 1.0042\n",
            "Batch 4290/7032, Loss: 0.9443\n",
            "Batch 4300/7032, Loss: 0.9605\n",
            "Batch 4310/7032, Loss: 1.0645\n",
            "Batch 4320/7032, Loss: 0.9821\n",
            "Batch 4330/7032, Loss: 1.0238\n",
            "Batch 4340/7032, Loss: 1.0756\n",
            "Batch 4350/7032, Loss: 0.8664\n",
            "Batch 4360/7032, Loss: 1.0015\n",
            "Batch 4370/7032, Loss: 1.1000\n",
            "Batch 4380/7032, Loss: 0.9863\n",
            "Batch 4390/7032, Loss: 0.8873\n",
            "Batch 4400/7032, Loss: 0.8640\n",
            "Batch 4410/7032, Loss: 0.8991\n",
            "Batch 4420/7032, Loss: 0.9974\n",
            "Batch 4430/7032, Loss: 0.8879\n",
            "Batch 4440/7032, Loss: 1.0053\n",
            "Batch 4450/7032, Loss: 0.9433\n",
            "Batch 4460/7032, Loss: 0.9190\n",
            "Batch 4470/7032, Loss: 0.8902\n",
            "Batch 4480/7032, Loss: 0.9345\n",
            "Batch 4490/7032, Loss: 0.9222\n",
            "Batch 4500/7032, Loss: 0.8811\n",
            "Batch 4510/7032, Loss: 0.9744\n",
            "Batch 4520/7032, Loss: 1.0269\n",
            "Batch 4530/7032, Loss: 0.9399\n",
            "Batch 4540/7032, Loss: 0.8628\n",
            "Batch 4550/7032, Loss: 1.0991\n",
            "Batch 4560/7032, Loss: 1.0086\n",
            "Batch 4570/7032, Loss: 0.9597\n",
            "Batch 4580/7032, Loss: 0.8636\n",
            "Batch 4590/7032, Loss: 0.8625\n",
            "Batch 4600/7032, Loss: 1.0624\n",
            "Batch 4610/7032, Loss: 1.0841\n",
            "Batch 4620/7032, Loss: 0.9731\n",
            "Batch 4630/7032, Loss: 1.0544\n",
            "Batch 4640/7032, Loss: 0.9189\n",
            "Batch 4650/7032, Loss: 0.8636\n",
            "Batch 4660/7032, Loss: 0.9169\n",
            "Batch 4670/7032, Loss: 1.0004\n",
            "Batch 4680/7032, Loss: 0.9705\n",
            "Batch 4690/7032, Loss: 1.0237\n",
            "Batch 4700/7032, Loss: 0.9659\n",
            "Batch 4710/7032, Loss: 0.9806\n",
            "Batch 4720/7032, Loss: 1.0502\n",
            "Batch 4730/7032, Loss: 0.9395\n",
            "Batch 4740/7032, Loss: 0.9565\n",
            "Batch 4750/7032, Loss: 0.8903\n",
            "Batch 4760/7032, Loss: 0.9022\n",
            "Batch 4770/7032, Loss: 1.0465\n",
            "Batch 4780/7032, Loss: 0.9542\n",
            "Batch 4790/7032, Loss: 0.9896\n",
            "Batch 4800/7032, Loss: 1.0006\n",
            "Batch 4810/7032, Loss: 0.9591\n",
            "Batch 4820/7032, Loss: 0.9627\n",
            "Batch 4830/7032, Loss: 0.9982\n",
            "Batch 4840/7032, Loss: 0.9256\n",
            "Batch 4850/7032, Loss: 1.0011\n",
            "Batch 4860/7032, Loss: 0.9961\n",
            "Batch 4870/7032, Loss: 0.8781\n",
            "Batch 4880/7032, Loss: 0.9274\n",
            "Batch 4890/7032, Loss: 0.8934\n",
            "Batch 4900/7032, Loss: 1.0729\n",
            "Batch 4910/7032, Loss: 0.9135\n",
            "Batch 4920/7032, Loss: 0.9505\n",
            "Batch 4930/7032, Loss: 0.9873\n",
            "Batch 4940/7032, Loss: 0.8948\n",
            "Batch 4950/7032, Loss: 0.9403\n",
            "Batch 4960/7032, Loss: 0.9135\n",
            "Batch 4970/7032, Loss: 0.8902\n",
            "Batch 4980/7032, Loss: 0.8643\n",
            "Batch 4990/7032, Loss: 0.9600\n",
            "Batch 5000/7032, Loss: 0.8762\n",
            "Batch 5010/7032, Loss: 0.9740\n",
            "Batch 5020/7032, Loss: 0.8707\n",
            "Batch 5030/7032, Loss: 1.0371\n",
            "Batch 5040/7032, Loss: 0.9664\n",
            "Batch 5050/7032, Loss: 0.8021\n",
            "Batch 5060/7032, Loss: 0.9592\n",
            "Batch 5070/7032, Loss: 1.0111\n",
            "Batch 5080/7032, Loss: 0.9018\n",
            "Batch 5090/7032, Loss: 0.9833\n",
            "Batch 5100/7032, Loss: 0.9341\n",
            "Batch 5110/7032, Loss: 0.8762\n",
            "Batch 5120/7032, Loss: 0.8550\n",
            "Batch 5130/7032, Loss: 1.0299\n",
            "Batch 5140/7032, Loss: 0.9431\n",
            "Batch 5150/7032, Loss: 0.9757\n",
            "Batch 5160/7032, Loss: 0.9297\n",
            "Batch 5170/7032, Loss: 0.9662\n",
            "Batch 5180/7032, Loss: 0.8763\n",
            "Batch 5190/7032, Loss: 0.9067\n",
            "Batch 5200/7032, Loss: 0.8936\n",
            "Batch 5210/7032, Loss: 0.9786\n",
            "Batch 5220/7032, Loss: 0.7971\n",
            "Batch 5230/7032, Loss: 0.8936\n",
            "Batch 5240/7032, Loss: 0.8991\n",
            "Batch 5250/7032, Loss: 0.9230\n",
            "Batch 5260/7032, Loss: 0.9291\n",
            "Batch 5270/7032, Loss: 0.9969\n",
            "Batch 5280/7032, Loss: 0.9546\n",
            "Batch 5290/7032, Loss: 0.9998\n",
            "Batch 5300/7032, Loss: 0.9455\n",
            "Batch 5310/7032, Loss: 1.0068\n",
            "Batch 5320/7032, Loss: 0.8540\n",
            "Batch 5330/7032, Loss: 0.9475\n",
            "Batch 5340/7032, Loss: 0.9897\n",
            "Batch 5350/7032, Loss: 0.9789\n",
            "Batch 5360/7032, Loss: 0.9664\n",
            "Batch 5370/7032, Loss: 1.0163\n",
            "Batch 5380/7032, Loss: 0.9695\n",
            "Batch 5390/7032, Loss: 1.0411\n",
            "Batch 5400/7032, Loss: 0.9826\n",
            "Batch 5410/7032, Loss: 0.9566\n",
            "Batch 5420/7032, Loss: 0.9902\n",
            "Batch 5430/7032, Loss: 1.0034\n",
            "Batch 5440/7032, Loss: 1.0566\n",
            "Batch 5450/7032, Loss: 0.9483\n",
            "Batch 5460/7032, Loss: 0.9941\n",
            "Batch 5470/7032, Loss: 0.9300\n",
            "Batch 5480/7032, Loss: 0.9426\n",
            "Batch 5490/7032, Loss: 1.0110\n",
            "Batch 5500/7032, Loss: 0.9138\n",
            "Batch 5510/7032, Loss: 0.9527\n",
            "Batch 5520/7032, Loss: 0.8924\n",
            "Batch 5530/7032, Loss: 0.9948\n",
            "Batch 5540/7032, Loss: 0.8209\n",
            "Batch 5550/7032, Loss: 1.0189\n",
            "Batch 5560/7032, Loss: 0.8903\n",
            "Batch 5570/7032, Loss: 0.9007\n",
            "Batch 5580/7032, Loss: 0.8154\n",
            "Batch 5590/7032, Loss: 0.9835\n",
            "Batch 5600/7032, Loss: 0.8701\n",
            "Batch 5610/7032, Loss: 0.8273\n",
            "Batch 5620/7032, Loss: 0.8786\n",
            "Batch 5630/7032, Loss: 0.9189\n",
            "Batch 5640/7032, Loss: 0.9389\n",
            "Batch 5650/7032, Loss: 0.9743\n",
            "Batch 5660/7032, Loss: 0.9257\n",
            "Batch 5670/7032, Loss: 0.9326\n",
            "Batch 5680/7032, Loss: 1.0035\n",
            "Batch 5690/7032, Loss: 0.9195\n",
            "Batch 5700/7032, Loss: 0.9235\n",
            "Batch 5710/7032, Loss: 0.9517\n",
            "Batch 5720/7032, Loss: 0.9009\n",
            "Batch 5730/7032, Loss: 0.8969\n",
            "Batch 5740/7032, Loss: 0.7849\n",
            "Batch 5750/7032, Loss: 0.8814\n",
            "Batch 5760/7032, Loss: 0.9854\n",
            "Batch 5770/7032, Loss: 0.9301\n",
            "Batch 5780/7032, Loss: 1.0163\n",
            "Batch 5790/7032, Loss: 0.9921\n",
            "Batch 5800/7032, Loss: 0.8473\n",
            "Batch 5810/7032, Loss: 1.0134\n",
            "Batch 5820/7032, Loss: 1.0183\n",
            "Batch 5830/7032, Loss: 0.8085\n",
            "Batch 5840/7032, Loss: 0.8603\n",
            "Batch 5850/7032, Loss: 0.9488\n",
            "Batch 5860/7032, Loss: 0.9776\n",
            "Batch 5870/7032, Loss: 0.9427\n",
            "Batch 5880/7032, Loss: 0.9244\n",
            "Batch 5890/7032, Loss: 0.9439\n",
            "Batch 5900/7032, Loss: 0.8460\n",
            "Batch 5910/7032, Loss: 0.9649\n",
            "Batch 5920/7032, Loss: 0.9231\n",
            "Batch 5930/7032, Loss: 0.9701\n",
            "Batch 5940/7032, Loss: 0.8941\n",
            "Batch 5950/7032, Loss: 0.9396\n",
            "Batch 5960/7032, Loss: 0.8981\n",
            "Batch 5970/7032, Loss: 0.9532\n",
            "Batch 5980/7032, Loss: 0.9249\n",
            "Batch 5990/7032, Loss: 0.9388\n",
            "Batch 6000/7032, Loss: 0.9440\n",
            "Batch 6010/7032, Loss: 0.9700\n",
            "Batch 6020/7032, Loss: 0.9869\n",
            "Batch 6030/7032, Loss: 0.9651\n",
            "Batch 6040/7032, Loss: 0.9584\n",
            "Batch 6050/7032, Loss: 0.9416\n",
            "Batch 6060/7032, Loss: 0.8827\n",
            "Batch 6070/7032, Loss: 0.9203\n",
            "Batch 6080/7032, Loss: 0.9790\n",
            "Batch 6090/7032, Loss: 0.8770\n",
            "Batch 6100/7032, Loss: 1.0612\n",
            "Batch 6110/7032, Loss: 0.9143\n",
            "Batch 6120/7032, Loss: 0.9161\n",
            "Batch 6130/7032, Loss: 0.9689\n",
            "Batch 6140/7032, Loss: 1.0639\n",
            "Batch 6150/7032, Loss: 0.7633\n",
            "Batch 6160/7032, Loss: 0.8834\n",
            "Batch 6170/7032, Loss: 1.0327\n",
            "Batch 6180/7032, Loss: 0.9496\n",
            "Batch 6190/7032, Loss: 0.9346\n",
            "Batch 6200/7032, Loss: 0.9160\n",
            "Batch 6210/7032, Loss: 0.8646\n",
            "Batch 6220/7032, Loss: 0.8424\n",
            "Batch 6230/7032, Loss: 0.9582\n",
            "Batch 6240/7032, Loss: 1.0193\n",
            "Batch 6250/7032, Loss: 1.0573\n",
            "Batch 6260/7032, Loss: 0.9552\n",
            "Batch 6270/7032, Loss: 0.9187\n",
            "Batch 6280/7032, Loss: 0.9560\n",
            "Batch 6290/7032, Loss: 0.9465\n",
            "Batch 6300/7032, Loss: 0.8954\n",
            "Batch 6310/7032, Loss: 0.9935\n",
            "Batch 6320/7032, Loss: 0.7450\n",
            "Batch 6330/7032, Loss: 1.1111\n",
            "Batch 6340/7032, Loss: 1.0513\n",
            "Batch 6350/7032, Loss: 0.9655\n",
            "Batch 6360/7032, Loss: 1.0399\n",
            "Batch 6370/7032, Loss: 0.9414\n",
            "Batch 6380/7032, Loss: 0.9136\n",
            "Batch 6390/7032, Loss: 0.8745\n",
            "Batch 6400/7032, Loss: 1.0679\n",
            "Batch 6410/7032, Loss: 1.0253\n",
            "Batch 6420/7032, Loss: 0.9755\n",
            "Batch 6430/7032, Loss: 0.9092\n",
            "Batch 6440/7032, Loss: 0.9677\n",
            "Batch 6450/7032, Loss: 0.9895\n",
            "Batch 6460/7032, Loss: 0.9208\n",
            "Batch 6470/7032, Loss: 1.0508\n",
            "Batch 6480/7032, Loss: 1.0036\n",
            "Batch 6490/7032, Loss: 0.9311\n",
            "Batch 6500/7032, Loss: 0.9228\n",
            "Batch 6510/7032, Loss: 0.9416\n",
            "Batch 6520/7032, Loss: 0.9544\n",
            "Batch 6530/7032, Loss: 0.8787\n",
            "Batch 6540/7032, Loss: 0.9299\n",
            "Batch 6550/7032, Loss: 0.9283\n",
            "Batch 6560/7032, Loss: 0.9278\n",
            "Batch 6570/7032, Loss: 0.9708\n",
            "Batch 6580/7032, Loss: 1.1113\n",
            "Batch 6590/7032, Loss: 0.8702\n",
            "Batch 6600/7032, Loss: 0.9672\n",
            "Batch 6610/7032, Loss: 1.0333\n",
            "Batch 6620/7032, Loss: 0.8671\n",
            "Batch 6630/7032, Loss: 0.8403\n",
            "Batch 6640/7032, Loss: 0.9486\n",
            "Batch 6650/7032, Loss: 0.8977\n",
            "Batch 6660/7032, Loss: 0.9546\n",
            "Batch 6670/7032, Loss: 0.9224\n",
            "Batch 6680/7032, Loss: 1.0137\n",
            "Batch 6690/7032, Loss: 0.8706\n",
            "Batch 6700/7032, Loss: 0.9316\n",
            "Batch 6710/7032, Loss: 0.8537\n",
            "Batch 6720/7032, Loss: 1.0176\n",
            "Batch 6730/7032, Loss: 1.0547\n",
            "Batch 6740/7032, Loss: 0.8375\n",
            "Batch 6750/7032, Loss: 0.8037\n",
            "Batch 6760/7032, Loss: 0.9781\n",
            "Batch 6770/7032, Loss: 0.9220\n",
            "Batch 6780/7032, Loss: 0.9578\n",
            "Batch 6790/7032, Loss: 1.0080\n",
            "Batch 6800/7032, Loss: 0.9586\n",
            "Batch 6810/7032, Loss: 0.9462\n",
            "Batch 6820/7032, Loss: 0.9521\n",
            "Batch 6830/7032, Loss: 0.9736\n",
            "Batch 6840/7032, Loss: 0.8811\n",
            "Batch 6850/7032, Loss: 0.9852\n",
            "Batch 6860/7032, Loss: 0.8619\n",
            "Batch 6870/7032, Loss: 0.9812\n",
            "Batch 6880/7032, Loss: 1.0613\n",
            "Batch 6890/7032, Loss: 0.8550\n",
            "Batch 6900/7032, Loss: 0.8244\n",
            "Batch 6910/7032, Loss: 1.0216\n",
            "Batch 6920/7032, Loss: 0.9817\n",
            "Batch 6930/7032, Loss: 0.7818\n",
            "Batch 6940/7032, Loss: 1.0367\n",
            "Batch 6950/7032, Loss: 0.8135\n",
            "Batch 6960/7032, Loss: 0.9651\n",
            "Batch 6970/7032, Loss: 0.9966\n",
            "Batch 6980/7032, Loss: 1.0018\n",
            "Batch 6990/7032, Loss: 0.8875\n",
            "Batch 7000/7032, Loss: 0.8036\n",
            "Batch 7010/7032, Loss: 0.9523\n",
            "Batch 7020/7032, Loss: 0.8457\n",
            "Batch 7030/7032, Loss: 1.0826\n",
            "Batch 7032/7032, Loss: 1.2594\n",
            "Epoch 9/10\n",
            "Batch 10/7032, Loss: 0.9268\n",
            "Batch 20/7032, Loss: 0.9905\n",
            "Batch 30/7032, Loss: 0.8963\n",
            "Batch 40/7032, Loss: 0.9143\n",
            "Batch 50/7032, Loss: 0.9918\n",
            "Batch 60/7032, Loss: 1.0865\n",
            "Batch 70/7032, Loss: 1.0121\n",
            "Batch 80/7032, Loss: 0.8963\n",
            "Batch 90/7032, Loss: 1.0506\n",
            "Batch 100/7032, Loss: 0.9779\n",
            "Batch 110/7032, Loss: 0.9445\n",
            "Batch 120/7032, Loss: 0.9036\n",
            "Batch 130/7032, Loss: 0.9909\n",
            "Batch 140/7032, Loss: 0.9612\n",
            "Batch 150/7032, Loss: 0.9935\n",
            "Batch 160/7032, Loss: 1.0346\n",
            "Batch 170/7032, Loss: 0.9124\n",
            "Batch 180/7032, Loss: 0.9068\n",
            "Batch 190/7032, Loss: 0.8777\n",
            "Batch 200/7032, Loss: 0.8653\n",
            "Batch 210/7032, Loss: 0.9417\n",
            "Batch 220/7032, Loss: 0.9713\n",
            "Batch 230/7032, Loss: 1.1148\n",
            "Batch 240/7032, Loss: 0.9718\n",
            "Batch 250/7032, Loss: 0.8419\n",
            "Batch 260/7032, Loss: 0.9849\n",
            "Batch 270/7032, Loss: 0.9736\n",
            "Batch 280/7032, Loss: 0.9519\n",
            "Batch 290/7032, Loss: 0.9021\n",
            "Batch 300/7032, Loss: 0.9307\n",
            "Batch 310/7032, Loss: 0.9169\n",
            "Batch 320/7032, Loss: 0.8405\n",
            "Batch 330/7032, Loss: 0.7976\n",
            "Batch 340/7032, Loss: 1.0780\n",
            "Batch 350/7032, Loss: 0.9436\n",
            "Batch 360/7032, Loss: 1.0926\n",
            "Batch 370/7032, Loss: 0.9158\n",
            "Batch 380/7032, Loss: 0.9013\n",
            "Batch 390/7032, Loss: 0.8850\n",
            "Batch 400/7032, Loss: 0.9531\n",
            "Batch 410/7032, Loss: 0.8280\n",
            "Batch 420/7032, Loss: 0.9019\n",
            "Batch 430/7032, Loss: 0.9369\n",
            "Batch 440/7032, Loss: 0.8875\n",
            "Batch 450/7032, Loss: 1.0565\n",
            "Batch 460/7032, Loss: 0.9094\n",
            "Batch 470/7032, Loss: 0.8879\n",
            "Batch 480/7032, Loss: 0.9714\n",
            "Batch 490/7032, Loss: 0.9700\n",
            "Batch 500/7032, Loss: 1.0323\n",
            "Batch 510/7032, Loss: 0.7920\n",
            "Batch 520/7032, Loss: 0.9568\n",
            "Batch 530/7032, Loss: 0.9630\n",
            "Batch 540/7032, Loss: 1.0064\n",
            "Batch 550/7032, Loss: 0.9427\n",
            "Batch 560/7032, Loss: 0.8494\n",
            "Batch 570/7032, Loss: 0.9592\n",
            "Batch 580/7032, Loss: 0.9021\n",
            "Batch 590/7032, Loss: 0.8448\n",
            "Batch 600/7032, Loss: 0.9313\n",
            "Batch 610/7032, Loss: 0.9991\n",
            "Batch 620/7032, Loss: 0.8474\n",
            "Batch 630/7032, Loss: 0.8683\n",
            "Batch 640/7032, Loss: 1.0091\n",
            "Batch 650/7032, Loss: 0.9219\n",
            "Batch 660/7032, Loss: 1.0415\n",
            "Batch 670/7032, Loss: 0.8700\n",
            "Batch 680/7032, Loss: 0.9491\n",
            "Batch 690/7032, Loss: 0.8937\n",
            "Batch 700/7032, Loss: 0.9839\n",
            "Batch 710/7032, Loss: 0.9638\n",
            "Batch 720/7032, Loss: 0.8273\n",
            "Batch 730/7032, Loss: 0.9880\n",
            "Batch 740/7032, Loss: 0.9325\n",
            "Batch 750/7032, Loss: 0.9114\n",
            "Batch 760/7032, Loss: 0.9963\n",
            "Batch 770/7032, Loss: 0.9178\n",
            "Batch 780/7032, Loss: 0.9255\n",
            "Batch 790/7032, Loss: 1.0270\n",
            "Batch 800/7032, Loss: 0.7716\n",
            "Batch 810/7032, Loss: 0.9184\n",
            "Batch 820/7032, Loss: 0.8649\n",
            "Batch 830/7032, Loss: 0.9294\n",
            "Batch 840/7032, Loss: 0.9670\n",
            "Batch 850/7032, Loss: 0.8504\n",
            "Batch 860/7032, Loss: 0.9280\n",
            "Batch 870/7032, Loss: 0.9590\n",
            "Batch 880/7032, Loss: 0.8723\n",
            "Batch 890/7032, Loss: 0.8276\n",
            "Batch 900/7032, Loss: 0.9093\n",
            "Batch 910/7032, Loss: 0.8380\n",
            "Batch 920/7032, Loss: 0.9275\n",
            "Batch 930/7032, Loss: 0.9472\n",
            "Batch 940/7032, Loss: 0.9161\n",
            "Batch 950/7032, Loss: 0.9509\n",
            "Batch 960/7032, Loss: 0.8724\n",
            "Batch 970/7032, Loss: 1.0064\n",
            "Batch 980/7032, Loss: 0.9343\n",
            "Batch 990/7032, Loss: 0.9191\n",
            "Batch 1000/7032, Loss: 0.9013\n",
            "Batch 1010/7032, Loss: 0.8223\n",
            "Batch 1020/7032, Loss: 0.9888\n",
            "Batch 1030/7032, Loss: 0.9775\n",
            "Batch 1040/7032, Loss: 0.9126\n",
            "Batch 1050/7032, Loss: 0.9511\n",
            "Batch 1060/7032, Loss: 0.9143\n",
            "Batch 1070/7032, Loss: 0.8629\n",
            "Batch 1080/7032, Loss: 0.9387\n",
            "Batch 1090/7032, Loss: 0.9254\n",
            "Batch 1100/7032, Loss: 0.9978\n",
            "Batch 1110/7032, Loss: 1.0265\n",
            "Batch 1120/7032, Loss: 0.8575\n",
            "Batch 1130/7032, Loss: 1.0317\n",
            "Batch 1140/7032, Loss: 0.9542\n",
            "Batch 1150/7032, Loss: 0.8782\n",
            "Batch 1160/7032, Loss: 0.8552\n",
            "Batch 1170/7032, Loss: 0.9054\n",
            "Batch 1180/7032, Loss: 0.8977\n",
            "Batch 1190/7032, Loss: 0.9785\n",
            "Batch 1200/7032, Loss: 0.8370\n",
            "Batch 1210/7032, Loss: 0.9196\n",
            "Batch 1220/7032, Loss: 0.8488\n",
            "Batch 1230/7032, Loss: 1.0590\n",
            "Batch 1240/7032, Loss: 0.9806\n",
            "Batch 1250/7032, Loss: 0.8328\n",
            "Batch 1260/7032, Loss: 0.8880\n",
            "Batch 1270/7032, Loss: 0.9239\n",
            "Batch 1280/7032, Loss: 0.8759\n",
            "Batch 1290/7032, Loss: 0.9220\n",
            "Batch 1300/7032, Loss: 0.9498\n",
            "Batch 1310/7032, Loss: 0.8365\n",
            "Batch 1320/7032, Loss: 0.9045\n",
            "Batch 1330/7032, Loss: 0.8498\n",
            "Batch 1340/7032, Loss: 0.9380\n",
            "Batch 1350/7032, Loss: 0.9050\n",
            "Batch 1360/7032, Loss: 0.9005\n",
            "Batch 1370/7032, Loss: 0.9105\n",
            "Batch 1380/7032, Loss: 0.8666\n",
            "Batch 1390/7032, Loss: 0.8489\n",
            "Batch 1400/7032, Loss: 0.8998\n",
            "Batch 1410/7032, Loss: 0.9522\n",
            "Batch 1420/7032, Loss: 0.8161\n",
            "Batch 1430/7032, Loss: 1.0361\n",
            "Batch 1440/7032, Loss: 0.9209\n",
            "Batch 1450/7032, Loss: 0.9625\n",
            "Batch 1460/7032, Loss: 0.8799\n",
            "Batch 1470/7032, Loss: 0.8585\n",
            "Batch 1480/7032, Loss: 0.8750\n",
            "Batch 1490/7032, Loss: 1.0137\n",
            "Batch 1500/7032, Loss: 0.8049\n",
            "Batch 1510/7032, Loss: 0.8730\n",
            "Batch 1520/7032, Loss: 0.9857\n",
            "Batch 1530/7032, Loss: 0.9169\n",
            "Batch 1540/7032, Loss: 1.0832\n",
            "Batch 1550/7032, Loss: 0.9692\n",
            "Batch 1560/7032, Loss: 0.9941\n",
            "Batch 1570/7032, Loss: 0.8322\n",
            "Batch 1580/7032, Loss: 0.9662\n",
            "Batch 1590/7032, Loss: 0.9924\n",
            "Batch 1600/7032, Loss: 1.0008\n",
            "Batch 1610/7032, Loss: 0.9322\n",
            "Batch 1620/7032, Loss: 0.9136\n",
            "Batch 1630/7032, Loss: 0.9603\n",
            "Batch 1640/7032, Loss: 0.9210\n",
            "Batch 1650/7032, Loss: 0.9548\n",
            "Batch 1660/7032, Loss: 0.8993\n",
            "Batch 1670/7032, Loss: 0.9593\n",
            "Batch 1680/7032, Loss: 0.9491\n",
            "Batch 1690/7032, Loss: 0.9747\n",
            "Batch 1700/7032, Loss: 0.9580\n",
            "Batch 1710/7032, Loss: 0.9219\n",
            "Batch 1720/7032, Loss: 0.9128\n",
            "Batch 1730/7032, Loss: 0.9470\n",
            "Batch 1740/7032, Loss: 1.0529\n",
            "Batch 1750/7032, Loss: 1.0046\n",
            "Batch 1760/7032, Loss: 0.8018\n",
            "Batch 1770/7032, Loss: 0.9301\n",
            "Batch 1780/7032, Loss: 0.9227\n",
            "Batch 1790/7032, Loss: 0.9279\n",
            "Batch 1800/7032, Loss: 0.9728\n",
            "Batch 1810/7032, Loss: 0.8760\n",
            "Batch 1820/7032, Loss: 0.9812\n",
            "Batch 1830/7032, Loss: 1.0303\n",
            "Batch 1840/7032, Loss: 1.0815\n",
            "Batch 1850/7032, Loss: 0.8909\n",
            "Batch 1860/7032, Loss: 1.0589\n",
            "Batch 1870/7032, Loss: 0.8404\n",
            "Batch 1880/7032, Loss: 0.8069\n",
            "Batch 1890/7032, Loss: 0.9729\n",
            "Batch 1900/7032, Loss: 1.0101\n",
            "Batch 1910/7032, Loss: 0.9929\n",
            "Batch 1920/7032, Loss: 0.9176\n",
            "Batch 1930/7032, Loss: 0.9521\n",
            "Batch 1940/7032, Loss: 0.9845\n",
            "Batch 1950/7032, Loss: 0.8926\n",
            "Batch 1960/7032, Loss: 0.9516\n",
            "Batch 1970/7032, Loss: 0.8557\n",
            "Batch 1980/7032, Loss: 0.8522\n",
            "Batch 1990/7032, Loss: 0.8939\n",
            "Batch 2000/7032, Loss: 0.8782\n",
            "Batch 2010/7032, Loss: 1.0306\n",
            "Batch 2020/7032, Loss: 0.8832\n",
            "Batch 2030/7032, Loss: 0.8513\n",
            "Batch 2040/7032, Loss: 0.9616\n",
            "Batch 2050/7032, Loss: 0.9500\n",
            "Batch 2060/7032, Loss: 0.9355\n",
            "Batch 2070/7032, Loss: 1.0117\n",
            "Batch 2080/7032, Loss: 1.0479\n",
            "Batch 2090/7032, Loss: 1.0072\n",
            "Batch 2100/7032, Loss: 0.9865\n",
            "Batch 2110/7032, Loss: 0.9490\n",
            "Batch 2120/7032, Loss: 0.8605\n",
            "Batch 2130/7032, Loss: 1.0217\n",
            "Batch 2140/7032, Loss: 0.9473\n",
            "Batch 2150/7032, Loss: 0.9089\n",
            "Batch 2160/7032, Loss: 0.8870\n",
            "Batch 2170/7032, Loss: 0.9402\n",
            "Batch 2180/7032, Loss: 0.9821\n",
            "Batch 2190/7032, Loss: 0.9056\n",
            "Batch 2200/7032, Loss: 0.8277\n",
            "Batch 2210/7032, Loss: 0.9470\n",
            "Batch 2220/7032, Loss: 0.9438\n",
            "Batch 2230/7032, Loss: 0.9664\n",
            "Batch 2240/7032, Loss: 1.0083\n",
            "Batch 2250/7032, Loss: 0.9862\n",
            "Batch 2260/7032, Loss: 0.9245\n",
            "Batch 2270/7032, Loss: 1.0089\n",
            "Batch 2280/7032, Loss: 0.8252\n",
            "Batch 2290/7032, Loss: 0.9467\n",
            "Batch 2300/7032, Loss: 0.9100\n",
            "Batch 2310/7032, Loss: 0.8404\n",
            "Batch 2320/7032, Loss: 0.8875\n",
            "Batch 2330/7032, Loss: 0.9943\n",
            "Batch 2340/7032, Loss: 0.9353\n",
            "Batch 2350/7032, Loss: 0.9860\n",
            "Batch 2360/7032, Loss: 0.8399\n",
            "Batch 2370/7032, Loss: 0.8886\n",
            "Batch 2380/7032, Loss: 0.9568\n",
            "Batch 2390/7032, Loss: 0.9565\n",
            "Batch 2400/7032, Loss: 0.9815\n",
            "Batch 2410/7032, Loss: 0.9540\n",
            "Batch 2420/7032, Loss: 0.8166\n",
            "Batch 2430/7032, Loss: 1.0768\n",
            "Batch 2440/7032, Loss: 0.9519\n",
            "Batch 2450/7032, Loss: 0.9333\n",
            "Batch 2460/7032, Loss: 0.8916\n",
            "Batch 2470/7032, Loss: 0.8578\n",
            "Batch 2480/7032, Loss: 0.8757\n",
            "Batch 2490/7032, Loss: 0.7628\n",
            "Batch 2500/7032, Loss: 0.7900\n",
            "Batch 2510/7032, Loss: 0.9570\n",
            "Batch 2520/7032, Loss: 0.8883\n",
            "Batch 2530/7032, Loss: 0.9348\n",
            "Batch 2540/7032, Loss: 0.9815\n",
            "Batch 2550/7032, Loss: 0.9733\n",
            "Batch 2560/7032, Loss: 1.1094\n",
            "Batch 2570/7032, Loss: 0.8262\n",
            "Batch 2580/7032, Loss: 0.9732\n",
            "Batch 2590/7032, Loss: 0.9723\n",
            "Batch 2600/7032, Loss: 0.9371\n",
            "Batch 2610/7032, Loss: 1.0305\n",
            "Batch 2620/7032, Loss: 0.9559\n",
            "Batch 2630/7032, Loss: 0.9434\n",
            "Batch 2640/7032, Loss: 0.9855\n",
            "Batch 2650/7032, Loss: 0.8481\n",
            "Batch 2660/7032, Loss: 0.9616\n",
            "Batch 2670/7032, Loss: 1.0475\n",
            "Batch 2680/7032, Loss: 0.8908\n",
            "Batch 2690/7032, Loss: 0.9788\n",
            "Batch 2700/7032, Loss: 0.9225\n",
            "Batch 2710/7032, Loss: 0.9806\n",
            "Batch 2720/7032, Loss: 0.9240\n",
            "Batch 2730/7032, Loss: 1.0012\n",
            "Batch 2740/7032, Loss: 0.9244\n",
            "Batch 2750/7032, Loss: 0.9255\n",
            "Batch 2760/7032, Loss: 0.9012\n",
            "Batch 2770/7032, Loss: 0.9836\n",
            "Batch 2780/7032, Loss: 0.9756\n",
            "Batch 2790/7032, Loss: 0.8130\n",
            "Batch 2800/7032, Loss: 0.9747\n",
            "Batch 2810/7032, Loss: 1.0106\n",
            "Batch 2820/7032, Loss: 0.8663\n",
            "Batch 2830/7032, Loss: 0.9328\n",
            "Batch 2840/7032, Loss: 0.8665\n",
            "Batch 2850/7032, Loss: 0.8412\n",
            "Batch 2860/7032, Loss: 1.1206\n",
            "Batch 2870/7032, Loss: 0.9751\n",
            "Batch 2880/7032, Loss: 0.9907\n",
            "Batch 2890/7032, Loss: 0.7806\n",
            "Batch 2900/7032, Loss: 0.9321\n",
            "Batch 2910/7032, Loss: 1.0019\n",
            "Batch 2920/7032, Loss: 0.9368\n",
            "Batch 2930/7032, Loss: 0.9675\n",
            "Batch 2940/7032, Loss: 1.0323\n",
            "Batch 2950/7032, Loss: 0.9273\n",
            "Batch 2960/7032, Loss: 0.9135\n",
            "Batch 2970/7032, Loss: 0.9167\n",
            "Batch 2980/7032, Loss: 0.9489\n",
            "Batch 2990/7032, Loss: 0.9677\n",
            "Batch 3000/7032, Loss: 0.9121\n",
            "Batch 3010/7032, Loss: 0.9286\n",
            "Batch 3020/7032, Loss: 1.0723\n",
            "Batch 3030/7032, Loss: 0.9518\n",
            "Batch 3040/7032, Loss: 0.8372\n",
            "Batch 3050/7032, Loss: 0.9965\n",
            "Batch 3060/7032, Loss: 1.0123\n",
            "Batch 3070/7032, Loss: 0.8986\n",
            "Batch 3080/7032, Loss: 0.8833\n",
            "Batch 3090/7032, Loss: 0.9534\n",
            "Batch 3100/7032, Loss: 0.8049\n",
            "Batch 3110/7032, Loss: 1.0164\n",
            "Batch 3120/7032, Loss: 0.9477\n",
            "Batch 3130/7032, Loss: 0.9784\n",
            "Batch 3140/7032, Loss: 1.0278\n",
            "Batch 3150/7032, Loss: 0.9664\n",
            "Batch 3160/7032, Loss: 0.9355\n",
            "Batch 3170/7032, Loss: 0.9409\n",
            "Batch 3180/7032, Loss: 0.8437\n",
            "Batch 3190/7032, Loss: 0.9745\n",
            "Batch 3200/7032, Loss: 0.9703\n",
            "Batch 3210/7032, Loss: 0.9505\n",
            "Batch 3220/7032, Loss: 0.8529\n",
            "Batch 3230/7032, Loss: 0.8313\n",
            "Batch 3240/7032, Loss: 0.9543\n",
            "Batch 3250/7032, Loss: 0.9698\n",
            "Batch 3260/7032, Loss: 0.9475\n",
            "Batch 3270/7032, Loss: 0.8285\n",
            "Batch 3280/7032, Loss: 0.9166\n",
            "Batch 3290/7032, Loss: 0.9027\n",
            "Batch 3300/7032, Loss: 0.9099\n",
            "Batch 3310/7032, Loss: 0.9995\n",
            "Batch 3320/7032, Loss: 0.8458\n",
            "Batch 3330/7032, Loss: 0.8858\n",
            "Batch 3340/7032, Loss: 0.9658\n",
            "Batch 3350/7032, Loss: 0.9334\n",
            "Batch 3360/7032, Loss: 0.8917\n",
            "Batch 3370/7032, Loss: 1.0110\n",
            "Batch 3380/7032, Loss: 0.9780\n",
            "Batch 3390/7032, Loss: 1.0789\n",
            "Batch 3400/7032, Loss: 0.9873\n",
            "Batch 3410/7032, Loss: 0.9317\n",
            "Batch 3420/7032, Loss: 0.8845\n",
            "Batch 3430/7032, Loss: 0.9203\n",
            "Batch 3440/7032, Loss: 0.8941\n",
            "Batch 3450/7032, Loss: 0.9369\n",
            "Batch 3460/7032, Loss: 0.9643\n",
            "Batch 3470/7032, Loss: 0.8966\n",
            "Batch 3480/7032, Loss: 1.0864\n",
            "Batch 3490/7032, Loss: 0.8593\n",
            "Batch 3500/7032, Loss: 0.8715\n",
            "Batch 3510/7032, Loss: 0.9069\n",
            "Batch 3520/7032, Loss: 0.9033\n",
            "Batch 3530/7032, Loss: 0.9300\n",
            "Batch 3540/7032, Loss: 0.8431\n",
            "Batch 3550/7032, Loss: 0.8501\n",
            "Batch 3560/7032, Loss: 0.9131\n",
            "Batch 3570/7032, Loss: 1.0475\n",
            "Batch 3580/7032, Loss: 0.8638\n",
            "Batch 3590/7032, Loss: 0.8011\n",
            "Batch 3600/7032, Loss: 0.7972\n",
            "Batch 3610/7032, Loss: 0.9388\n",
            "Batch 3620/7032, Loss: 0.9613\n",
            "Batch 3630/7032, Loss: 0.9970\n",
            "Batch 3640/7032, Loss: 1.0117\n",
            "Batch 3650/7032, Loss: 0.8720\n",
            "Batch 3660/7032, Loss: 0.9803\n",
            "Batch 3670/7032, Loss: 0.8876\n",
            "Batch 3680/7032, Loss: 1.0567\n",
            "Batch 3690/7032, Loss: 0.8802\n",
            "Batch 3700/7032, Loss: 0.9628\n",
            "Batch 3710/7032, Loss: 0.9042\n",
            "Batch 3720/7032, Loss: 1.0326\n",
            "Batch 3730/7032, Loss: 0.8666\n",
            "Batch 3740/7032, Loss: 0.9319\n",
            "Batch 3750/7032, Loss: 0.9926\n",
            "Batch 3760/7032, Loss: 1.0417\n",
            "Batch 3770/7032, Loss: 0.8883\n",
            "Batch 3780/7032, Loss: 0.9577\n",
            "Batch 3790/7032, Loss: 1.1383\n",
            "Batch 3800/7032, Loss: 0.9497\n",
            "Batch 3810/7032, Loss: 0.8857\n",
            "Batch 3820/7032, Loss: 0.9072\n",
            "Batch 3830/7032, Loss: 0.9253\n",
            "Batch 3840/7032, Loss: 0.9011\n",
            "Batch 3850/7032, Loss: 0.9245\n",
            "Batch 3860/7032, Loss: 0.7605\n",
            "Batch 3870/7032, Loss: 0.9808\n",
            "Batch 3880/7032, Loss: 0.9749\n",
            "Batch 3890/7032, Loss: 0.8500\n",
            "Batch 3900/7032, Loss: 0.8518\n",
            "Batch 3910/7032, Loss: 0.9870\n",
            "Batch 3920/7032, Loss: 0.9326\n",
            "Batch 3930/7032, Loss: 0.9924\n",
            "Batch 3940/7032, Loss: 0.9089\n",
            "Batch 3950/7032, Loss: 0.8405\n",
            "Batch 3960/7032, Loss: 0.9398\n",
            "Batch 3970/7032, Loss: 0.9968\n",
            "Batch 3980/7032, Loss: 0.8436\n",
            "Batch 3990/7032, Loss: 0.9180\n",
            "Batch 4000/7032, Loss: 0.8982\n",
            "Batch 4010/7032, Loss: 0.9945\n",
            "Batch 4020/7032, Loss: 0.7305\n",
            "Batch 4030/7032, Loss: 0.9267\n",
            "Batch 4040/7032, Loss: 0.9178\n",
            "Batch 4050/7032, Loss: 0.8886\n",
            "Batch 4060/7032, Loss: 0.9054\n",
            "Batch 4070/7032, Loss: 1.0610\n",
            "Batch 4080/7032, Loss: 1.0605\n",
            "Batch 4090/7032, Loss: 0.9040\n",
            "Batch 4100/7032, Loss: 0.9102\n",
            "Batch 4110/7032, Loss: 1.0922\n",
            "Batch 4120/7032, Loss: 0.9446\n",
            "Batch 4130/7032, Loss: 0.8688\n",
            "Batch 4140/7032, Loss: 0.9097\n",
            "Batch 4150/7032, Loss: 0.9565\n",
            "Batch 4160/7032, Loss: 0.8809\n",
            "Batch 4170/7032, Loss: 0.8681\n",
            "Batch 4180/7032, Loss: 0.9846\n",
            "Batch 4190/7032, Loss: 0.9641\n",
            "Batch 4200/7032, Loss: 0.9276\n",
            "Batch 4210/7032, Loss: 0.8043\n",
            "Batch 4220/7032, Loss: 0.9423\n",
            "Batch 4230/7032, Loss: 0.8773\n",
            "Batch 4240/7032, Loss: 0.8934\n",
            "Batch 4250/7032, Loss: 0.9811\n",
            "Batch 4260/7032, Loss: 0.9582\n",
            "Batch 4270/7032, Loss: 0.9520\n",
            "Batch 4280/7032, Loss: 0.9198\n",
            "Batch 4290/7032, Loss: 0.9937\n",
            "Batch 4300/7032, Loss: 0.8810\n",
            "Batch 4310/7032, Loss: 1.0136\n",
            "Batch 4320/7032, Loss: 0.9352\n",
            "Batch 4330/7032, Loss: 0.7650\n",
            "Batch 4340/7032, Loss: 0.8459\n",
            "Batch 4350/7032, Loss: 0.8183\n",
            "Batch 4360/7032, Loss: 0.9249\n",
            "Batch 4370/7032, Loss: 1.0113\n",
            "Batch 4380/7032, Loss: 0.8656\n",
            "Batch 4390/7032, Loss: 0.9019\n",
            "Batch 4400/7032, Loss: 1.0292\n",
            "Batch 4410/7032, Loss: 0.7987\n",
            "Batch 4420/7032, Loss: 0.8599\n",
            "Batch 4430/7032, Loss: 0.8299\n",
            "Batch 4440/7032, Loss: 0.8509\n",
            "Batch 4450/7032, Loss: 0.8930\n",
            "Batch 4460/7032, Loss: 0.8374\n",
            "Batch 4470/7032, Loss: 0.9895\n",
            "Batch 4480/7032, Loss: 0.9223\n",
            "Batch 4490/7032, Loss: 0.9667\n",
            "Batch 4500/7032, Loss: 0.8659\n",
            "Batch 4510/7032, Loss: 1.0488\n",
            "Batch 4520/7032, Loss: 0.9936\n",
            "Batch 4530/7032, Loss: 1.0133\n",
            "Batch 4540/7032, Loss: 0.9434\n",
            "Batch 4550/7032, Loss: 0.9537\n",
            "Batch 4560/7032, Loss: 0.7979\n",
            "Batch 4570/7032, Loss: 1.0037\n",
            "Batch 4580/7032, Loss: 0.9994\n",
            "Batch 4590/7032, Loss: 1.0264\n",
            "Batch 4600/7032, Loss: 1.0047\n",
            "Batch 4610/7032, Loss: 1.0267\n",
            "Batch 4620/7032, Loss: 1.0022\n",
            "Batch 4630/7032, Loss: 0.9583\n",
            "Batch 4640/7032, Loss: 0.9350\n",
            "Batch 4650/7032, Loss: 0.9400\n",
            "Batch 4660/7032, Loss: 0.9150\n",
            "Batch 4670/7032, Loss: 0.9348\n",
            "Batch 4680/7032, Loss: 0.9521\n",
            "Batch 4690/7032, Loss: 0.9530\n",
            "Batch 4700/7032, Loss: 0.8142\n",
            "Batch 4710/7032, Loss: 0.9443\n",
            "Batch 4720/7032, Loss: 0.9687\n",
            "Batch 4730/7032, Loss: 0.9034\n",
            "Batch 4740/7032, Loss: 0.9856\n",
            "Batch 4750/7032, Loss: 0.8837\n",
            "Batch 4760/7032, Loss: 1.0480\n",
            "Batch 4770/7032, Loss: 1.0192\n",
            "Batch 4780/7032, Loss: 1.0090\n",
            "Batch 4790/7032, Loss: 0.8326\n",
            "Batch 4800/7032, Loss: 0.8553\n",
            "Batch 4810/7032, Loss: 1.0006\n",
            "Batch 4820/7032, Loss: 0.9228\n",
            "Batch 4830/7032, Loss: 0.9486\n",
            "Batch 4840/7032, Loss: 0.9857\n",
            "Batch 4850/7032, Loss: 0.8900\n",
            "Batch 4860/7032, Loss: 0.9665\n",
            "Batch 4870/7032, Loss: 0.7957\n",
            "Batch 4880/7032, Loss: 0.8755\n",
            "Batch 4890/7032, Loss: 0.9331\n",
            "Batch 4900/7032, Loss: 1.0218\n",
            "Batch 4910/7032, Loss: 0.8737\n",
            "Batch 4920/7032, Loss: 0.9623\n",
            "Batch 4930/7032, Loss: 0.9102\n",
            "Batch 4940/7032, Loss: 0.9428\n",
            "Batch 4950/7032, Loss: 0.9032\n",
            "Batch 4960/7032, Loss: 0.9234\n",
            "Batch 4970/7032, Loss: 0.8708\n",
            "Batch 4980/7032, Loss: 0.9385\n",
            "Batch 4990/7032, Loss: 1.0246\n",
            "Batch 5000/7032, Loss: 0.8286\n",
            "Batch 5010/7032, Loss: 0.9933\n",
            "Batch 5020/7032, Loss: 0.8299\n",
            "Batch 5030/7032, Loss: 0.8733\n",
            "Batch 5040/7032, Loss: 1.0054\n",
            "Batch 5050/7032, Loss: 1.0823\n",
            "Batch 5060/7032, Loss: 0.9890\n",
            "Batch 5070/7032, Loss: 0.8413\n",
            "Batch 5080/7032, Loss: 0.9467\n",
            "Batch 5090/7032, Loss: 0.9602\n",
            "Batch 5100/7032, Loss: 0.9023\n",
            "Batch 5110/7032, Loss: 0.9107\n",
            "Batch 5120/7032, Loss: 0.9435\n",
            "Batch 5130/7032, Loss: 0.9616\n",
            "Batch 5140/7032, Loss: 0.8817\n",
            "Batch 5150/7032, Loss: 0.9003\n",
            "Batch 5160/7032, Loss: 0.8809\n",
            "Batch 5170/7032, Loss: 0.9422\n",
            "Batch 5180/7032, Loss: 0.8747\n",
            "Batch 5190/7032, Loss: 0.8904\n",
            "Batch 5200/7032, Loss: 0.9384\n",
            "Batch 5210/7032, Loss: 0.9201\n",
            "Batch 5220/7032, Loss: 0.9218\n",
            "Batch 5230/7032, Loss: 0.9189\n",
            "Batch 5240/7032, Loss: 0.8844\n",
            "Batch 5250/7032, Loss: 0.9102\n",
            "Batch 5260/7032, Loss: 0.9031\n",
            "Batch 5270/7032, Loss: 0.9388\n",
            "Batch 5280/7032, Loss: 0.8814\n",
            "Batch 5290/7032, Loss: 0.9711\n",
            "Batch 5300/7032, Loss: 0.9214\n",
            "Batch 5310/7032, Loss: 0.9076\n",
            "Batch 5320/7032, Loss: 0.9638\n",
            "Batch 5330/7032, Loss: 0.8864\n",
            "Batch 5340/7032, Loss: 0.9221\n",
            "Batch 5350/7032, Loss: 0.9601\n",
            "Batch 5360/7032, Loss: 0.9637\n",
            "Batch 5370/7032, Loss: 0.9979\n",
            "Batch 5380/7032, Loss: 0.9392\n",
            "Batch 5390/7032, Loss: 0.8424\n",
            "Batch 5400/7032, Loss: 0.9372\n",
            "Batch 5410/7032, Loss: 0.8107\n",
            "Batch 5420/7032, Loss: 0.8849\n",
            "Batch 5430/7032, Loss: 0.8461\n",
            "Batch 5440/7032, Loss: 0.8089\n",
            "Batch 5450/7032, Loss: 0.9588\n",
            "Batch 5460/7032, Loss: 0.9581\n",
            "Batch 5470/7032, Loss: 0.9594\n",
            "Batch 5480/7032, Loss: 0.8373\n",
            "Batch 5490/7032, Loss: 0.9720\n",
            "Batch 5500/7032, Loss: 0.9195\n",
            "Batch 5510/7032, Loss: 0.8679\n",
            "Batch 5520/7032, Loss: 0.9807\n",
            "Batch 5530/7032, Loss: 0.9823\n",
            "Batch 5540/7032, Loss: 0.8042\n",
            "Batch 5550/7032, Loss: 1.0832\n",
            "Batch 5560/7032, Loss: 0.8844\n",
            "Batch 5570/7032, Loss: 0.9727\n",
            "Batch 5580/7032, Loss: 0.9814\n",
            "Batch 5590/7032, Loss: 0.7980\n",
            "Batch 5600/7032, Loss: 0.9157\n",
            "Batch 5610/7032, Loss: 0.9365\n",
            "Batch 5620/7032, Loss: 0.9733\n",
            "Batch 5630/7032, Loss: 0.8979\n",
            "Batch 5640/7032, Loss: 0.7993\n",
            "Batch 5650/7032, Loss: 0.8677\n",
            "Batch 5660/7032, Loss: 0.8977\n",
            "Batch 5670/7032, Loss: 0.8504\n",
            "Batch 5680/7032, Loss: 0.8776\n",
            "Batch 5690/7032, Loss: 0.9558\n",
            "Batch 5700/7032, Loss: 1.0263\n",
            "Batch 5710/7032, Loss: 0.8973\n",
            "Batch 5720/7032, Loss: 0.9329\n",
            "Batch 5730/7032, Loss: 1.0043\n",
            "Batch 5740/7032, Loss: 1.0201\n",
            "Batch 5750/7032, Loss: 0.8568\n",
            "Batch 5760/7032, Loss: 1.0261\n",
            "Batch 5770/7032, Loss: 0.9733\n",
            "Batch 5780/7032, Loss: 0.8967\n",
            "Batch 5790/7032, Loss: 0.9035\n",
            "Batch 5800/7032, Loss: 0.9233\n",
            "Batch 5810/7032, Loss: 0.8639\n",
            "Batch 5820/7032, Loss: 0.9311\n",
            "Batch 5830/7032, Loss: 0.9050\n",
            "Batch 5840/7032, Loss: 0.9614\n",
            "Batch 5850/7032, Loss: 0.9997\n",
            "Batch 5860/7032, Loss: 0.9245\n",
            "Batch 5870/7032, Loss: 0.9414\n",
            "Batch 5880/7032, Loss: 0.8579\n",
            "Batch 5890/7032, Loss: 1.0333\n",
            "Batch 5900/7032, Loss: 0.8111\n",
            "Batch 5910/7032, Loss: 0.9514\n",
            "Batch 5920/7032, Loss: 0.7955\n",
            "Batch 5930/7032, Loss: 0.9311\n",
            "Batch 5940/7032, Loss: 0.9803\n",
            "Batch 5950/7032, Loss: 0.9452\n",
            "Batch 5960/7032, Loss: 0.8580\n",
            "Batch 5970/7032, Loss: 1.0265\n",
            "Batch 5980/7032, Loss: 0.8470\n",
            "Batch 5990/7032, Loss: 0.9473\n",
            "Batch 6000/7032, Loss: 0.9027\n",
            "Batch 6010/7032, Loss: 0.8543\n",
            "Batch 6020/7032, Loss: 0.9831\n",
            "Batch 6030/7032, Loss: 0.9122\n",
            "Batch 6040/7032, Loss: 0.8454\n",
            "Batch 6050/7032, Loss: 1.0298\n",
            "Batch 6060/7032, Loss: 0.9597\n",
            "Batch 6070/7032, Loss: 1.0398\n",
            "Batch 6080/7032, Loss: 1.0104\n",
            "Batch 6090/7032, Loss: 0.8673\n",
            "Batch 6100/7032, Loss: 0.8769\n",
            "Batch 6110/7032, Loss: 0.9557\n",
            "Batch 6120/7032, Loss: 0.9197\n",
            "Batch 6130/7032, Loss: 0.9499\n",
            "Batch 6140/7032, Loss: 0.9861\n",
            "Batch 6150/7032, Loss: 0.9205\n",
            "Batch 6160/7032, Loss: 0.9706\n",
            "Batch 6170/7032, Loss: 0.8471\n",
            "Batch 6180/7032, Loss: 0.9372\n",
            "Batch 6190/7032, Loss: 0.8565\n",
            "Batch 6200/7032, Loss: 0.9684\n",
            "Batch 6210/7032, Loss: 1.0829\n",
            "Batch 6220/7032, Loss: 1.0078\n",
            "Batch 6230/7032, Loss: 0.8929\n",
            "Batch 6240/7032, Loss: 0.8889\n",
            "Batch 6250/7032, Loss: 0.9240\n",
            "Batch 6260/7032, Loss: 0.8944\n",
            "Batch 6270/7032, Loss: 1.0255\n",
            "Batch 6280/7032, Loss: 0.9480\n",
            "Batch 6290/7032, Loss: 0.9990\n",
            "Batch 6300/7032, Loss: 0.7876\n",
            "Batch 6310/7032, Loss: 0.9363\n",
            "Batch 6320/7032, Loss: 1.0062\n",
            "Batch 6330/7032, Loss: 0.8577\n",
            "Batch 6340/7032, Loss: 0.8181\n",
            "Batch 6350/7032, Loss: 0.9828\n",
            "Batch 6360/7032, Loss: 0.8521\n",
            "Batch 6370/7032, Loss: 0.8233\n",
            "Batch 6380/7032, Loss: 0.8176\n",
            "Batch 6390/7032, Loss: 0.9931\n",
            "Batch 6400/7032, Loss: 1.0073\n",
            "Batch 6410/7032, Loss: 1.0387\n",
            "Batch 6420/7032, Loss: 0.9068\n",
            "Batch 6430/7032, Loss: 0.9000\n",
            "Batch 6440/7032, Loss: 0.9256\n",
            "Batch 6450/7032, Loss: 0.8662\n",
            "Batch 6460/7032, Loss: 0.8425\n",
            "Batch 6470/7032, Loss: 0.9231\n",
            "Batch 6480/7032, Loss: 0.9847\n",
            "Batch 6490/7032, Loss: 0.8218\n",
            "Batch 6500/7032, Loss: 0.9534\n",
            "Batch 6510/7032, Loss: 0.9326\n",
            "Batch 6520/7032, Loss: 0.8869\n",
            "Batch 6530/7032, Loss: 0.9339\n",
            "Batch 6540/7032, Loss: 0.9549\n",
            "Batch 6550/7032, Loss: 0.9963\n",
            "Batch 6560/7032, Loss: 0.9573\n",
            "Batch 6570/7032, Loss: 0.9473\n",
            "Batch 6580/7032, Loss: 1.0354\n",
            "Batch 6590/7032, Loss: 0.8777\n",
            "Batch 6600/7032, Loss: 0.9921\n",
            "Batch 6610/7032, Loss: 0.9738\n",
            "Batch 6620/7032, Loss: 0.9172\n",
            "Batch 6630/7032, Loss: 0.9099\n",
            "Batch 6640/7032, Loss: 1.0140\n",
            "Batch 6650/7032, Loss: 0.8614\n",
            "Batch 6660/7032, Loss: 0.8855\n",
            "Batch 6670/7032, Loss: 0.9213\n",
            "Batch 6680/7032, Loss: 0.8540\n",
            "Batch 6690/7032, Loss: 0.9400\n",
            "Batch 6700/7032, Loss: 0.8595\n",
            "Batch 6710/7032, Loss: 1.0150\n",
            "Batch 6720/7032, Loss: 0.9381\n",
            "Batch 6730/7032, Loss: 0.7732\n",
            "Batch 6740/7032, Loss: 0.8792\n",
            "Batch 6750/7032, Loss: 1.0050\n",
            "Batch 6760/7032, Loss: 0.9945\n",
            "Batch 6770/7032, Loss: 0.9482\n",
            "Batch 6780/7032, Loss: 0.9195\n",
            "Batch 6790/7032, Loss: 0.8999\n",
            "Batch 6800/7032, Loss: 0.8835\n",
            "Batch 6810/7032, Loss: 0.9533\n",
            "Batch 6820/7032, Loss: 0.9719\n",
            "Batch 6830/7032, Loss: 0.8740\n",
            "Batch 6840/7032, Loss: 0.9709\n",
            "Batch 6850/7032, Loss: 0.9490\n",
            "Batch 6860/7032, Loss: 0.8810\n",
            "Batch 6870/7032, Loss: 0.8798\n",
            "Batch 6880/7032, Loss: 0.9524\n",
            "Batch 6890/7032, Loss: 0.8798\n",
            "Batch 6900/7032, Loss: 0.9981\n",
            "Batch 6910/7032, Loss: 0.9321\n",
            "Batch 6920/7032, Loss: 0.9910\n",
            "Batch 6930/7032, Loss: 0.9334\n",
            "Batch 6940/7032, Loss: 0.9138\n",
            "Batch 6950/7032, Loss: 0.9217\n",
            "Batch 6960/7032, Loss: 0.8500\n",
            "Batch 6970/7032, Loss: 0.9927\n",
            "Batch 6980/7032, Loss: 0.9892\n",
            "Batch 6990/7032, Loss: 0.8932\n",
            "Batch 7000/7032, Loss: 0.8830\n",
            "Batch 7010/7032, Loss: 0.8622\n",
            "Batch 7020/7032, Loss: 0.9005\n",
            "Batch 7030/7032, Loss: 0.9092\n",
            "Batch 7032/7032, Loss: 0.9441\n",
            "Epoch 10/10\n",
            "Batch 10/7032, Loss: 0.9110\n",
            "Batch 20/7032, Loss: 0.9822\n",
            "Batch 30/7032, Loss: 0.8800\n",
            "Batch 40/7032, Loss: 0.9628\n",
            "Batch 50/7032, Loss: 0.9592\n",
            "Batch 60/7032, Loss: 1.0088\n",
            "Batch 70/7032, Loss: 0.9172\n",
            "Batch 80/7032, Loss: 0.9451\n",
            "Batch 90/7032, Loss: 0.7800\n",
            "Batch 100/7032, Loss: 0.8910\n",
            "Batch 110/7032, Loss: 0.7837\n",
            "Batch 120/7032, Loss: 0.9733\n",
            "Batch 130/7032, Loss: 0.9223\n",
            "Batch 140/7032, Loss: 0.8441\n",
            "Batch 150/7032, Loss: 0.9363\n",
            "Batch 160/7032, Loss: 0.9145\n",
            "Batch 170/7032, Loss: 0.8872\n",
            "Batch 180/7032, Loss: 0.7434\n",
            "Batch 190/7032, Loss: 0.7161\n",
            "Batch 200/7032, Loss: 0.9360\n",
            "Batch 210/7032, Loss: 0.8808\n",
            "Batch 220/7032, Loss: 0.8650\n",
            "Batch 230/7032, Loss: 0.7746\n",
            "Batch 240/7032, Loss: 0.9649\n",
            "Batch 250/7032, Loss: 0.8722\n",
            "Batch 260/7032, Loss: 0.9276\n",
            "Batch 270/7032, Loss: 0.9410\n",
            "Batch 280/7032, Loss: 0.9587\n",
            "Batch 290/7032, Loss: 0.8636\n",
            "Batch 300/7032, Loss: 1.0712\n",
            "Batch 310/7032, Loss: 0.9012\n",
            "Batch 320/7032, Loss: 0.8975\n",
            "Batch 330/7032, Loss: 0.9304\n",
            "Batch 340/7032, Loss: 0.9130\n",
            "Batch 350/7032, Loss: 0.8722\n",
            "Batch 360/7032, Loss: 0.9152\n",
            "Batch 370/7032, Loss: 0.8137\n",
            "Batch 380/7032, Loss: 0.8284\n",
            "Batch 390/7032, Loss: 0.9155\n",
            "Batch 400/7032, Loss: 0.8906\n",
            "Batch 410/7032, Loss: 0.8709\n",
            "Batch 420/7032, Loss: 0.9150\n",
            "Batch 430/7032, Loss: 0.8972\n",
            "Batch 440/7032, Loss: 0.9298\n",
            "Batch 450/7032, Loss: 0.9433\n",
            "Batch 460/7032, Loss: 1.0079\n",
            "Batch 470/7032, Loss: 0.9466\n",
            "Batch 480/7032, Loss: 0.9012\n",
            "Batch 490/7032, Loss: 0.8542\n",
            "Batch 500/7032, Loss: 0.8591\n",
            "Batch 510/7032, Loss: 0.8880\n",
            "Batch 520/7032, Loss: 0.9099\n",
            "Batch 530/7032, Loss: 0.8793\n",
            "Batch 540/7032, Loss: 0.9724\n",
            "Batch 550/7032, Loss: 0.9430\n",
            "Batch 560/7032, Loss: 0.8994\n",
            "Batch 570/7032, Loss: 1.0308\n",
            "Batch 580/7032, Loss: 0.8827\n",
            "Batch 590/7032, Loss: 0.8199\n",
            "Batch 600/7032, Loss: 0.8668\n",
            "Batch 610/7032, Loss: 0.8279\n",
            "Batch 620/7032, Loss: 0.8150\n",
            "Batch 630/7032, Loss: 0.9472\n",
            "Batch 640/7032, Loss: 0.8668\n",
            "Batch 650/7032, Loss: 0.7998\n",
            "Batch 660/7032, Loss: 0.8969\n",
            "Batch 670/7032, Loss: 0.9600\n",
            "Batch 680/7032, Loss: 0.9538\n",
            "Batch 690/7032, Loss: 0.8764\n",
            "Batch 700/7032, Loss: 0.9126\n",
            "Batch 710/7032, Loss: 1.0484\n",
            "Batch 720/7032, Loss: 0.8624\n",
            "Batch 730/7032, Loss: 0.8005\n",
            "Batch 740/7032, Loss: 1.0275\n",
            "Batch 750/7032, Loss: 0.9030\n",
            "Batch 760/7032, Loss: 0.8755\n",
            "Batch 770/7032, Loss: 0.9143\n",
            "Batch 780/7032, Loss: 0.8181\n",
            "Batch 790/7032, Loss: 0.9338\n",
            "Batch 800/7032, Loss: 0.9033\n",
            "Batch 810/7032, Loss: 0.9180\n",
            "Batch 820/7032, Loss: 0.8948\n",
            "Batch 830/7032, Loss: 0.8001\n",
            "Batch 840/7032, Loss: 0.8988\n",
            "Batch 850/7032, Loss: 0.9137\n",
            "Batch 860/7032, Loss: 1.0228\n",
            "Batch 870/7032, Loss: 0.9593\n",
            "Batch 880/7032, Loss: 1.0423\n",
            "Batch 890/7032, Loss: 0.9408\n",
            "Batch 900/7032, Loss: 0.9025\n",
            "Batch 910/7032, Loss: 0.8307\n",
            "Batch 920/7032, Loss: 0.8977\n",
            "Batch 930/7032, Loss: 0.9467\n",
            "Batch 940/7032, Loss: 1.0514\n",
            "Batch 950/7032, Loss: 0.8325\n",
            "Batch 960/7032, Loss: 0.8214\n",
            "Batch 970/7032, Loss: 0.9328\n",
            "Batch 980/7032, Loss: 0.8840\n",
            "Batch 990/7032, Loss: 1.0170\n",
            "Batch 1000/7032, Loss: 0.9337\n",
            "Batch 1010/7032, Loss: 0.8085\n",
            "Batch 1020/7032, Loss: 0.9954\n",
            "Batch 1030/7032, Loss: 0.9706\n",
            "Batch 1040/7032, Loss: 0.9374\n",
            "Batch 1050/7032, Loss: 1.0525\n",
            "Batch 1060/7032, Loss: 0.7933\n",
            "Batch 1070/7032, Loss: 0.8786\n",
            "Batch 1080/7032, Loss: 0.9598\n",
            "Batch 1090/7032, Loss: 0.9994\n",
            "Batch 1100/7032, Loss: 0.9044\n",
            "Batch 1110/7032, Loss: 0.9847\n",
            "Batch 1120/7032, Loss: 0.8603\n",
            "Batch 1130/7032, Loss: 0.9212\n",
            "Batch 1140/7032, Loss: 0.7897\n",
            "Batch 1150/7032, Loss: 0.8932\n",
            "Batch 1160/7032, Loss: 0.9706\n",
            "Batch 1170/7032, Loss: 0.8667\n",
            "Batch 1180/7032, Loss: 0.8981\n",
            "Batch 1190/7032, Loss: 0.8218\n",
            "Batch 1200/7032, Loss: 0.9017\n",
            "Batch 1210/7032, Loss: 0.8533\n",
            "Batch 1220/7032, Loss: 0.9023\n",
            "Batch 1230/7032, Loss: 0.9467\n",
            "Batch 1240/7032, Loss: 0.9148\n",
            "Batch 1250/7032, Loss: 0.9864\n",
            "Batch 1260/7032, Loss: 0.8829\n",
            "Batch 1270/7032, Loss: 0.9320\n",
            "Batch 1280/7032, Loss: 0.9392\n",
            "Batch 1290/7032, Loss: 0.9135\n",
            "Batch 1300/7032, Loss: 0.7345\n",
            "Batch 1310/7032, Loss: 0.7561\n",
            "Batch 1320/7032, Loss: 1.1177\n",
            "Batch 1330/7032, Loss: 0.9120\n",
            "Batch 1340/7032, Loss: 1.0025\n",
            "Batch 1350/7032, Loss: 0.9053\n",
            "Batch 1360/7032, Loss: 0.9377\n",
            "Batch 1370/7032, Loss: 0.8372\n",
            "Batch 1380/7032, Loss: 0.9123\n",
            "Batch 1390/7032, Loss: 0.8992\n",
            "Batch 1400/7032, Loss: 0.9691\n",
            "Batch 1410/7032, Loss: 0.9584\n",
            "Batch 1420/7032, Loss: 0.8703\n",
            "Batch 1430/7032, Loss: 0.9598\n",
            "Batch 1440/7032, Loss: 0.8089\n",
            "Batch 1450/7032, Loss: 0.8523\n",
            "Batch 1460/7032, Loss: 0.9311\n",
            "Batch 1470/7032, Loss: 0.9016\n",
            "Batch 1480/7032, Loss: 0.9042\n",
            "Batch 1490/7032, Loss: 0.9639\n",
            "Batch 1500/7032, Loss: 0.9933\n",
            "Batch 1510/7032, Loss: 0.9351\n",
            "Batch 1520/7032, Loss: 0.8918\n",
            "Batch 1530/7032, Loss: 0.9125\n",
            "Batch 1540/7032, Loss: 0.8683\n",
            "Batch 1550/7032, Loss: 0.8109\n",
            "Batch 1560/7032, Loss: 0.9701\n",
            "Batch 1570/7032, Loss: 0.9511\n",
            "Batch 1580/7032, Loss: 0.9258\n",
            "Batch 1590/7032, Loss: 0.8970\n",
            "Batch 1600/7032, Loss: 0.9535\n",
            "Batch 1610/7032, Loss: 0.9804\n",
            "Batch 1620/7032, Loss: 0.9801\n",
            "Batch 1630/7032, Loss: 1.0093\n",
            "Batch 1640/7032, Loss: 0.9106\n",
            "Batch 1650/7032, Loss: 0.8755\n",
            "Batch 1660/7032, Loss: 0.8849\n",
            "Batch 1670/7032, Loss: 0.9260\n",
            "Batch 1680/7032, Loss: 0.9916\n",
            "Batch 1690/7032, Loss: 1.0093\n",
            "Batch 1700/7032, Loss: 0.9293\n",
            "Batch 1710/7032, Loss: 0.7722\n",
            "Batch 1720/7032, Loss: 0.8697\n",
            "Batch 1730/7032, Loss: 0.9167\n",
            "Batch 1740/7032, Loss: 0.9718\n",
            "Batch 1750/7032, Loss: 1.0144\n",
            "Batch 1760/7032, Loss: 0.8757\n",
            "Batch 1770/7032, Loss: 0.8881\n",
            "Batch 1780/7032, Loss: 0.8711\n",
            "Batch 1790/7032, Loss: 0.9961\n",
            "Batch 1800/7032, Loss: 0.8348\n",
            "Batch 1810/7032, Loss: 0.8102\n",
            "Batch 1820/7032, Loss: 1.0086\n",
            "Batch 1830/7032, Loss: 0.8525\n",
            "Batch 1840/7032, Loss: 0.7666\n",
            "Batch 1850/7032, Loss: 0.8964\n",
            "Batch 1860/7032, Loss: 0.9476\n",
            "Batch 1870/7032, Loss: 0.8638\n",
            "Batch 1880/7032, Loss: 0.8712\n",
            "Batch 1890/7032, Loss: 0.9011\n",
            "Batch 1900/7032, Loss: 1.0406\n",
            "Batch 1910/7032, Loss: 0.8430\n",
            "Batch 1920/7032, Loss: 0.9944\n",
            "Batch 1930/7032, Loss: 0.9524\n",
            "Batch 1940/7032, Loss: 0.8644\n",
            "Batch 1950/7032, Loss: 0.9514\n",
            "Batch 1960/7032, Loss: 0.7664\n",
            "Batch 1970/7032, Loss: 0.8606\n",
            "Batch 1980/7032, Loss: 0.8766\n",
            "Batch 1990/7032, Loss: 0.8616\n",
            "Batch 2000/7032, Loss: 0.9127\n",
            "Batch 2010/7032, Loss: 0.8574\n",
            "Batch 2020/7032, Loss: 1.0088\n",
            "Batch 2030/7032, Loss: 0.8945\n",
            "Batch 2040/7032, Loss: 1.0017\n",
            "Batch 2050/7032, Loss: 0.8248\n",
            "Batch 2060/7032, Loss: 0.9304\n",
            "Batch 2070/7032, Loss: 0.8545\n",
            "Batch 2080/7032, Loss: 0.8950\n",
            "Batch 2090/7032, Loss: 0.8622\n",
            "Batch 2100/7032, Loss: 1.0268\n",
            "Batch 2110/7032, Loss: 0.8641\n",
            "Batch 2120/7032, Loss: 0.9431\n",
            "Batch 2130/7032, Loss: 0.9751\n",
            "Batch 2140/7032, Loss: 0.9451\n",
            "Batch 2150/7032, Loss: 0.7925\n",
            "Batch 2160/7032, Loss: 0.8897\n",
            "Batch 2170/7032, Loss: 0.8913\n",
            "Batch 2180/7032, Loss: 0.9134\n",
            "Batch 2190/7032, Loss: 0.9555\n",
            "Batch 2200/7032, Loss: 0.9451\n",
            "Batch 2210/7032, Loss: 0.9751\n",
            "Batch 2220/7032, Loss: 0.8803\n",
            "Batch 2230/7032, Loss: 0.9081\n",
            "Batch 2240/7032, Loss: 0.8055\n",
            "Batch 2250/7032, Loss: 0.8091\n",
            "Batch 2260/7032, Loss: 0.9126\n",
            "Batch 2270/7032, Loss: 0.8841\n",
            "Batch 2280/7032, Loss: 0.9604\n",
            "Batch 2290/7032, Loss: 0.9973\n",
            "Batch 2300/7032, Loss: 0.9446\n",
            "Batch 2310/7032, Loss: 0.8256\n",
            "Batch 2320/7032, Loss: 0.9848\n",
            "Batch 2330/7032, Loss: 0.8485\n",
            "Batch 2340/7032, Loss: 1.0010\n",
            "Batch 2350/7032, Loss: 0.7851\n",
            "Batch 2360/7032, Loss: 0.8985\n",
            "Batch 2370/7032, Loss: 0.9422\n",
            "Batch 2380/7032, Loss: 0.9866\n",
            "Batch 2390/7032, Loss: 0.8596\n",
            "Batch 2400/7032, Loss: 1.0601\n",
            "Batch 2410/7032, Loss: 0.8573\n",
            "Batch 2420/7032, Loss: 0.9101\n",
            "Batch 2430/7032, Loss: 0.8801\n",
            "Batch 2440/7032, Loss: 0.9738\n",
            "Batch 2450/7032, Loss: 1.0303\n",
            "Batch 2460/7032, Loss: 0.8520\n",
            "Batch 2470/7032, Loss: 0.8241\n",
            "Batch 2480/7032, Loss: 0.8606\n",
            "Batch 2490/7032, Loss: 0.9131\n",
            "Batch 2500/7032, Loss: 0.9878\n",
            "Batch 2510/7032, Loss: 0.9071\n",
            "Batch 2520/7032, Loss: 0.9085\n",
            "Batch 2530/7032, Loss: 0.8340\n",
            "Batch 2540/7032, Loss: 0.8982\n",
            "Batch 2550/7032, Loss: 0.9058\n",
            "Batch 2560/7032, Loss: 0.9366\n",
            "Batch 2570/7032, Loss: 0.9497\n",
            "Batch 2580/7032, Loss: 0.8103\n",
            "Batch 2590/7032, Loss: 0.8833\n",
            "Batch 2600/7032, Loss: 0.9141\n",
            "Batch 2610/7032, Loss: 0.9027\n",
            "Batch 2620/7032, Loss: 1.0006\n",
            "Batch 2630/7032, Loss: 0.8421\n",
            "Batch 2640/7032, Loss: 0.8798\n",
            "Batch 2650/7032, Loss: 0.9216\n",
            "Batch 2660/7032, Loss: 0.9401\n",
            "Batch 2670/7032, Loss: 0.7864\n",
            "Batch 2680/7032, Loss: 0.8559\n",
            "Batch 2690/7032, Loss: 0.8920\n",
            "Batch 2700/7032, Loss: 0.9361\n",
            "Batch 2710/7032, Loss: 0.6887\n",
            "Batch 2720/7032, Loss: 0.8501\n",
            "Batch 2730/7032, Loss: 1.0445\n",
            "Batch 2740/7032, Loss: 0.8978\n",
            "Batch 2750/7032, Loss: 0.9345\n",
            "Batch 2760/7032, Loss: 1.0249\n",
            "Batch 2770/7032, Loss: 0.8992\n",
            "Batch 2780/7032, Loss: 0.9581\n",
            "Batch 2790/7032, Loss: 0.9062\n",
            "Batch 2800/7032, Loss: 0.8578\n",
            "Batch 2810/7032, Loss: 0.8806\n",
            "Batch 2820/7032, Loss: 0.8705\n",
            "Batch 2830/7032, Loss: 0.8989\n",
            "Batch 2840/7032, Loss: 0.9166\n",
            "Batch 2850/7032, Loss: 0.9254\n",
            "Batch 2860/7032, Loss: 0.9689\n",
            "Batch 2870/7032, Loss: 0.9556\n",
            "Batch 2880/7032, Loss: 0.9311\n",
            "Batch 2890/7032, Loss: 0.8390\n",
            "Batch 2900/7032, Loss: 0.7827\n",
            "Batch 2910/7032, Loss: 0.8828\n",
            "Batch 2920/7032, Loss: 0.8428\n",
            "Batch 2930/7032, Loss: 0.8797\n",
            "Batch 2940/7032, Loss: 0.8694\n",
            "Batch 2950/7032, Loss: 0.8104\n",
            "Batch 2960/7032, Loss: 0.8888\n",
            "Batch 2970/7032, Loss: 0.9161\n",
            "Batch 2980/7032, Loss: 0.9064\n",
            "Batch 2990/7032, Loss: 1.0089\n",
            "Batch 3000/7032, Loss: 0.8248\n",
            "Batch 3010/7032, Loss: 0.8556\n",
            "Batch 3020/7032, Loss: 0.9440\n",
            "Batch 3030/7032, Loss: 0.8541\n",
            "Batch 3040/7032, Loss: 0.8923\n",
            "Batch 3050/7032, Loss: 0.9358\n",
            "Batch 3060/7032, Loss: 0.9555\n",
            "Batch 3070/7032, Loss: 0.8937\n",
            "Batch 3080/7032, Loss: 0.8861\n",
            "Batch 3090/7032, Loss: 0.9748\n",
            "Batch 3100/7032, Loss: 0.9478\n",
            "Batch 3110/7032, Loss: 0.8887\n",
            "Batch 3120/7032, Loss: 0.8210\n",
            "Batch 3130/7032, Loss: 0.8965\n",
            "Batch 3140/7032, Loss: 0.9675\n",
            "Batch 3150/7032, Loss: 0.9309\n",
            "Batch 3160/7032, Loss: 0.8514\n",
            "Batch 3170/7032, Loss: 0.9427\n",
            "Batch 3180/7032, Loss: 0.9307\n",
            "Batch 3190/7032, Loss: 1.0493\n",
            "Batch 3200/7032, Loss: 0.8357\n",
            "Batch 3210/7032, Loss: 0.7718\n",
            "Batch 3220/7032, Loss: 0.8066\n",
            "Batch 3230/7032, Loss: 0.9478\n",
            "Batch 3240/7032, Loss: 0.9033\n",
            "Batch 3250/7032, Loss: 0.8116\n",
            "Batch 3260/7032, Loss: 0.9220\n",
            "Batch 3270/7032, Loss: 0.9624\n",
            "Batch 3280/7032, Loss: 0.8728\n",
            "Batch 3290/7032, Loss: 1.0232\n",
            "Batch 3300/7032, Loss: 0.8469\n",
            "Batch 3310/7032, Loss: 0.8691\n",
            "Batch 3320/7032, Loss: 0.9282\n",
            "Batch 3330/7032, Loss: 0.9158\n",
            "Batch 3340/7032, Loss: 0.8370\n",
            "Batch 3350/7032, Loss: 0.8645\n",
            "Batch 3360/7032, Loss: 0.8509\n",
            "Batch 3370/7032, Loss: 0.9960\n",
            "Batch 3380/7032, Loss: 0.8183\n",
            "Batch 3390/7032, Loss: 0.9683\n",
            "Batch 3400/7032, Loss: 1.0047\n",
            "Batch 3410/7032, Loss: 1.0255\n",
            "Batch 3420/7032, Loss: 0.9259\n",
            "Batch 3430/7032, Loss: 0.7859\n",
            "Batch 3440/7032, Loss: 0.7725\n",
            "Batch 3450/7032, Loss: 0.9075\n",
            "Batch 3460/7032, Loss: 0.8077\n",
            "Batch 3470/7032, Loss: 0.9065\n",
            "Batch 3480/7032, Loss: 0.9172\n",
            "Batch 3490/7032, Loss: 0.9600\n",
            "Batch 3500/7032, Loss: 0.9449\n",
            "Batch 3510/7032, Loss: 0.8762\n",
            "Batch 3520/7032, Loss: 0.8863\n",
            "Batch 3530/7032, Loss: 1.0171\n",
            "Batch 3540/7032, Loss: 0.9361\n",
            "Batch 3550/7032, Loss: 0.9018\n",
            "Batch 3560/7032, Loss: 0.9019\n",
            "Batch 3570/7032, Loss: 0.8122\n",
            "Batch 3580/7032, Loss: 0.9687\n",
            "Batch 3590/7032, Loss: 0.9959\n",
            "Batch 3600/7032, Loss: 0.9507\n",
            "Batch 3610/7032, Loss: 0.9772\n",
            "Batch 3620/7032, Loss: 0.8180\n",
            "Batch 3630/7032, Loss: 0.8070\n",
            "Batch 3640/7032, Loss: 0.8245\n",
            "Batch 3650/7032, Loss: 0.9678\n",
            "Batch 3660/7032, Loss: 0.9664\n",
            "Batch 3670/7032, Loss: 0.7312\n",
            "Batch 3680/7032, Loss: 0.8999\n",
            "Batch 3690/7032, Loss: 0.9072\n",
            "Batch 3700/7032, Loss: 0.8946\n",
            "Batch 3710/7032, Loss: 1.0003\n",
            "Batch 3720/7032, Loss: 0.9999\n",
            "Batch 3730/7032, Loss: 0.8719\n",
            "Batch 3740/7032, Loss: 0.8980\n",
            "Batch 3750/7032, Loss: 0.9337\n",
            "Batch 3760/7032, Loss: 0.7847\n",
            "Batch 3770/7032, Loss: 0.8949\n",
            "Batch 3780/7032, Loss: 0.9989\n",
            "Batch 3790/7032, Loss: 0.8810\n",
            "Batch 3800/7032, Loss: 0.9593\n",
            "Batch 3810/7032, Loss: 0.8809\n",
            "Batch 3820/7032, Loss: 0.8668\n",
            "Batch 3830/7032, Loss: 1.0265\n",
            "Batch 3840/7032, Loss: 1.0060\n",
            "Batch 3850/7032, Loss: 0.8430\n",
            "Batch 3860/7032, Loss: 0.8886\n",
            "Batch 3870/7032, Loss: 0.8525\n",
            "Batch 3880/7032, Loss: 1.0211\n",
            "Batch 3890/7032, Loss: 0.9449\n",
            "Batch 3900/7032, Loss: 0.9120\n",
            "Batch 3910/7032, Loss: 0.7557\n",
            "Batch 3920/7032, Loss: 0.9069\n",
            "Batch 3930/7032, Loss: 0.8069\n",
            "Batch 3940/7032, Loss: 0.9183\n",
            "Batch 3950/7032, Loss: 0.9253\n",
            "Batch 3960/7032, Loss: 0.8711\n",
            "Batch 3970/7032, Loss: 0.8233\n",
            "Batch 3980/7032, Loss: 0.8646\n",
            "Batch 3990/7032, Loss: 0.8771\n",
            "Batch 4000/7032, Loss: 0.9334\n",
            "Batch 4010/7032, Loss: 0.8922\n",
            "Batch 4020/7032, Loss: 0.9789\n",
            "Batch 4030/7032, Loss: 0.8218\n",
            "Batch 4040/7032, Loss: 0.8987\n",
            "Batch 4050/7032, Loss: 1.0599\n",
            "Batch 4060/7032, Loss: 0.8409\n",
            "Batch 4070/7032, Loss: 0.9083\n",
            "Batch 4080/7032, Loss: 0.9795\n",
            "Batch 4090/7032, Loss: 0.8529\n",
            "Batch 4100/7032, Loss: 0.8593\n",
            "Batch 4110/7032, Loss: 0.9101\n",
            "Batch 4120/7032, Loss: 0.9274\n",
            "Batch 4130/7032, Loss: 0.9211\n",
            "Batch 4140/7032, Loss: 0.8497\n",
            "Batch 4150/7032, Loss: 0.8688\n",
            "Batch 4160/7032, Loss: 1.0880\n",
            "Batch 4170/7032, Loss: 0.9713\n",
            "Batch 4180/7032, Loss: 0.9708\n",
            "Batch 4190/7032, Loss: 0.8898\n",
            "Batch 4200/7032, Loss: 0.8998\n",
            "Batch 4210/7032, Loss: 0.9506\n",
            "Batch 4220/7032, Loss: 0.9869\n",
            "Batch 4230/7032, Loss: 0.8703\n",
            "Batch 4240/7032, Loss: 1.0205\n",
            "Batch 4250/7032, Loss: 0.8766\n",
            "Batch 4260/7032, Loss: 0.9152\n",
            "Batch 4270/7032, Loss: 0.8943\n",
            "Batch 4280/7032, Loss: 0.8580\n",
            "Batch 4290/7032, Loss: 0.8697\n",
            "Batch 4300/7032, Loss: 0.8905\n",
            "Batch 4310/7032, Loss: 0.7630\n",
            "Batch 4320/7032, Loss: 0.8699\n",
            "Batch 4330/7032, Loss: 0.9020\n",
            "Batch 4340/7032, Loss: 0.9066\n",
            "Batch 4350/7032, Loss: 0.8024\n",
            "Batch 4360/7032, Loss: 0.8182\n",
            "Batch 4370/7032, Loss: 0.8933\n",
            "Batch 4380/7032, Loss: 0.7870\n",
            "Batch 4390/7032, Loss: 0.9291\n",
            "Batch 4400/7032, Loss: 0.7036\n",
            "Batch 4410/7032, Loss: 0.9226\n",
            "Batch 4420/7032, Loss: 0.9047\n",
            "Batch 4430/7032, Loss: 0.7650\n",
            "Batch 4440/7032, Loss: 0.8836\n",
            "Batch 4450/7032, Loss: 0.7722\n",
            "Batch 4460/7032, Loss: 0.8734\n",
            "Batch 4470/7032, Loss: 0.8754\n",
            "Batch 4480/7032, Loss: 0.7853\n",
            "Batch 4490/7032, Loss: 0.9050\n",
            "Batch 4500/7032, Loss: 0.7943\n",
            "Batch 4510/7032, Loss: 0.8650\n",
            "Batch 4520/7032, Loss: 0.8766\n",
            "Batch 4530/7032, Loss: 0.9674\n",
            "Batch 4540/7032, Loss: 0.9868\n",
            "Batch 4550/7032, Loss: 0.9599\n",
            "Batch 4560/7032, Loss: 0.9494\n",
            "Batch 4570/7032, Loss: 0.9210\n",
            "Batch 4580/7032, Loss: 1.0072\n",
            "Batch 4590/7032, Loss: 0.7942\n",
            "Batch 4600/7032, Loss: 0.6802\n",
            "Batch 4610/7032, Loss: 0.8585\n",
            "Batch 4620/7032, Loss: 0.9402\n",
            "Batch 4630/7032, Loss: 0.9314\n",
            "Batch 4640/7032, Loss: 1.0140\n",
            "Batch 4650/7032, Loss: 0.8739\n",
            "Batch 4660/7032, Loss: 0.8506\n",
            "Batch 4670/7032, Loss: 0.9199\n",
            "Batch 4680/7032, Loss: 0.8753\n",
            "Batch 4690/7032, Loss: 0.9402\n",
            "Batch 4700/7032, Loss: 0.8592\n",
            "Batch 4710/7032, Loss: 0.9821\n",
            "Batch 4720/7032, Loss: 0.9278\n",
            "Batch 4730/7032, Loss: 0.9449\n",
            "Batch 4740/7032, Loss: 0.9472\n",
            "Batch 4750/7032, Loss: 0.9283\n",
            "Batch 4760/7032, Loss: 0.8650\n",
            "Batch 4770/7032, Loss: 0.8917\n",
            "Batch 4780/7032, Loss: 0.7353\n",
            "Batch 4790/7032, Loss: 0.9083\n",
            "Batch 4800/7032, Loss: 0.9371\n",
            "Batch 4810/7032, Loss: 0.9243\n",
            "Batch 4820/7032, Loss: 0.8966\n",
            "Batch 4830/7032, Loss: 0.9219\n",
            "Batch 4840/7032, Loss: 0.8610\n",
            "Batch 4850/7032, Loss: 0.9280\n",
            "Batch 4860/7032, Loss: 0.8098\n",
            "Batch 4870/7032, Loss: 0.8437\n",
            "Batch 4880/7032, Loss: 0.8485\n",
            "Batch 4890/7032, Loss: 0.8425\n",
            "Batch 4900/7032, Loss: 0.8132\n",
            "Batch 4910/7032, Loss: 0.8324\n",
            "Batch 4920/7032, Loss: 0.8604\n",
            "Batch 4930/7032, Loss: 0.8736\n",
            "Batch 4940/7032, Loss: 0.8847\n",
            "Batch 4950/7032, Loss: 0.8181\n",
            "Batch 4960/7032, Loss: 0.9506\n",
            "Batch 4970/7032, Loss: 1.0506\n",
            "Batch 4980/7032, Loss: 0.8695\n",
            "Batch 4990/7032, Loss: 0.9875\n",
            "Batch 5000/7032, Loss: 1.0262\n",
            "Batch 5010/7032, Loss: 0.9568\n",
            "Batch 5020/7032, Loss: 1.0212\n",
            "Batch 5030/7032, Loss: 0.9913\n",
            "Batch 5040/7032, Loss: 0.8715\n",
            "Batch 5050/7032, Loss: 0.9937\n",
            "Batch 5060/7032, Loss: 0.8964\n",
            "Batch 5070/7032, Loss: 0.8196\n",
            "Batch 5080/7032, Loss: 0.8007\n",
            "Batch 5090/7032, Loss: 0.8556\n",
            "Batch 5100/7032, Loss: 0.9396\n",
            "Batch 5110/7032, Loss: 0.9460\n",
            "Batch 5120/7032, Loss: 0.9268\n",
            "Batch 5130/7032, Loss: 0.9785\n",
            "Batch 5140/7032, Loss: 0.8960\n",
            "Batch 5150/7032, Loss: 0.8786\n",
            "Batch 5160/7032, Loss: 0.8354\n",
            "Batch 5170/7032, Loss: 0.7955\n",
            "Batch 5180/7032, Loss: 0.8673\n",
            "Batch 5190/7032, Loss: 0.9163\n",
            "Batch 5200/7032, Loss: 0.9603\n",
            "Batch 5210/7032, Loss: 0.8790\n",
            "Batch 5220/7032, Loss: 0.8436\n",
            "Batch 5230/7032, Loss: 0.9770\n",
            "Batch 5240/7032, Loss: 0.7985\n",
            "Batch 5250/7032, Loss: 0.8611\n",
            "Batch 5260/7032, Loss: 0.8967\n",
            "Batch 5270/7032, Loss: 0.9221\n",
            "Batch 5280/7032, Loss: 0.9472\n",
            "Batch 5290/7032, Loss: 0.7580\n",
            "Batch 5300/7032, Loss: 0.9618\n",
            "Batch 5310/7032, Loss: 0.8695\n",
            "Batch 5320/7032, Loss: 0.9661\n",
            "Batch 5330/7032, Loss: 0.9266\n",
            "Batch 5340/7032, Loss: 0.8391\n",
            "Batch 5350/7032, Loss: 0.9607\n",
            "Batch 5360/7032, Loss: 0.8533\n",
            "Batch 5370/7032, Loss: 0.8696\n",
            "Batch 5380/7032, Loss: 0.8022\n",
            "Batch 5390/7032, Loss: 1.0813\n",
            "Batch 5400/7032, Loss: 0.8588\n",
            "Batch 5410/7032, Loss: 0.8725\n",
            "Batch 5420/7032, Loss: 0.9112\n",
            "Batch 5430/7032, Loss: 0.8397\n",
            "Batch 5440/7032, Loss: 0.8394\n",
            "Batch 5450/7032, Loss: 0.9465\n",
            "Batch 5460/7032, Loss: 0.8433\n",
            "Batch 5470/7032, Loss: 0.8877\n",
            "Batch 5480/7032, Loss: 0.9479\n",
            "Batch 5490/7032, Loss: 0.8620\n",
            "Batch 5500/7032, Loss: 0.8954\n",
            "Batch 5510/7032, Loss: 0.8452\n",
            "Batch 5520/7032, Loss: 0.9588\n",
            "Batch 5530/7032, Loss: 0.8547\n",
            "Batch 5540/7032, Loss: 0.8260\n",
            "Batch 5550/7032, Loss: 0.8353\n",
            "Batch 5560/7032, Loss: 0.8392\n",
            "Batch 5570/7032, Loss: 1.0765\n",
            "Batch 5580/7032, Loss: 0.9585\n",
            "Batch 5590/7032, Loss: 0.9241\n",
            "Batch 5600/7032, Loss: 0.9647\n",
            "Batch 5610/7032, Loss: 0.9768\n",
            "Batch 5620/7032, Loss: 0.8682\n",
            "Batch 5630/7032, Loss: 0.9673\n",
            "Batch 5640/7032, Loss: 1.0660\n",
            "Batch 5650/7032, Loss: 0.8626\n",
            "Batch 5660/7032, Loss: 1.0186\n",
            "Batch 5670/7032, Loss: 0.8748\n",
            "Batch 5680/7032, Loss: 0.9272\n",
            "Batch 5690/7032, Loss: 0.9530\n",
            "Batch 5700/7032, Loss: 0.7793\n",
            "Batch 5710/7032, Loss: 0.9032\n",
            "Batch 5720/7032, Loss: 0.9453\n",
            "Batch 5730/7032, Loss: 0.8865\n",
            "Batch 5740/7032, Loss: 0.9118\n",
            "Batch 5750/7032, Loss: 0.8750\n",
            "Batch 5760/7032, Loss: 0.9991\n",
            "Batch 5770/7032, Loss: 0.9226\n",
            "Batch 5780/7032, Loss: 0.8864\n",
            "Batch 5790/7032, Loss: 0.9158\n",
            "Batch 5800/7032, Loss: 0.9710\n",
            "Batch 5810/7032, Loss: 0.8139\n",
            "Batch 5820/7032, Loss: 0.8251\n",
            "Batch 5830/7032, Loss: 0.8145\n",
            "Batch 5840/7032, Loss: 0.8798\n",
            "Batch 5850/7032, Loss: 0.8836\n",
            "Batch 5860/7032, Loss: 0.9435\n",
            "Batch 5870/7032, Loss: 0.8871\n",
            "Batch 5880/7032, Loss: 0.9090\n",
            "Batch 5890/7032, Loss: 0.8204\n",
            "Batch 5900/7032, Loss: 0.8698\n",
            "Batch 5910/7032, Loss: 0.9859\n",
            "Batch 5920/7032, Loss: 0.8491\n",
            "Batch 5930/7032, Loss: 0.8970\n",
            "Batch 5940/7032, Loss: 0.9671\n",
            "Batch 5950/7032, Loss: 0.9774\n",
            "Batch 5960/7032, Loss: 0.9203\n",
            "Batch 5970/7032, Loss: 0.9488\n",
            "Batch 5980/7032, Loss: 0.8966\n",
            "Batch 5990/7032, Loss: 0.9056\n",
            "Batch 6000/7032, Loss: 0.8816\n",
            "Batch 6010/7032, Loss: 1.0114\n",
            "Batch 6020/7032, Loss: 0.8756\n",
            "Batch 6030/7032, Loss: 0.8579\n",
            "Batch 6040/7032, Loss: 0.8550\n",
            "Batch 6050/7032, Loss: 1.0089\n",
            "Batch 6060/7032, Loss: 0.7635\n",
            "Batch 6070/7032, Loss: 0.8953\n",
            "Batch 6080/7032, Loss: 0.8911\n",
            "Batch 6090/7032, Loss: 0.9177\n",
            "Batch 6100/7032, Loss: 0.8323\n",
            "Batch 6110/7032, Loss: 0.8154\n",
            "Batch 6120/7032, Loss: 0.8726\n",
            "Batch 6130/7032, Loss: 0.9501\n",
            "Batch 6140/7032, Loss: 0.9099\n",
            "Batch 6150/7032, Loss: 0.8905\n",
            "Batch 6160/7032, Loss: 0.8805\n",
            "Batch 6170/7032, Loss: 0.8794\n",
            "Batch 6180/7032, Loss: 1.0272\n",
            "Batch 6190/7032, Loss: 0.9326\n",
            "Batch 6200/7032, Loss: 0.8850\n",
            "Batch 6210/7032, Loss: 0.8565\n",
            "Batch 6220/7032, Loss: 0.9807\n",
            "Batch 6230/7032, Loss: 0.8716\n",
            "Batch 6240/7032, Loss: 0.9183\n",
            "Batch 6250/7032, Loss: 0.9138\n",
            "Batch 6260/7032, Loss: 0.8520\n",
            "Batch 6270/7032, Loss: 0.8732\n",
            "Batch 6280/7032, Loss: 0.8854\n",
            "Batch 6290/7032, Loss: 0.8998\n",
            "Batch 6300/7032, Loss: 0.8786\n",
            "Batch 6310/7032, Loss: 0.9120\n",
            "Batch 6320/7032, Loss: 0.9357\n",
            "Batch 6330/7032, Loss: 0.8673\n",
            "Batch 6340/7032, Loss: 0.8626\n",
            "Batch 6350/7032, Loss: 0.7683\n",
            "Batch 6360/7032, Loss: 0.8995\n",
            "Batch 6370/7032, Loss: 0.8275\n",
            "Batch 6380/7032, Loss: 0.9390\n",
            "Batch 6390/7032, Loss: 0.9210\n",
            "Batch 6400/7032, Loss: 0.8127\n",
            "Batch 6410/7032, Loss: 1.0008\n",
            "Batch 6420/7032, Loss: 0.9613\n",
            "Batch 6430/7032, Loss: 0.8413\n",
            "Batch 6440/7032, Loss: 0.7597\n",
            "Batch 6450/7032, Loss: 0.9289\n",
            "Batch 6460/7032, Loss: 0.9317\n",
            "Batch 6470/7032, Loss: 0.8371\n",
            "Batch 6480/7032, Loss: 0.8256\n",
            "Batch 6490/7032, Loss: 0.8424\n",
            "Batch 6500/7032, Loss: 0.8602\n",
            "Batch 6510/7032, Loss: 0.8985\n",
            "Batch 6520/7032, Loss: 0.9247\n",
            "Batch 6530/7032, Loss: 0.9632\n",
            "Batch 6540/7032, Loss: 0.8981\n",
            "Batch 6550/7032, Loss: 1.0445\n",
            "Batch 6560/7032, Loss: 0.9539\n",
            "Batch 6570/7032, Loss: 0.8519\n",
            "Batch 6580/7032, Loss: 0.9630\n",
            "Batch 6590/7032, Loss: 0.9602\n",
            "Batch 6600/7032, Loss: 0.8995\n",
            "Batch 6610/7032, Loss: 0.9005\n",
            "Batch 6620/7032, Loss: 0.8760\n",
            "Batch 6630/7032, Loss: 0.9930\n",
            "Batch 6640/7032, Loss: 0.8749\n",
            "Batch 6650/7032, Loss: 0.8839\n",
            "Batch 6660/7032, Loss: 0.8766\n",
            "Batch 6670/7032, Loss: 0.9974\n",
            "Batch 6680/7032, Loss: 0.8858\n",
            "Batch 6690/7032, Loss: 1.0185\n",
            "Batch 6700/7032, Loss: 0.8940\n",
            "Batch 6710/7032, Loss: 0.8508\n",
            "Batch 6720/7032, Loss: 0.8526\n",
            "Batch 6730/7032, Loss: 0.9425\n",
            "Batch 6740/7032, Loss: 0.9550\n",
            "Batch 6750/7032, Loss: 0.8906\n",
            "Batch 6760/7032, Loss: 0.8512\n",
            "Batch 6770/7032, Loss: 0.7096\n",
            "Batch 6780/7032, Loss: 0.9653\n",
            "Batch 6790/7032, Loss: 1.0138\n",
            "Batch 6800/7032, Loss: 0.7909\n",
            "Batch 6810/7032, Loss: 0.8727\n",
            "Batch 6820/7032, Loss: 0.9227\n",
            "Batch 6830/7032, Loss: 0.9561\n",
            "Batch 6840/7032, Loss: 0.7750\n",
            "Batch 6850/7032, Loss: 1.0110\n",
            "Batch 6860/7032, Loss: 0.8274\n",
            "Batch 6870/7032, Loss: 0.9245\n",
            "Batch 6880/7032, Loss: 0.9125\n",
            "Batch 6890/7032, Loss: 0.9440\n",
            "Batch 6900/7032, Loss: 0.7655\n",
            "Batch 6910/7032, Loss: 0.7883\n",
            "Batch 6920/7032, Loss: 0.8692\n",
            "Batch 6930/7032, Loss: 0.9178\n",
            "Batch 6940/7032, Loss: 0.9041\n",
            "Batch 6950/7032, Loss: 0.8286\n",
            "Batch 6960/7032, Loss: 0.8223\n",
            "Batch 6970/7032, Loss: 0.8335\n",
            "Batch 6980/7032, Loss: 0.6939\n",
            "Batch 6990/7032, Loss: 1.0366\n",
            "Batch 7000/7032, Loss: 0.9016\n",
            "Batch 7010/7032, Loss: 0.8875\n",
            "Batch 7020/7032, Loss: 0.9197\n",
            "Batch 7030/7032, Loss: 0.9270\n",
            "Batch 7032/7032, Loss: 0.8012\n"
          ]
        }
      ],
      "source": [
        "train_transformer(transformer, train_dataset, batch_size, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9gEKt0swk9f"
      },
      "outputs": [],
      "source": [
        "# Save parameters\n",
        "save_dir = \"/content/transformer.pt\"\n",
        "torch.save(transformer.state_dict(), save_dir)\n",
        "\n",
        "# Load parameters to same architecture transformer\n",
        "checkpoint = torch.load(save_dir, map_location=\"cuda\")\n",
        "transformer.load_state_dict(checkpoint)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nTXvSFCG2xg"
      },
      "source": [
        "# **Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93iIkbezG5bU"
      },
      "outputs": [],
      "source": [
        "def test_transformer(transformer, test_dataset, batch_size=128, device=None):\n",
        "    if dNevice is None:\n",
        "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    transformer.to(device)\n",
        "\n",
        "    # Load test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Get padding token index\n",
        "    PADDING_TOKEN = transformer.encoder.sentence_embedding.PADDING_TOKEN\n",
        "    token_to_index = transformer.encoder.sentence_embedding.language_to_index\n",
        "    padding_idx = token_to_index[PADDING_TOKEN]\n",
        "\n",
        "    # Same loss setup as training (ignoring padding)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=padding_idx, reduction='none')\n",
        "\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    transformer.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_num, batch in enumerate(test_loader):\n",
        "            src_sentences, tgt_sentences = batch\n",
        "            src_sentences = list(src_sentences)\n",
        "            tgt_sentences = list(tgt_sentences)\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = transformer(\n",
        "                src_sentences,\n",
        "                tgt_sentences,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=True,\n",
        "                dec_end_token=True\n",
        "            )\n",
        "\n",
        "            # Tokenize the true sentences\n",
        "            labels = transformer.decoder.sentence_embedding.batch_tokenize(\n",
        "                tgt_sentences, start_token=False, end_token=True\n",
        "            ).to(device)\n",
        "\n",
        "            # Compute per-token loss\n",
        "            loss = criterion(predictions.view(-1, predictions.size(-1)),\n",
        "                             labels.view(-1))\n",
        "\n",
        "            # Mask out padding and compute mean\n",
        "            valid = labels.view(-1) != padding_idx\n",
        "            loss = loss.sum() / valid.sum()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "    print(f\"\\nTest Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(test_transformer(transformer, test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA3m2s3SIIwL"
      },
      "source": [
        "# **Translate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YXPOAYveuJNb"
      },
      "outputs": [],
      "source": [
        "def translate(ip_sentence, device=None):\n",
        "  if device is None:\n",
        "      device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  transformer.eval()\n",
        "  transformer.to(device) # Ensure the model is on the correct device\n",
        "\n",
        "  ip_sentence = (ip_sentence.lower(),)\n",
        "  op_sentence = (\"\",)\n",
        "\n",
        "  max_sequence_length = transformer.decoder.sentence_embedding.max_sequence_length\n",
        "  # lookup table\n",
        "  idx_to_token = transformer.decoder.sentence_embedding.language_to_index\n",
        "\n",
        "  END_TOKEN = transformer.decoder.sentence_embedding.END_TOKEN\n",
        "  END_IDX = idx_to_token[END_TOKEN]\n",
        "\n",
        "\n",
        "  for step in range(max_sequence_length):\n",
        "    predictions = transformer(\n",
        "            ip_sentence,\n",
        "            op_sentence,\n",
        "            enc_start_token=False,\n",
        "            enc_end_token=False,\n",
        "            dec_start_token=True,   # important\n",
        "            dec_end_token=False     # we don't want end token in interference\n",
        "        )\n",
        "\n",
        "    next_token_prob_distribution = predictions[0][step]\n",
        "    next_token_index = torch.argmax(next_token_prob_distribution).item() # .item() to convert a 0-dim PyTorch tensor into a plain Python number.\n",
        "    next_token = idx_to_token[next_token_index]\n",
        "\n",
        "    if next_token == END_TOKEN:\n",
        "      break\n",
        "\n",
        "    op_sentence = (op_sentence[0] + next_token, )\n",
        "\n",
        "  return  op_sentence[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9677JXKxJrk",
        "outputId": "b7dc08d1-9629-4eb1-8a5a-6ce281987522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "كيف حالك؟\n"
          ]
        }
      ],
      "source": [
        "sentence = translate(transformer, \"how are you\")\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeZKFAG-yBZk",
        "outputId": "4d0fe05a-cb36-436f-e1b9-5d33da57cc92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ذهبت اليوم\n"
          ]
        }
      ],
      "source": [
        "sentence = translate(transformer, \"I went there today\")\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q49aWS8-14Rf",
        "outputId": "7bd41d66-cff6-412c-8585-05b6554a14f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ارني الامر الامر الامر\n"
          ]
        }
      ],
      "source": [
        "sentence = translate(transformer, \"show me your strength\")\n",
        "print(sentence)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lGxIxUNULEBs",
        "LHGEQqsdTbGs",
        "CfoDUiLF9C_a",
        "3kubRXssW2r-",
        "CvJ24TjZWqqK"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
